{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **calt Demo Notebook**\n",
    "\n",
    "This notebook shows a minimal end‑to‑end workflow for the **calt** library:\n",
    "\n",
    "1. **Install and import** the library  \n",
    "2. **Generate** a dataset of *polynomial‑sum* examples  \n",
    "3. **Configure** the tokenizer and model  \n",
    "4. **Train** the Transformer  \n",
    "5. **Visualize** training result  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on Google Colab: \n",
    "- Change the runtime type to GPU (e.g., T4 GPU) from the Runtime tab -> Change runtime type -> GPU\n",
    "- The `Sympy` backend to simplify the installation dependencies. For extensive usage, we recommend using the `SageMath` backend, which for example allows parallel sample generations.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  – Installation & Imports  \n",
    "Run the next cell to ensure **calt** and its dependencies are installed, then import the required Python packages.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install calt-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PolynomialSampler' from 'calt.generator.sympy' (/home/ara_shun/workspace/calt/src/calt/generator/sympy/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcalt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     PolynomialTrainer,\n\u001b[32m      9\u001b[39m     data_loader,\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcalt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerator\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msympy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     PolynomialSampler,\n\u001b[32m     13\u001b[39m     DatasetGenerator,\n\u001b[32m     14\u001b[39m     DatasetWriter,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcalt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     load_eval_results,\n\u001b[32m     18\u001b[39m     parse_poly,\n\u001b[32m     19\u001b[39m     display_with_diff\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PolynomialSampler' from 'calt.generator.sympy' (/home/ara_shun/workspace/calt/src/calt/generator/sympy/__init__.py)"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import random\n",
    "from sympy import GF, ZZ\n",
    "from sympy.polys.rings import ring, PolyElement\n",
    "from transformers import BartConfig, BartForConditionalGeneration as Transformer\n",
    "from transformers import TrainingArguments\n",
    "from calt import (\n",
    "    PolynomialTrainer,\n",
    "    data_loader,\n",
    ")\n",
    "from calt.generator.sympy import (\n",
    "    PolynomialSampler,\n",
    "    DatasetGenerator,\n",
    "    DatasetWriter,\n",
    ")\n",
    "from calt.data_loader.utils import (\n",
    "    load_eval_results,\n",
    "    parse_poly,\n",
    "    display_with_diff\n",
    ")\n",
    "import torch, random, numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2  – Dataset Generation *(Polynomial Addition)*  \n",
    "This cell uses `calt.generator` utilities to create a synthetic dataset of polynomial‑addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumProblemGenerator:\n",
    "    \"\"\"\n",
    "    Problem generator for polynomial sum problems.\n",
    "\n",
    "    This generator creates problems in which the input is a list of polynomials F = [f_1, f_2, ..., f_n],\n",
    "    and the output is a single polynomial g = f_1 + f_2 + ... + f_n.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, sampler: PolynomialSampler, max_polynomials: int, min_polynomials: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize polynomial sum generator.\n",
    "\n",
    "        Args:\n",
    "            sampler: Polynomial sampler\n",
    "            max_polynomials: Maximum number of polynomials in F\n",
    "            min_polynomials: Minimum number of polynomials in F\n",
    "        \"\"\"\n",
    "        self.sampler = sampler\n",
    "        self.max_polynomials = max_polynomials\n",
    "        self.min_polynomials = min_polynomials\n",
    "\n",
    "    def __call__(self, seed: int) -> Tuple[List[PolyElement], PolyElement]:\n",
    "        \"\"\"\n",
    "        Generate a single sample.\n",
    "\n",
    "        Each sample consists of:\n",
    "        - Input polynomial system F\n",
    "        - Output polynomial g (sum of F)\n",
    "\n",
    "        Args:\n",
    "            seed: Seed for random number generator\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Choose number of polynomials for this sample\n",
    "        num_polys = random.randint(self.min_polynomials, self.max_polynomials)\n",
    "\n",
    "        # Generate input polynomials using sampler\n",
    "        F = self.sampler.sample(num_samples=num_polys)\n",
    "\n",
    "        # Generate output polynomial g (sum of F)\n",
    "        g = sum(F)\n",
    "\n",
    "        return F, g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 4049 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done 4999 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done 6049 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done 7199 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=1)]: Done 8449 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=1)]: Done 9799 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "save_dir = \".\"\n",
    "\n",
    "# set up polynomial ring\n",
    "R, *gens = ring(\"x0,x1\", GF(7), order=\"grevlex\")\n",
    "# Initialize polynomial sampler\n",
    "sampler = PolynomialSampler(\n",
    "    ring=R,\n",
    "    max_num_terms=2,\n",
    "    max_degree=2,\n",
    "    min_degree=1,\n",
    "    degree_sampling=\"uniform\",  # \"uniform\" or \"fixed\"\n",
    "    term_sampling=\"uniform\",  # \"uniform\" or \"fixed\"\n",
    "    max_coeff=None,  # Used for RR and ZZ\n",
    "    num_bound=None,  # Used for QQ\n",
    "    strictly_conditioned=False,\n",
    "    nonzero_instance=True,\n",
    ")\n",
    "# Initialize problem generator\n",
    "problem_generator = SumProblemGenerator(\n",
    "    sampler=sampler,\n",
    "    max_polynomials=2,\n",
    "    min_polynomials=2,\n",
    ")\n",
    "# Initialize dataset generator\n",
    "dataset_generator = DatasetGenerator(\n",
    "    backend=\"multiprocessing\",\n",
    "    n_jobs=1,  # warning: the current version with Sympy backend only supports n_jobs=1.\n",
    "    verbose=True,\n",
    "    root_seed=100,\n",
    ")\n",
    "# Generate training set\n",
    "train_samples, _ = dataset_generator.run(\n",
    "    train=True,\n",
    "    num_samples=10000,\n",
    "    problem_generator=problem_generator,\n",
    ")\n",
    "# Generate test set\n",
    "test_samples, _ = dataset_generator.run(\n",
    "    train=False,\n",
    "    num_samples=1000,\n",
    "    problem_generator=problem_generator,\n",
    ")\n",
    "# Initialize writer\n",
    "dataset_writer = DatasetWriter(save_dir)\n",
    "# Save datasets\n",
    "dataset_writer.save_dataset(train_samples, tag=\"train\")\n",
    "dataset_writer.save_dataset(test_samples, tag=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  – Model Configuration  \n",
    "Here we instantiate the tokenizer, define the Transformer architecture, and prepare the training pipeline.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to any dataset you like; here we assume the toy Sum dataset from the data‑generation notebook.\n",
    "TRAIN_PATH = \"train_raw.txt\"\n",
    "TEST_PATH = \"test_raw.txt\"\n",
    "dataset, tokenizer, data_collator = data_loader(\n",
    "    train_dataset_path=TRAIN_PATH,\n",
    "    test_dataset_path=TEST_PATH,\n",
    "    field=\"GF7\",\n",
    "    num_variables=2,\n",
    "    max_degree=10,  # Should cover the range of generated samples\n",
    "    max_coeff=7,   # Should cover the range of generated samples\n",
    "    max_length=256,\n",
    ")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal architecture.\n",
    "model_cfg = BartConfig(\n",
    "    d_model=256,       # 'width' of the model\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    encoder_layers=2,  # 'depth' of encoder network\n",
    "    decoder_layers=2,  # 'depth' of decoder network\n",
    "    max_position_embeddings=256,  # max length of input/output\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    decoder_start_token_id=tokenizer.bos_token_id,\n",
    "    max_length=256,  # max length of input/output\n",
    ")\n",
    "model = Transformer(config=model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4  – Training Hyper‑parameters  \n",
    "Learning‑rate schedule, batch size, number of epochs, and other trainer options are declared in this cell.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"results/\",\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=50,\n",
    "    per_device_train_batch_size=int(128),\n",
    "    per_device_eval_batch_size=int(128),\n",
    "    save_strategy=\"no\",  # skip checkpoints for the quick demo\n",
    "    seed=SEED,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  – Model Training  \n",
    "Launch the training loop. Progress is typically logged to the console (and optionally to Weights & Biases).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PolynomialTrainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# train\n",
    "results = trainer.train()\n",
    "trainer.save_model()\n",
    "metrics = results.metrics\n",
    "\n",
    "# eval\n",
    "eval_metrics = trainer.evaluate()\n",
    "metrics.update(eval_metrics)\n",
    "success_rate = trainer.generate_evaluation(max_length=128)\n",
    "metrics[\"success_rate\"] = success_rate\n",
    "\n",
    "# save metrics\n",
    "trainer.save_metrics(\"all\", metrics)\n",
    "\n",
    "print(f'success rate on test set: {100*metrics[\"success_rate\"]:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6  – Visualizing Training Results  \n",
    "Finally, we visualize the differences between the mispredicted samples and their correct counterparts. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_texts, ref_texts = load_eval_results(\"results/eval_results.json\")\n",
    "\n",
    "success_cases = [(i, gen, ref) for i, (gen, ref) in enumerate(zip(gen_texts, ref_texts)) if gen == ref]\n",
    "failure_cases = [(i, gen, ref) for i, (gen, ref) in enumerate(zip(gen_texts, ref_texts)) if gen != ref]\n",
    "\n",
    "num_show = 5\n",
    "\n",
    "print('-------------------------')\n",
    "print(''' Success cases ''')\n",
    "print('-------------------------')\n",
    "for (i, gen, ref) in success_cases[:num_show]:\n",
    "    gen_expr = parse_poly(gen, [\"x\", \"y\"])\n",
    "    ref_expr = parse_poly(ref, [\"x\", \"y\"])\n",
    "\n",
    "    print(f\"===== sample id: {i+1} =====\")\n",
    "    display_with_diff(ref_expr, gen_expr)\n",
    "\n",
    "\n",
    "\n",
    "print('\\n-------------------------')\n",
    "print(''' Failure cases ''')\n",
    "print('-------------------------')\n",
    "for (i, gen, ref) in failure_cases[:num_show]:\n",
    "    gen_expr = parse_poly(gen, [\"x\", \"y\"])\n",
    "    ref_expr = parse_poly(ref, [\"x\", \"y\"])\n",
    "\n",
    "    print(f\"===== sample id: {i+1} =====\")\n",
    "    display_with_diff(ref_expr, gen_expr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
