{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EZwRFX_LGg4"
      },
      "source": [
        "# **calt Demo Notebook**\n",
        "\n",
        "This notebook shows a minimal end‑to‑end workflow for the **calt** library:\n",
        "\n",
        "1. **Install and import** the library  \n",
        "2. **Generate** a dataset of *polynomial‑sum* examples  \n",
        "3. **Configure** the tokenizer and model  \n",
        "4. **Train** the Transformer  \n",
        "5. **Visualize** training result  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7miyzfuLGg4"
      },
      "source": [
        "Note on Google Colab:\n",
        "- Change the runtime type to GPU (e.g., T4 GPU) from the Runtime tab -> Change runtime type -> GPU\n",
        "- The `Sympy` backend to simplify the installation dependencies. For extensive usage, we recommend using the `SageMath` backend, which for example allows parallel sample generations.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4vvos4jLGg4"
      },
      "source": [
        "## 1  – Installation & Imports  \n",
        "Run the next cell to ensure **calt** and its dependencies are installed, then import the required Python packages.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv2lW9vtLGg5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install calt-x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gJvzRzqYLGg5",
        "outputId": "77f178c1-4f26-450e-9c27-b3c210c15dc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sato/workspace/calt/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x150d85fdc1b0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "from sympy.polys.orderings import grevlex\n",
        "from sympy.polys.rings import PolyElement\n",
        "from transformers import BartConfig, BartForConditionalGeneration as Transformer\n",
        "from transformers import TrainingArguments\n",
        "from calt import (\n",
        "    Trainer,\n",
        "    load_data,\n",
        ")\n",
        "from calt.dataset_generator.sympy import (\n",
        "    PolynomialSampler,\n",
        "    DatasetGenerator,\n",
        ")\n",
        "from calt.data_loader.utils import (\n",
        "    load_eval_results,\n",
        "    parse_poly,\n",
        "    display_with_diff\n",
        ")\n",
        "import torch, random, numpy as np\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6MRRqjoLGg5"
      },
      "source": [
        "## 2  – Dataset Generation *(Polynomial Addition)*  \n",
        "This cell uses `calt.generator` utilities to create a synthetic dataset of polynomial‑addition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sum_problem_generator(\n",
        "    seed: int,\n",
        ") -> tuple[list[PolyElement], list[PolyElement]]:\n",
        "    \"\"\"\n",
        "    Generate a partial sum problem involving polynomials.\n",
        "\n",
        "    This function creates problems in which the problem is a list of polynomials F = [f_1, f_2, ..., f_n],\n",
        "    and the solution is a list of polynomials G = [g_1, g_2, ..., g_n], where g_i = f_1 + f_2 + ... + f_i.\n",
        "\n",
        "    Args:\n",
        "        seed: Seed for random number generator\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing (F, G) where F is the problem and G is the solution\n",
        "    \"\"\"\n",
        "    # Set random seed\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Initialize polynomial sampler\n",
        "    sampler = PolynomialSampler(\n",
        "        symbols=\"x0, x1\", # \"x, y, z, ... \" or \"x0, x1, x2, ... \"\n",
        "        field_str=\"GF(7)\", # \"QQ\", \"RR\", \"ZZ\", \"GF(p)\", \"GFp\", where p is a prime number\n",
        "        order=\"grevlex\", # \"lex\", \"grevlex\", \"grlex\", \"ilex\", \"igrevlex\", \"igrlex\"\n",
        "        max_num_terms=2,\n",
        "        max_degree=2,\n",
        "        min_degree=1,\n",
        "    )\n",
        "\n",
        "    # Generate problem polynomials using sampler\n",
        "    F = sampler.sample(num_samples=2)\n",
        "\n",
        "    # Generate solution polynomial g (sum of F)\n",
        "    g = sum(F)\n",
        "\n",
        "    return F, g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XtjnDWRtLGg5",
        "outputId": "d02a955d-2165-418d-8cfc-2bc9cb728ee7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "save_dir: .\n",
            "Text output: True\n",
            "JSON output: True\n",
            "=========================== Dataset generation ===========================\n",
            "\n",
            "Starting dataset generation for 2 dataset(s)\n",
            "Dataset sizes: {'train': 10000, 'test': 1000}\n",
            "\n",
            "---------------------------------- train ----------------------------------\n",
            "Dataset size: 10000 samples  (Batch size: 100000)\n",
            "\n",
            "Overall statistics saved for train dataset\n",
            "Total time: 0.71 seconds\n",
            "\n",
            "\n",
            "---------------------------------- test ----------------------------------\n",
            "Dataset size: 1000 samples  (Batch size: 100000)\n",
            "\n",
            "Overall statistics saved for test dataset\n",
            "Total time: 0.48 seconds\n",
            "\n",
            "\n",
            "All datasets generated successfully!\n",
            "==========================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "save_dir = \".\"\n",
        "\n",
        "# Initialize dataset generator\n",
        "dataset_generator = DatasetGenerator(\n",
        "    backend=\"multiprocessing\",\n",
        "    n_jobs=-1,  \n",
        "    verbose=False,\n",
        "    root_seed=100,\n",
        ")\n",
        "# Generate training set with batch processing\n",
        "dataset_generator.run(\n",
        "    dataset_sizes={\"train\": 10000, \"test\": 1000},\n",
        "    problem_generator=sum_problem_generator,\n",
        "    save_dir=save_dir,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCx_Cma7LGg5"
      },
      "source": [
        "## 3  – Model Configuration  \n",
        "Here we instantiate the tokenizer, define the Transformer architecture, and prepare the training pipeline.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bab8fEw2LGg5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded 10000 samples from train_raw.txt\n",
            "Loaded 1000 samples from test_raw.txt\n"
          ]
        }
      ],
      "source": [
        "# Point to any dataset you like; here we assume the toy Sum dataset from the data‑generation notebook.\n",
        "TRAIN_PATH = \"train_raw.txt\"\n",
        "TEST_PATH = \"test_raw.txt\"\n",
        "dataset, tokenizer, data_collator = load_data(\n",
        "    train_dataset_path=TRAIN_PATH,\n",
        "    test_dataset_path=TEST_PATH,\n",
        "    field=\"GF7\",\n",
        "    num_variables=2,\n",
        "    max_degree=10,  # Should cover the range of generated samples\n",
        "    max_coeff=7,   # Should cover the range of generated samples\n",
        "    max_length=256,\n",
        ")\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_OORFesZLGg5"
      },
      "outputs": [],
      "source": [
        "# Minimal architecture.\n",
        "model_cfg = BartConfig(\n",
        "    d_model=256,       # 'width' of the model\n",
        "    vocab_size=len(tokenizer.vocab),\n",
        "    encoder_layers=2,  # 'depth' of encoder network\n",
        "    decoder_layers=2,  # 'depth' of decoder network\n",
        "    max_position_embeddings=256,  # max length of input/output\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    decoder_start_token_id=tokenizer.bos_token_id,\n",
        "    max_length=256,  # max length of input/output\n",
        ")\n",
        "model = Transformer(config=model_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xhiuDmyLGg5"
      },
      "source": [
        "## 4  – Training Hyper‑parameters  \n",
        "Learning‑rate schedule, batch size, number of epochs, and other trainer options are declared in this cell.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HOGIJUfvLGg6"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"results/\",\n",
        "    num_train_epochs=20,\n",
        "    logging_steps=50,\n",
        "    per_device_train_batch_size=int(128),\n",
        "    per_device_eval_batch_size=int(128),\n",
        "    save_strategy=\"no\",  # skip checkpoints for the quick demo\n",
        "    seed=SEED,\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[\"labels\"],\n",
        "    report_to=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhxiww50LGg6"
      },
      "source": [
        "## 5  – Model Training  \n",
        "Launch the training loop. Progress is typically logged to the console (and optionally to Weights & Biases).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yQ22rA9oLGg6",
        "outputId": "bbb93a1d-af2a-4d39-b09f-1aba147e2b23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sato/workspace/calt/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 00:25, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.118000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.269100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.069100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.984400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sato/workspace/calt/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 256}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "success rate on test set: 1.2 %\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    args=args,\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# train\n",
        "results = trainer.train()\n",
        "trainer.save_model()\n",
        "metrics = results.metrics\n",
        "\n",
        "# eval\n",
        "eval_metrics = trainer.evaluate()\n",
        "metrics.update(eval_metrics)\n",
        "success_rate = trainer.evaluate_and_save_generation(max_length=128)\n",
        "metrics[\"success_rate\"] = success_rate\n",
        "\n",
        "# save metrics\n",
        "trainer.save_metrics(\"all\", metrics)\n",
        "\n",
        "print(f'success rate on test set: {100*metrics[\"success_rate\"]:.1f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_1MFnHeLGg6"
      },
      "source": [
        "## 6  – Visualizing Training Results  \n",
        "Finally, we visualize the differences between the mispredicted samples and their correct counterparts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "h4SvyPTJLGg6",
        "outputId": "5a838d7c-7a4f-481f-adf5-2ec36dedbcd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------\n",
            " Success cases \n",
            "-------------------------\n",
            "===== sample id: 148 =====\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\begin{aligned}\n",
              "        \\text{Ground truth\\,:}\\; & {}4 x_{0} + 4 x_{1}\\\\\n",
              "        \\text{Prediction\\,:}\\;   & {}4\\, x_{0} +4\\, x_{1}\n",
              "        \\end{aligned}$"
            ],
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== sample id: 172 =====\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\begin{aligned}\n",
              "        \\text{Ground truth\\,:}\\; & {}4 x_{0} + 6 x_{1}\\\\\n",
              "        \\text{Prediction\\,:}\\;   & {}4\\, x_{0} +6\\, x_{1}\n",
              "        \\end{aligned}$"
            ],
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== sample id: 217 =====\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\begin{aligned}\n",
              "        \\text{Ground truth\\,:}\\; & {}x_{0} + x_{1}\\\\\n",
              "        \\text{Prediction\\,:}\\;   & {}x_{0} +x_{1}\n",
              "        \\end{aligned}$"
            ],
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== sample id: 242 =====\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\begin{aligned}\n",
              "        \\text{Ground truth\\,:}\\; & {}3 x_{0}^{2} + 3 x_{0}\\\\\n",
              "        \\text{Prediction\\,:}\\;   & {}3\\, x_{0}^{2} +3\\, x_{0}\n",
              "        \\end{aligned}$"
            ],
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== sample id: 272 =====\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\begin{aligned}\n",
              "        \\text{Ground truth\\,:}\\; & {}3 x_{0} x_{1} + 6 x_{0}\\\\\n",
              "        \\text{Prediction\\,:}\\;   & {}3\\, x_{0}\\, x_{1} +6\\, x_{0}\n",
              "        \\end{aligned}$"
            ],
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------------\n",
            " Failure cases \n",
            "-------------------------\n",
            "===== sample id: 1 =====\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\begin{aligned}\n",
              "        \\text{Ground truth\\,:}\\; & {}2 x_{1}\\\\\n",
              "        \\text{Prediction\\,:}\\;   & {}\\cancel{11}\\, x_{1}\n",
              "        \\end{aligned}$"
            ],
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== sample id: 2 =====\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\begin{aligned}\n",
              "        \\text{Ground truth\\,:}\\; & {}4 x_{0}^{2} + 4 x_{1}^{2} + 5 x_{1} + 1\\\\\n",
              "        \\text{Prediction\\,:}\\;   & {}4\\, x_{0}^{2} +4\\, x_{1}^{2}\n",
              "        \\end{aligned}$"
            ],
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== sample id: 3 =====\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\begin{aligned}\n",
              "        \\text{Ground truth\\,:}\\; & {}4 x_{0}^{2} + 5 x_{0} x_{1}\\\\\n",
              "        \\text{Prediction\\,:}\\;   & {}\\cancel{3}\\, x_{0}^{2}\n",
              "        \\end{aligned}$"
            ],
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== sample id: 4 =====\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\begin{aligned}\n",
              "        \\text{Ground truth\\,:}\\; & {}4 x_{0} x_{1} + 5 x_{0}\\\\\n",
              "        \\text{Prediction\\,:}\\;   & {}\\cancel{5}\\, x_{0}\\, x_{1} +\\cancel{2}\\, x_{0}\n",
              "        \\end{aligned}$"
            ],
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== sample id: 5 =====\n"
          ]
        },
        {
          "data": {
            "text/latex": [
              "$\\displaystyle \\begin{aligned}\n",
              "        \\text{Ground truth\\,:}\\; & {}x_{0}^{2} + 6 x_{0} + 2 x_{1} + 2\\\\\n",
              "        \\text{Prediction\\,:}\\;   & {}\\cancel{2}\\, x_{0}^{2} +\\cancel{5}\\, x_{0}\n",
              "        \\end{aligned}$"
            ],
            "text/plain": [
              "<IPython.core.display.Math object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "gen_texts, ref_texts = load_eval_results(\"results/eval_results.json\")\n",
        "\n",
        "success_cases = [(i, gen, ref) for i, (gen, ref) in enumerate(zip(gen_texts, ref_texts)) if gen == ref]\n",
        "failure_cases = [(i, gen, ref) for i, (gen, ref) in enumerate(zip(gen_texts, ref_texts)) if gen != ref]\n",
        "\n",
        "num_show = 5\n",
        "\n",
        "print('-------------------------')\n",
        "print(''' Success cases ''')\n",
        "print('-------------------------')\n",
        "for (i, gen, ref) in success_cases[:num_show]:\n",
        "    gen_expr = test_dataset.preprocessor.to_original(gen)\n",
        "    ref_expr = test_dataset.preprocessor.to_original(ref)\n",
        "\n",
        "    print(f\"===== sample id: {i+1} =====\")\n",
        "    display_with_diff(ref_expr, gen_expr)\n",
        "\n",
        "\n",
        "\n",
        "print('\\n-------------------------')\n",
        "print(''' Failure cases ''')\n",
        "print('-------------------------')\n",
        "for (i, gen, ref) in failure_cases[:num_show]:\n",
        "    gen_expr = test_dataset.preprocessor.to_original(gen)\n",
        "    ref_expr = test_dataset.preprocessor.to_original(ref)\n",
        "\n",
        "    print(f\"===== sample id: {i+1} =====\")\n",
        "    display_with_diff(ref_expr, gen_expr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
