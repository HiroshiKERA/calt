{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CALT: Computer ALgebra with Transformer","text":"<p>(Note: This project is currently in its initial development phase. The file structure and content are subject to significant changes. Please ensure you are referring to the latest version when using it.)</p>"},{"location":"#overview","title":"Overview","text":"<p><code>calt</code> is a simple Python library for learning arithmetic and symbolic computation with a Transformer model (a deep neural model to realize sequece-to-sequence functions). </p> <p>It offers a basic Transformer model and training, and non-experts of deep learning (e.g., mathematicians) can focus on constructing datasets to train and evaluate the model. Particularly, users only need to implement an instance generator for their own task.</p> <p>For example, for the polynomial addition task, the following will work. <pre><code>class SumProblemGenerator:\n    # Task - input: F=[f_1, ..., f_s], target: G=[g:= f_1+...+f_s]\n    def __init__(\n        self, sampler: PolynomialSampler, max_polynomials: int, min_polynomials: int\n    ):\n        self.sampler = sampler\n        self.max_polynomials = max_polynomials  \n        self.min_polynomials = min_polynomials\n\n    def __call__(self, seed: int) -&gt; Tuple[List[PolyElement], PolyElement]:\n        random.seed(seed) # Set random seed\n        num_polys = random.randint(self.min_polynomials, self.max_polynomials) \n\n        F = self.sampler.sample(num_samples=num_polys)\n        g = sum(F)\n\n        return F, g\n</code></pre></p> <p>Then, <code>calt</code> calls this in parallel to efficiently construct a large dataset and then train a Transformer model to learn this computation. For hard problems, the sample generation itself can suggest unexplored problems, and one can study theoretical and algorithmic solutions of them. The following is a small list of such studies from our group. </p> <ul> <li>\"Learning to Compute Gr\u00f6bner Bases,\" Kera et al., 2024</li> <li>\"Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms,\" Kera and Pelleriti et al., 2025</li> <li>\"Geometric Generality of Transformer-Based Gr\u00f6bner Basis Computation,\" Kambe et al., 2025</li> </ul> <p>Refer to our paper \"CALT: A Library for Computer Algebra with Transformer,\" Kera et al., 2025 for a comprehensive overview.</p>"},{"location":"#installation","title":"Installation","text":"<p><code>calt-x</code> is available on PyPI. You can install it with:</p> <pre><code>pip install calt-x\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python \u2265 3.10</li> </ul>"},{"location":"#weights-biases-wandb-setup","title":"Weights &amp; Biases (wandb) Setup","text":"<p>If you are using Weights &amp; Biases (wandb) for the first time to log training progress, you will need to create an account on their website and set up your API key. When you run the training script for the first time, you will be prompted to enter your API key.</p> <p>https://wandb.ai/site/</p>"},{"location":"#demos-and-tutorials","title":"Demos and Tutorials","text":"<p>Simple demonstrations for data generation and training are available as Jupyter Notebook files. You can find them in the <code>notebooks</code> directory</p> <p></p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this code in your research, please cite our paper:</p> <pre><code>@misc{kera2025calt,\n  title={CALT: A Library for Computer Algebra with Transformer},\n  author={Hiroshi Kera and Shun Arawaka and Yuta Sato},\n  year={2025},\n  archivePrefix={arXiv},\n  eprint={2506.08600}\n}\n</code></pre>"}]}