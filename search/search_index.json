{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CALT: Computer ALgebra with Transformer","text":"<p>CALT is a simple Python library for learning arithmetic and symbolic computation with Transformer models. It offers a basic Transformer model and training utilities so that non-experts in deep learning (e.g., mathematicians) can focus on constructing datasets and defining tasks.</p> <p>The library is organised around three main pipelines:</p> <ul> <li>Dataset pipeline \u2013 generate paired problems/answers with SageMath or SymPy backends.</li> <li>IO pipeline \u2013 tokenise text and build datasets and collators from configuration.</li> <li>Trainer pipeline \u2013 build and run HuggingFace <code>Trainer</code> instances from YAML configs.</li> </ul> <p>For most users, the recommended entry point is to start from one of the example tasks under <code>calt/examples/*</code> and customise only the dataset generator and configuration files.</p>"},{"location":"#documentation-map","title":"Documentation map","text":"<ul> <li>Dataset pipeline \u2013 dataset generation backends and <code>DatasetPipeline</code>.<ul> <li>Overview (includes DatasetWriter), SageMath backend, SymPy backend</li> </ul> </li> <li>IO pipeline \u2013 tokenisation and <code>lexer.yaml</code> configuration.<ul> <li>Overview, Lexer and vocabulary, Load preprocessors, Visualization</li> </ul> </li> <li>Trainer pipeline \u2013 model and high-level training flow.<ul> <li>Overview, Model pipeline</li> </ul> </li> <li>Configuration \u2013 how <code>data.yaml</code>, <code>lexer.yaml</code>, and <code>train.yaml</code> work together.<ul> <li>Configuration</li> </ul> </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>CALT can be installed via <code>pip</code>:</p> <pre><code>pip install calt-x\n</code></pre> <p>We highly recommend using the CALT codebase \u2013 a comprehensive template repository to build your own projects using CALT. The quickstart guide can be found in the CALT codebase documentation.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this code in your research, please cite our paper:</p> <pre><code>@misc{kera2025calt,\n  title={CALT: A Library for Computer Algebra with Transformer},\n  author={Hiroshi Kera and Shun Arawaka and Yuta Sato},\n  year={2025},\n  archivePrefix={arXiv},\n  eprint={2506.08600}\n}\n</code></pre> <p>The following is a small list of related studies from our group:</p> <ul> <li>\"Learning to Compute Gr\u00f6bner Bases,\" Kera et al., 2024</li> <li>\"Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms,\" Kera and Pelleriti et al., 2025</li> <li>\"Geometric Generality of Transformer-Based Gr\u00f6bner Basis Computation,\" Kambe et al., 2025</li> </ul> <p>Refer to our paper \"CALT: A Library for Computer Algebra with Transformer,\" Kera et al., 2025 for a comprehensive overview.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Example tasks in <code>calt/examples/*</code> share a common configuration pattern based on three YAML files:</p> <ul> <li>configs/data.yaml \u2013 controls dataset generation via <code>DatasetPipeline</code>.</li> <li>configs/train.yaml \u2013 main control file for the model, training loop, and WandB logging via   <code>ModelPipeline</code> and <code>TrainerPipeline</code> (references <code>lexer.yaml</code> in its <code>data</code> block).</li> <li>configs/lexer.yaml \u2013 controls tokenisation and vocabulary via <code>IOPipeline</code>; path set in <code>train.yaml</code>\u2019s <code>data.lexer_config</code>.</li> </ul> <p>All three are loaded with <code>OmegaConf.load</code> and passed around as <code>omegaconf.DictConfig</code> objects, so they support dot-style access (e.g. <code>cfg.data</code>, <code>cfg.model</code>, <code>cfg.train</code>). WandB configuration can be included in <code>cfg.train.wandb</code> or passed separately as <code>cfg.wandb</code>.</p>"},{"location":"configuration/#datayaml-dataset-generation-datasetpipeline","title":"<code>data.yaml</code> \u2013 dataset generation (<code>DatasetPipeline</code>)","text":"<p>Example tasks under <code>calt/examples/*</code> use <code>configs/data.yaml</code> to drive dataset generation through DatasetPipeline. Typical usage:</p> <pre><code>from omegaconf import OmegaConf\nfrom calt.dataset import DatasetPipeline\n\ncfg = OmegaConf.load(\"configs/data.yaml\")\npipeline = DatasetPipeline.from_config(\n    cfg.dataset,\n    instance_generator=my_instance_generator,\n    statistics_calculator=None,\n)\npipeline.run()\n</code></pre> <p>The <code>dataset</code> block in <code>data.yaml</code> controls all dataset-generation behaviour. Example:</p> <pre><code>dataset:\n  save_dir: \"./data\"\n  num_train_samples: 100000\n  num_test_samples: 1000\n  batch_size: 10000\n  n_jobs: 4\n  root_seed: 42\n  verbose: true\n  backend: \"sagemath\"\n  save_text: true\n  save_json: false\n</code></pre> <code>dataset</code> \u2014 Passed to DatasetPipeline.from_config Name Description <code>save_dir</code> Base directory where all splits (train/test/\u2026) are written. <code>num_train_samples</code> Number of training samples to generate (size of the <code>\"train\"</code> split). <code>num_test_samples</code> Number of test samples to generate (size of the <code>\"test\"</code> split). <code>batch_size</code> Batch size passed to <code>DatasetGenerator.run</code> for efficient multiprocessing. <code>n_jobs</code> Number of worker processes used by the generator (<code>backend=\"multiprocessing\"</code>). <code>root_seed</code> Global seed used to derive per-sample seeds in the backend. <code>verbose</code> Whether the generator prints progress information. <code>backend</code> Which implementation to use: <code>\"sagemath\"</code> \u2192 SageMath backend, <code>\"sympy\"</code> \u2192 SymPy backend. <code>save_text</code> Whether to write human-readable <code>.txt</code> files (<code>text_raw.txt</code>, etc.). <code>save_json</code> Whether to write <code>.jsonl</code> files preserving the nested Python structure. <p>Under the hood, DatasetPipeline resolves the appropriate <code>DatasetGenerator</code> for the chosen <code>backend</code> and uses the common DatasetWriter; both are configured from the same options.</p>"},{"location":"configuration/#trainyaml-model-and-training-modelpipeline-trainerpipeline","title":"<code>train.yaml</code> \u2013 model and training (<code>ModelPipeline</code>, <code>TrainerPipeline</code>)","text":"<p>Example tasks under <code>calt/examples/*</code> use <code>configs/train.yaml</code> as the main control file for experiments. It is loaded with <code>OmegaConf.load</code> and passed to IOPipeline, ModelPipeline, and TrainerPipeline. Typical usage:</p> <pre><code>from omegaconf import OmegaConf\nfrom calt.io import IOPipeline\nfrom calt.models import ModelPipeline\nfrom calt.trainer import TrainerPipeline\n\ncfg = OmegaConf.load(\"configs/train.yaml\")\n\nio_dict = IOPipeline.from_config(cfg.data).build()\nmodel = ModelPipeline.from_io_dict(cfg.model, io_dict).build()\ntrainer_pipeline = TrainerPipeline.from_io_dict(cfg.train, model, io_dict).build()\n\ntrainer_pipeline.train()\nsuccess_rate = trainer_pipeline.evaluate_and_save_generation()\nprint(f\"Success rate: {100 * success_rate:.1f}%\")\n</code></pre> <p>The top-level blocks in <code>train.yaml</code> are <code>data</code>, <code>model</code>, <code>train</code>, and optionally <code>wandb</code> (under <code>train</code> or separate). Example:</p> <pre><code>data:\n  train_dataset_path: ./data/train_raw.txt\n  test_dataset_path: ./data/test_raw.txt\n  lexer_config: ./configs/lexer.yaml\n  num_train_samples: -1\n  num_test_samples: -1\n  validate_train_tokens: true\n  validate_test_tokens: true\n  display_samples: 5\n\nmodel:\n  model_type: generic\n  num_encoder_layers: 6\n  num_encoder_heads: 8\n  num_decoder_layers: 6\n  num_decoder_heads: 8\n  d_model: 512\n  encoder_ffn_dim: 2048\n  decoder_ffn_dim: 2048\n  max_sequence_length: 512\n\ntrain:\n  save_dir: ./results\n  num_train_epochs: 100\n  learning_rate: 0.0001\n  weight_decay: 0.01\n  warmup_ratio: 0.1\n  batch_size: 16\n  test_batch_size: 16\n  seed: 42\n  wandb:\n    project: calt\n    group: gf17_addition\n    name: gf17_addition\n</code></pre> <code>data</code> \u2014 Passed to IOPipeline.from_config Name Description <code>lexer_config</code> Path to <code>lexer.yaml</code> (required). <code>train_dataset_path</code> Path to training raw text file. <code>test_dataset_path</code> Path to test raw text file. <code>num_train_samples</code> Number of samples to load for training (-1 = all). <code>num_test_samples</code> Number of samples to load for evaluation (-1 = all). <code>validate_train_tokens</code> Whether to validate that all training tokens are in vocab (default false). <code>validate_test_tokens</code> Whether to validate test tokens (default true). <code>display_samples</code> Number of sample lines to print when loading (0 to disable). <code>use_jsonl</code> If true, load from JSONL instead of raw text. <code>use_pickle</code> If true, load from pickle. <code>train_dataset_jsonl</code>, <code>test_dataset_jsonl</code> Paths when using JSONL. <code>train_dataset_pickle</code>, <code>test_dataset_pickle</code> Paths when using pickle. <code>dataset_load_preprocessor</code> Optional preprocessor for custom loading. <code>model</code> \u2014 Passed to ModelPipeline.from_io_dict Name Description <code>model_type</code> Model architecture (e.g. <code>generic</code>, <code>bart</code>). <code>num_encoder_layers</code>, <code>num_encoder_heads</code> Encoder depth and attention heads. <code>num_decoder_layers</code>, <code>num_decoder_heads</code> Decoder depth and attention heads. <code>d_model</code> Hidden size (embedding dimension). <code>encoder_ffn_dim</code>, <code>decoder_ffn_dim</code> Feed-forward dimension in encoder/decoder. <code>max_sequence_length</code> Maximum sequence length. <code>train</code> \u2014 Converted to TrainingArguments by TrainerPipeline Name Description <code>save_dir</code> Output directory for checkpoints and logs. Passed to HuggingFace <code>TrainingArguments.output_dir</code>. If omitted, <code>output_dir</code> is used; if both are missing, defaults to <code>\"./tmp\"</code>. <code>output_dir</code> Alias for <code>save_dir</code>. Used when <code>save_dir</code> is not set. <code>num_train_epochs</code> Number of training epochs. <code>learning_rate</code> Learning rate for the optimizer. <code>weight_decay</code> Weight decay (L2 penalty) coefficient. <code>warmup_ratio</code> Fraction of training steps used for a linear warmup from 0 to <code>learning_rate</code> (0 to 1). <code>batch_size</code> Per-device training batch size. With multiple GPUs, this is divided by the number of devices and passed as <code>per_device_train_batch_size</code>. <code>test_batch_size</code> Per-device evaluation batch size. Similarly passed as <code>per_device_eval_batch_size</code>. <code>lr_scheduler_type</code> Learning rate schedule. Use <code>\"linear\"</code> or <code>\"constant\"</code>. Defaults to <code>\"linear\"</code> if not set. <code>max_grad_norm</code> Maximum gradient norm for clipping. <code>optimizer</code> Optimizer name (passed to HuggingFace <code>optim</code>). <code>num_workers</code> Number of DataLoader worker processes (<code>dataloader_num_workers</code>). <code>dataloader_pin_memory</code> DataLoader <code>pin_memory</code> option. Defaults to <code>true</code>. <code>eval_strategy</code> When to run evaluation (e.g. <code>\"steps\"</code>, <code>\"epoch\"</code>). Defaults to <code>\"steps\"</code>. <code>eval_steps</code> Run evaluation every this many steps when <code>eval_strategy</code> is <code>\"steps\"</code>. Defaults to <code>1000</code>. <code>save_strategy</code> When to save checkpoints (e.g. <code>\"steps\"</code>, <code>\"epoch\"</code>). Defaults to <code>\"steps\"</code>. <code>save_steps</code> Save a checkpoint every this many steps when <code>save_strategy</code> is <code>\"steps\"</code>. Defaults to <code>1000</code>. <code>save_total_limit</code> Maximum number of checkpoints to keep; older ones are removed. Defaults to <code>1</code>. <code>save_safetensors</code> If <code>true</code>, save model weights in safetensors format. Defaults to <code>false</code>. <code>label_names</code> List of label keys used by the Trainer. Defaults to <code>[\"labels\"]</code>. <code>logging_strategy</code> When to log (e.g. <code>\"steps\"</code>, <code>\"epoch\"</code>). Defaults to <code>\"steps\"</code>. <code>logging_steps</code> Log every this many steps when <code>logging_strategy</code> is <code>\"steps\"</code>. Defaults to <code>50</code>. <code>seed</code> Random seed for reproducibility. <code>remove_unused_columns</code> Whether to drop dataset columns not used by the model. Defaults to <code>false</code>. <code>disable_tqdm</code> Whether to disable the progress bar. Defaults to <code>true</code>. <code>train.wandb</code> (or top-level <code>wandb</code>) \u2014 Used by TrainerPipeline for Weights &amp; Biases Name Description <code>project</code> Project name on WandB. <code>group</code> Logical experiment group (e.g. task name). <code>name</code> Run identifier shown in the UI. <code>tags</code> Optional list of tags. <code>no_wandb</code> If true, disable WandB logging. <p>The trainer pipeline sets the corresponding environment variables and ensures <code>TrainingArguments.report_to</code> includes <code>\"wandb\"</code> when WandB is configured.</p>"},{"location":"configuration/#lexeryaml-io-and-vocabulary-iopipeline","title":"<code>lexer.yaml</code> \u2013 IO and vocabulary (<code>IOPipeline</code>)","text":"<p>The <code>data</code> block in <code>train.yaml</code> points to <code>lexer.yaml</code> via <code>data.lexer_config</code>. That file controls tokenisation and vocabulary and is loaded by IOPipeline.from_config. Typical usage:</p> <pre><code>from omegaconf import OmegaConf\nfrom calt.io import IOPipeline\n\ncfg = OmegaConf.load(\"configs/train.yaml\")\nio_pipeline = IOPipeline.from_config(cfg.data)  # loads lexer_config from cfg.data.lexer_config\n</code></pre> <p>The top-level keys in <code>lexer.yaml</code> control vocabulary and number tokenisation. Example:</p> <pre><code>vocab:\n  range:\n    numbers: [\"\", 0, 16]\n  misc: [\"+\", \"*\", \"^\", \"(\", \")\",\"|\", \"-\"]\n  special_tokens: {}\n  flags:\n    include_base_vocab: true\n    include_base_special_tokens: true\n\nnumber_policy:\n  attach_sign: true\n  digit_group: 0\n  allow_float: false\n\nstrict: true\ninclude_base_vocab: true\n</code></pre> <code>vocab</code> \u2014 Passed to VocabConfig.from_config Name Description <code>range</code> Dict of arbitrary key \u2192 <code>[prefix, min, max]</code> (inclusive). Each entry expands to tokens <code>prefix+str(i)</code> for <code>i</code> in <code>min..max</code>. For example, <code>numbers: [\"\", 0, 16]</code>, <code>coefficients: [\"C\", -50, 50]</code>, <code>exponents: [\"E\", 0, 20]</code>, <code>variables: [\"x\", 0, 2]</code>. <code>misc</code> List of extra tokens (e.g. <code>[\"+\", \"=\", \",\"]</code>). <code>special_tokens</code> Dict of special token names. Base special tokens are defined in code. <code>flags</code> Optional. <code>include_base_vocab</code>, <code>include_base_special_tokens</code> (both default true). <code>number_policy</code> \u2014 Builds NumberPolicy for UnifiedLexer Name Description <code>attach_sign</code> bool (default true). true = sign is part of the number token; false = sign is a separate token. <code>digit_group</code> int (default 0). 0 = no digit grouping; d \u2265 1 = split number into tokens of d digits. <code>allow_float</code> bool (default true). Whether to allow decimal numbers (adds <code>\".\"</code> to vocab if needed). <code>strict</code>, <code>include_base_vocab</code> \u2014 Passed to UnifiedLexer Name Description <code>strict</code> bool (default true). If true, raise an error on unknown characters; if false, emit the unknown token (e.g. <code>&lt;unk&gt;</code>) and continue. <code>include_base_vocab</code> bool (default true). If true, add built-in tokens (separators, operators <code>+</code>, <code>-</code>, <code>*</code>, brackets, etc.) to the lexer\u2019s reserved set; if false, only tokens from <code>vocab</code> are used. <p>Under the hood, IOPipeline instantiates <code>UnifiedLexer</code> and <code>VocabConfig</code> from this configuration, then builds a HuggingFace-compatible tokenizer, tokenised datasets, and a <code>StandardDataCollator</code>. See Lexer and vocabulary for the API of <code>UnifiedLexer</code>, <code>NumberPolicy</code>, and <code>VocabConfig</code>.</p>"},{"location":"dataset_generator/","title":"Overview","text":"<p>A unified interface with SageMath and SymPy backends for large-scale dataset generation. It produces paired problems and answers, supports batch writing, and computes incremental statistics.</p> <p>DatasetPipeline and DatasetWriter are shared regardless of backend (<code>sagemath</code> or <code>sympy</code>). For details on each component, see:</p> <ul> <li>DatasetWriter \u2014 writing samples to disk</li> <li>SageMath backend \u2014 <code>DatasetGenerator</code> and <code>PolynomialSampler</code> for SageMath</li> <li>SymPy backend \u2014 <code>DatasetGenerator</code> and <code>PolynomialSampler</code> for SymPy</li> </ul> <p>Configuration for the dataset pipeline is done via the <code>dataset</code> block in <code>data.yaml</code>. For the option list, usage example, and YAML sample, see Configuration.</p>"},{"location":"dataset_generator/#calt.dataset.pipeline.DatasetPipeline","title":"DatasetPipeline","text":"<pre><code>DatasetPipeline(\n    instance_generator,\n    statistics_calculator,\n    save_dir: str,\n    save_text: bool,\n    save_json: bool,\n    num_train_samples: int,\n    num_test_samples: int,\n    batch_size: int,\n    n_jobs: int,\n    root_seed: int,\n    verbose: bool,\n    backend: str = \"sagemath\",\n)\n</code></pre> <p>Pipeline for generating train/test datasets with a configurable backend.</p> <p>Uses an instance generator and optional statistics calculator to produce batches, then writes them to disk via the backend's DatasetWriter. Typically constructed via from_config() with a DictConfig (e.g. from YAML).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from omegaconf import OmegaConf\n&gt;&gt;&gt; from calt.dataset import DatasetPipeline\n&gt;&gt;&gt; cfg = OmegaConf.load(\"configs/dataset.yaml\")\n&gt;&gt;&gt; pipeline = DatasetPipeline.from_config(\n...     cfg.dataset,\n...     instance_generator=my_instance_generator,\n...     statistics_calculator=my_stats_fn,\n... )\n&gt;&gt;&gt; pipeline.run()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>instance_generator</code> <p>Callable that takes a single integer seed and returns (problem, answer). Used to generate each sample.</p> required <code>statistics_calculator</code> <p>Optional callable(problem, answer) returning a dict of per-sample statistics (e.g. {\"problem\": {...}, \"answer\": {...}}). Pass None to skip statistics.</p> required <code>save_dir</code> <code>str</code> <p>Directory path to write dataset files.</p> required <code>save_text</code> <code>bool</code> <p>Whether to save samples as text files.</p> required <code>save_json</code> <code>bool</code> <p>Whether to save metadata (e.g. statistics) as JSON.</p> required <code>num_train_samples</code> <code>int</code> <p>Number of training samples to generate.</p> required <code>num_test_samples</code> <code>int</code> <p>Number of test samples to generate.</p> required <code>batch_size</code> <code>int</code> <p>Number of samples per batch during generation.</p> required <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for the backend generator.</p> required <code>root_seed</code> <code>int</code> <p>Base seed for reproducibility; job seeds are derived from this.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print progress.</p> required <code>backend</code> <code>str</code> <p>Backend name for generation and writing (\"sagemath\" or \"sympy\").</p> <code>'sagemath'</code> Source code in <code>src/calt/dataset/pipeline.py</code> <pre><code>def __init__(\n    self,\n    instance_generator,\n    statistics_calculator,\n    save_dir: str,\n    save_text: bool,\n    save_json: bool,\n    num_train_samples: int,\n    num_test_samples: int,\n    batch_size: int,\n    n_jobs: int,\n    root_seed: int,\n    verbose: bool,\n    backend: str = \"sagemath\",\n) -&gt; None:\n    \"\"\"Initialize the dataset pipeline.\n\n    Args:\n        instance_generator: Callable that takes a single integer seed and\n            returns (problem, answer). Used to generate each sample.\n        statistics_calculator: Optional callable(problem, answer) returning\n            a dict of per-sample statistics (e.g. {\"problem\": {...}, \"answer\": {...}}).\n            Pass None to skip statistics.\n        save_dir: Directory path to write dataset files.\n        save_text: Whether to save samples as text files.\n        save_json: Whether to save metadata (e.g. statistics) as JSON.\n        num_train_samples: Number of training samples to generate.\n        num_test_samples: Number of test samples to generate.\n        batch_size: Number of samples per batch during generation.\n        n_jobs: Number of parallel jobs for the backend generator.\n        root_seed: Base seed for reproducibility; job seeds are derived from this.\n        verbose: Whether to print progress.\n        backend: Backend name for generation and writing (\"sagemath\" or \"sympy\").\n    \"\"\"\n    self.instance_generator = instance_generator\n    self.statistics_calculator = statistics_calculator\n    self.save_dir = save_dir\n    self.save_text = save_text\n    self.save_json = save_json\n    self.num_train_samples = num_train_samples\n    self.num_test_samples = num_test_samples\n    self.batch_size = batch_size\n    self.n_jobs = n_jobs\n    self.root_seed = root_seed\n    self.verbose = verbose\n    self.backend = backend\n</code></pre>"},{"location":"dataset_generator/#calt.dataset.pipeline.DatasetPipeline.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: DictConfig, instance_generator, statistics_calculator=None\n) -&gt; \"DatasetPipeline\"\n</code></pre> <p>Build a DatasetPipeline from a DictConfig.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>DictConfig for the dataset. Expected keys: save_dir, num_train_samples, num_test_samples, batch_size, n_jobs, root_seed. Optional (with defaults): save_text=True, save_json=True, verbose=True, backend=\"sagemath\". Missing required keys will raise when building the pipeline.</p> required <code>instance_generator</code> <p>Callable(seed) -&gt; (problem, answer). Required.</p> required <code>statistics_calculator</code> <p>Optional callable(problem, answer) -&gt; dict. Defaults to None (no per-sample statistics).</p> <code>None</code> <p>Returns:</p> Type Description <code>'DatasetPipeline'</code> <p>DatasetPipeline instance configured with config and the given callables.</p> Source code in <code>src/calt/dataset/pipeline.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: DictConfig,\n    instance_generator,\n    statistics_calculator=None,\n) -&gt; \"DatasetPipeline\":\n    \"\"\"Build a DatasetPipeline from a DictConfig.\n\n    Args:\n        config: DictConfig for the dataset. Expected keys: save_dir,\n            num_train_samples, num_test_samples, batch_size, n_jobs, root_seed.\n            Optional (with defaults): save_text=True, save_json=True,\n            verbose=True, backend=\"sagemath\".\n            Missing required keys will raise when building the pipeline.\n        instance_generator: Callable(seed) -&gt; (problem, answer). Required.\n        statistics_calculator: Optional callable(problem, answer) -&gt; dict.\n            Defaults to None (no per-sample statistics).\n\n    Returns:\n        DatasetPipeline instance configured with config and the given callables.\n    \"\"\"\n    return cls(\n        instance_generator=instance_generator,\n        statistics_calculator=statistics_calculator,\n        save_dir=config.save_dir,\n        save_text=getattr(config, \"save_text\", True),\n        save_json=getattr(config, \"save_json\", True),\n        num_train_samples=config.num_train_samples,\n        num_test_samples=config.num_test_samples,\n        batch_size=config.batch_size,\n        n_jobs=config.n_jobs,\n        root_seed=config.root_seed,\n        verbose=getattr(config, \"verbose\", True),\n        backend=getattr(config, \"backend\", \"sagemath\"),\n    )\n</code></pre>"},{"location":"dataset_generator/#calt.dataset.pipeline.DatasetPipeline.run","title":"run","text":"<pre><code>run() -&gt; None\n</code></pre> <p>Run the pipeline: generate train/test data and write to save_dir.</p> <p>Loads the backend (DatasetGenerator and DatasetWriter), then runs batch generation with the instance_generator and statistics_calculator, and writes outputs according to save_text and save_json.</p> Source code in <code>src/calt/dataset/pipeline.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Run the pipeline: generate train/test data and write to save_dir.\n\n    Loads the backend (DatasetGenerator and DatasetWriter), then runs\n    batch generation with the instance_generator and statistics_calculator,\n    and writes outputs according to save_text and save_json.\n    \"\"\"\n    DatasetGenerator, DatasetWriter = get_backend_classes(self.backend)\n\n    # Initialize dataset generator\n    dataset_generator = DatasetGenerator(\n        backend=\"multiprocessing\",\n        n_jobs=self.n_jobs,\n        verbose=self.verbose,\n        root_seed=self.root_seed,\n    )\n\n    # Initialize writer\n    dataset_writer = DatasetWriter(\n        save_dir=self.save_dir,\n        save_text=self.save_text,\n        save_json=self.save_json,\n    )\n\n    # Generate datasets with batch processing\n    dataset_generator.run(\n        dataset_sizes={\n            \"train\": self.num_train_samples,\n            \"test\": self.num_test_samples,\n        },\n        batch_size=self.batch_size,\n        instance_generator=self.instance_generator,\n        statistics_calculator=self.statistics_calculator,\n        dataset_writer=dataset_writer,\n    )\n</code></pre>"},{"location":"dataset_generator/#calt.dataset.utils.dataset_writer.DatasetWriter","title":"DatasetWriter","text":"<pre><code>DatasetWriter(\n    save_dir: str | None = None, save_text: bool = True, save_json: bool = True\n)\n</code></pre> <p>Dataset writer for saving problem-answer pairs in multiple formats.</p> <p>This class handles saving datasets with nested structure support up to 2 levels. It can save data in raw text (.txt) and JSON Lines (.jsonl) formats.</p> <p>Attributes:</p> Name Type Description <code>INNER_SEP</code> <code>str</code> <p>Separator for single-level lists (\" | \")</p> <code>OUTER_SEP</code> <code>str</code> <p>Separator for nested lists (\" || \")</p> <code>save_dir</code> <code>Path</code> <p>Base directory for saving datasets</p> <code>save_text</code> <code>bool</code> <p>Whether to save raw text files</p> <code>save_json</code> <code>bool</code> <p>Whether to save JSON Lines files</p> <code>_file_handles</code> <code>dict</code> <p>Dictionary to store open file handles</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str | None</code> <p>Base directory for saving datasets. If None, uses current working directory.</p> <code>None</code> <code>save_text</code> <code>bool</code> <p>Whether to save raw text files. Text files use \"#\" as separator       between problem and answer, with nested structures joined by separators.</p> <code>True</code> <code>save_json</code> <code>bool</code> <p>Whether to save JSON Lines files. JSON Lines files preserve the original       nested structure format, with one sample per line.</p> <code>True</code> Usage <pre><code># Efficient batch processing with file handle management\nwriter = DatasetWriter(save_dir=\"./datasets\")\nwriter.open(\"train\")  # Open file handles once\ntry:\n    for batch_idx, samples in enumerate(batches):\n        writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx)\nfinally:\n    writer.close(\"train\")  # Close file handles\n\n# Or use context manager\nwith DatasetWriter(save_dir=\"./datasets\") as writer:\n    writer.open(\"train\")\n    for batch_idx, samples in enumerate(batches):\n        writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx)\n    writer.close(\"train\")\n\n# Support for various dataset splits\nwriter.open(\"validation\")  # Validation set\nwriter.open(\"dev\")         # Development set\nwriter.open(\"eval\")        # Evaluation set\n</code></pre> Source code in <code>src/calt/dataset/utils/dataset_writer.py</code> <pre><code>def __init__(\n    self,\n    save_dir: str | None = None,\n    save_text: bool = True,\n    save_json: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize dataset writer.\n\n    Args:\n        save_dir: Base directory for saving datasets. If None, uses current working directory.\n        save_text: Whether to save raw text files. Text files use \"#\" as separator\n                  between problem and answer, with nested structures joined by separators.\n        save_json: Whether to save JSON Lines files. JSON Lines files preserve the original\n                  nested structure format, with one sample per line.\n\n    Usage:\n        ```python\n        # Efficient batch processing with file handle management\n        writer = DatasetWriter(save_dir=\"./datasets\")\n        writer.open(\"train\")  # Open file handles once\n        try:\n            for batch_idx, samples in enumerate(batches):\n                writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx)\n        finally:\n            writer.close(\"train\")  # Close file handles\n\n        # Or use context manager\n        with DatasetWriter(save_dir=\"./datasets\") as writer:\n            writer.open(\"train\")\n            for batch_idx, samples in enumerate(batches):\n                writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx)\n            writer.close(\"train\")\n\n        # Support for various dataset splits\n        writer.open(\"validation\")  # Validation set\n        writer.open(\"dev\")         # Development set\n        writer.open(\"eval\")        # Evaluation set\n        ```\n    \"\"\"\n    self.save_dir = Path(save_dir) if save_dir else Path.cwd()\n    self.save_text = save_text\n    self.save_json = save_json\n    self.logger = logging.getLogger(__name__)\n    self._file_handles: dict[\n        str, dict[str, Any]\n    ] = {}  # {tag: {file_type: file_handle}}\n    TimedeltaDumper.add_representer(timedelta, timedelta_representer)\n</code></pre>"},{"location":"dataset_sagemath/","title":"SageMath backend","text":"<p>When using <code>backend=\"sagemath\"</code>, the following classes are used for generation and sampling. You can also use them directly without DatasetPipeline.</p> <p>See Dataset Generator (Overview) for the pipeline and <code>data.yaml</code> configuration.</p>"},{"location":"dataset_sagemath/#calt.dataset.sagemath.dataset_generator.DatasetGenerator","title":"DatasetGenerator","text":"<pre><code>DatasetGenerator(\n    backend: str = \"multiprocessing\",\n    n_jobs: int = -1,\n    verbose: bool = True,\n    root_seed: int = 42,\n)\n</code></pre> <p>Base class for instance generators</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Backend for parallel processing</p> <code>'multiprocessing'</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs (-1 for all cores)</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to display progress information</p> <code>True</code> <code>root_seed</code> <code>int</code> <p>Root seed for reproducibility</p> <code>42</code> Source code in <code>src/calt/dataset/sagemath/dataset_generator.py</code> <pre><code>def __init__(\n    self,\n    backend: str = \"multiprocessing\",\n    n_jobs: int = -1,\n    verbose: bool = True,\n    root_seed: int = 42,\n):\n    \"\"\"\n    Initialize instance generator.\n\n    Args:\n        backend: Backend for parallel processing\n        n_jobs: Number of parallel jobs (-1 for all cores)\n        verbose: Whether to display progress information\n        root_seed: Root seed for reproducibility\n    \"\"\"\n\n    self.backend = backend\n    self.n_jobs = n_jobs\n    self.verbose = verbose\n    self.root_seed = root_seed\n\n    # Configure logging only once at initialization\n    self.logger = logger\n\n    # Configure joblib logging to show progress but not overwhelm\n    # Only set if not already configured\n    joblib_logger = logging.getLogger(\"joblib\")\n    if joblib_logger.level == logging.NOTSET:\n        joblib_logger.setLevel(logging.INFO)\n\n    parallel_logger = logging.getLogger(\"joblib.Parallel\")\n    if parallel_logger.level == logging.NOTSET:\n        parallel_logger.setLevel(logging.INFO)\n</code></pre>"},{"location":"dataset_sagemath/#calt.dataset.sagemath.dataset_generator.DatasetGenerator.run","title":"run","text":"<pre><code>run(\n    dataset_sizes: dict[str, int],\n    instance_generator: Callable,\n    statistics_calculator: Callable | None = None,\n    dataset_writer: DatasetWriter | None = None,\n    batch_size: int = 100000,\n    save_dir: str | None = None,\n    save_text: bool = True,\n    save_json: bool = True,\n)\n</code></pre> <p>Generate multiple datasets using parallel processing with batch writing.</p> <p>This is the main entry point for dataset generation. It supports generating multiple datasets (train/test) simultaneously or separately, with efficient memory management through batch processing and parallel execution.</p> <p>Key features:</p> <ul> <li>Parallel processing using joblib for high performance</li> <li>Batch-based memory management to handle large datasets</li> <li>Incremental statistics calculation to avoid memory issues</li> <li>Reproducible generation with unique seeds for each sample</li> <li>Support for nested data structures (up to 2 levels)</li> <li>Multiple output formats (text, JSON Lines) via DatasetWriter</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset_sizes</code> <code>dict[str, int]</code> <p>Dictionary mapping dataset names to number of samples.           Any string can be used as dataset name (e.g., \"train\", \"test\", \"validation\").           Duplicate names are not allowed.           Example: {\"train\": 100000, \"test\": 1000} or {\"train\": 100000, \"validation\": 5000}</p> required <code>instance_generator</code> <code>Callable</code> <p>Function that generates (problem, answer) pair given a seed.              Must accept a single integer seed parameter.</p> required <code>statistics_calculator</code> <code>Callable | None</code> <p>Optional function to calculate sample-specific statistics.                  Must accept (problem, answer) and return dict or None.</p> <code>None</code> <code>dataset_writer</code> <code>DatasetWriter | None</code> <p>DatasetWriter object for saving datasets to files.           If None, a new DatasetWriter will be created using save_dir, save_text, and save_json parameters.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of samples to process in each batch. Larger batches        use more memory but may be more efficient for I/O operations.</p> <code>100000</code> <code>save_dir</code> <code>str | None</code> <p>Base directory for saving datasets. Used only if dataset_writer is None.      If None, uses current working directory.</p> <code>None</code> <code>save_text</code> <code>bool</code> <p>Whether to save raw text files. Used only if dataset_writer is None.       Text files use \"#\" as separator between problem and answer.</p> <code>True</code> <code>save_json</code> <code>bool</code> <p>Whether to save JSON Lines files. Used only if dataset_writer is None.       JSON Lines files preserve the original nested structure format.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset_sizes is invalid or instance_generator is None</p> <code>Exception</code> <p>If parallel processing fails</p> Note <ul> <li>Each sample gets a unique seed for reproducibility</li> <li>Progress is logged if verbose=True (set in init)</li> <li>Memory usage scales with batch_size, not total dataset size</li> <li>Statistics are calculated incrementally to handle large datasets</li> <li>If dataset_writer is provided, save_dir, save_text, and save_json parameters are ignored</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Define instance generator function\n&gt;&gt;&gt; def instance_generator(seed):\n...     import random\n...     random.seed(seed)\n...     # Generate random polynomial problem\n...     problem = [random.randint(1, 1000) for _ in range(random.randint(1, 10))]\n...     answer = sum(problem)\n...     return problem, answer\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize dataset generator\n&gt;&gt;&gt; generator = DatasetGenerator(n_jobs=-1, verbose=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 1: Automatic DatasetWriter creation\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000, \"test\": 1000, \"validation\": 500},\n...     instance_generator=instance_generator,\n...     save_dir=\"./datasets\",\n...     save_text=True,\n...     save_json=True,\n...     batch_size=100\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 2: Manual DatasetWriter creation (for advanced use cases)\n&gt;&gt;&gt; from calt.dataset.sagemath import DatasetWriter\n&gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000, \"test\": 1000},\n...     instance_generator=instance_generator,\n...     dataset_writer=writer,\n...     batch_size=100\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 3: Generate datasets separately (if needed)\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000},\n...     instance_generator=instance_generator,\n...     save_dir=\"./datasets\",\n...     batch_size=100\n... )\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"test\": 1000, \"validation\": 500},\n...     instance_generator=instance_generator,\n...     save_dir=\"./datasets\",\n...     batch_size=100\n... )\n</code></pre> Source code in <code>src/calt/dataset/sagemath/dataset_generator.py</code> <pre><code>def run(\n    self,\n    dataset_sizes: dict[str, int],\n    instance_generator: Callable,\n    statistics_calculator: Callable | None = None,\n    dataset_writer: DatasetWriter | None = None,\n    batch_size: int = 100000,\n    save_dir: str | None = None,\n    save_text: bool = True,\n    save_json: bool = True,\n):\n    \"\"\"\n    Generate multiple datasets using parallel processing with batch writing.\n\n    This is the main entry point for dataset generation. It supports generating\n    multiple datasets (train/test) simultaneously or separately, with efficient\n    memory management through batch processing and parallel execution.\n\n    Key features:\n\n    - Parallel processing using joblib for high performance\n    - Batch-based memory management to handle large datasets\n    - Incremental statistics calculation to avoid memory issues\n    - Reproducible generation with unique seeds for each sample\n    - Support for nested data structures (up to 2 levels)\n    - Multiple output formats (text, JSON Lines) via DatasetWriter\n\n    Args:\n        dataset_sizes: Dictionary mapping dataset names to number of samples.\n                      Any string can be used as dataset name (e.g., \"train\", \"test\", \"validation\").\n                      Duplicate names are not allowed.\n                      Example: {\"train\": 100000, \"test\": 1000} or {\"train\": 100000, \"validation\": 5000}\n        instance_generator: Function that generates (problem, answer) pair given a seed.\n                         Must accept a single integer seed parameter.\n        statistics_calculator: Optional function to calculate sample-specific statistics.\n                             Must accept (problem, answer) and return dict or None.\n        dataset_writer: DatasetWriter object for saving datasets to files.\n                      If None, a new DatasetWriter will be created using save_dir, save_text, and save_json parameters.\n        batch_size: Number of samples to process in each batch. Larger batches\n                   use more memory but may be more efficient for I/O operations.\n        save_dir: Base directory for saving datasets. Used only if dataset_writer is None.\n                 If None, uses current working directory.\n        save_text: Whether to save raw text files. Used only if dataset_writer is None.\n                  Text files use \"#\" as separator between problem and answer.\n        save_json: Whether to save JSON Lines files. Used only if dataset_writer is None.\n                  JSON Lines files preserve the original nested structure format.\n\n    Raises:\n        ValueError: If dataset_sizes is invalid or instance_generator is None\n        Exception: If parallel processing fails\n\n    Note:\n        - Each sample gets a unique seed for reproducibility\n        - Progress is logged if verbose=True (set in __init__)\n        - Memory usage scales with batch_size, not total dataset size\n        - Statistics are calculated incrementally to handle large datasets\n        - If dataset_writer is provided, save_dir, save_text, and save_json parameters are ignored\n\n    Examples:\n        &gt;&gt;&gt; # Define instance generator function\n        &gt;&gt;&gt; def instance_generator(seed):\n        ...     import random\n        ...     random.seed(seed)\n        ...     # Generate random polynomial problem\n        ...     problem = [random.randint(1, 1000) for _ in range(random.randint(1, 10))]\n        ...     answer = sum(problem)\n        ...     return problem, answer\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Initialize dataset generator\n        &gt;&gt;&gt; generator = DatasetGenerator(n_jobs=-1, verbose=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 1: Automatic DatasetWriter creation\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000, \"test\": 1000, \"validation\": 500},\n        ...     instance_generator=instance_generator,\n        ...     save_dir=\"./datasets\",\n        ...     save_text=True,\n        ...     save_json=True,\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 2: Manual DatasetWriter creation (for advanced use cases)\n        &gt;&gt;&gt; from calt.dataset.sagemath import DatasetWriter\n        &gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000, \"test\": 1000},\n        ...     instance_generator=instance_generator,\n        ...     dataset_writer=writer,\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 3: Generate datasets separately (if needed)\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000},\n        ...     instance_generator=instance_generator,\n        ...     save_dir=\"./datasets\",\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"test\": 1000, \"validation\": 500},\n        ...     instance_generator=instance_generator,\n        ...     save_dir=\"./datasets\",\n        ...     batch_size=100\n        ... )\n    \"\"\"\n    # Create DatasetWriter if not provided\n    if dataset_writer is None:\n        dataset_writer = DatasetWriter(\n            save_dir=save_dir,\n            save_text=save_text,\n            save_json=save_json,\n        )\n        self.logger.info(f\"save_dir: {dataset_writer.save_dir}\")\n        self.logger.info(f\"Text output: {save_text}\")\n        self.logger.info(f\"JSON output: {save_json}\")\n\n    # Prepare common arguments\n    common_args = {\n        \"instance_generator\": instance_generator,\n        \"statistics_calculator\": statistics_calculator,\n        \"dataset_writer\": dataset_writer,\n        \"batch_size\": batch_size,\n    }\n\n    # Validate dataset_sizes\n    if not isinstance(dataset_sizes, dict):\n        raise ValueError(\"dataset_sizes must be a dictionary\")\n\n    if not dataset_sizes:\n        raise ValueError(\"dataset_sizes cannot be empty\")\n\n    if instance_generator is None:\n        raise ValueError(\"instance_generator must be provided\")\n\n    # Check for duplicate dataset names\n    if len(dataset_sizes) != len(set(dataset_sizes.keys())):\n        raise ValueError(\"Duplicate dataset names are not allowed\")\n\n    for dataset_name, num_samples in dataset_sizes.items():\n        if not isinstance(num_samples, int) or num_samples &lt;= 0:\n            raise ValueError(\n                f\"Number of samples must be a positive integer, got {num_samples} for {dataset_name}\"\n            )\n\n    # Log overall generation start\n    self.logger.info(\n        \"=========================== Dataset generation ===========================\\n\"\n    )\n    self.logger.info(\n        f\"Starting dataset generation for {len(dataset_sizes)} dataset(s)\"\n    )\n    self.logger.info(f\"Dataset sizes: {dataset_sizes}\\n\")\n\n    # Generate each dataset\n    for dataset_name, num_samples in dataset_sizes.items():\n        self._generate_dataset(\n            tag=dataset_name, num_samples=num_samples, **common_args\n        )\n\n    self.logger.info(\"All datasets generated successfully!\")\n    self.logger.info(\n        \"==========================================================================\\n\"\n    )\n</code></pre>"},{"location":"dataset_sagemath/#calt.dataset.sagemath.utils.polynomial_sampler.PolynomialSampler","title":"PolynomialSampler","text":"<pre><code>PolynomialSampler(\n    symbols: str | None = None,\n    field_str: str | None = None,\n    order: str | TermOrder | None = \"degrevlex\",\n    ring: Any = None,\n    max_num_terms: int | None = 10,\n    max_degree: int = 5,\n    min_degree: int = 0,\n    degree_sampling: str = \"uniform\",\n    term_sampling: str = \"uniform\",\n    max_coeff: int | None = None,\n    num_bound: int | None = None,\n    strictly_conditioned: bool = True,\n    nonzero_instance: bool = True,\n    nonzero_coeff: bool = True,\n    max_attempts: int = 1000,\n)\n</code></pre> <p>Generator for random polynomials with specific constraints.</p> <p>The sampler builds polynomials by first choosing a target degree and number of terms (within min/max bounds), then selecting that many distinct monomials and assigning random coefficients from the base ring. Ring and constraints can be given either as (symbols, field_str, order) or as a pre-built PolynomialRing.</p>"},{"location":"dataset_sagemath/#calt.dataset.sagemath.utils.polynomial_sampler.PolynomialSampler--behavior-summary","title":"Behavior summary","text":"<p>degree_sampling controls how monomial degrees are chosen:</p> <ul> <li><code>'uniform'</code>: For each term, a degree in [min_degree, max_degree] is   chosen uniformly at random, then a monomial of that degree is chosen.   The resulting polynomial's degree distribution is more uniform over the   range.</li> <li><code>'fixed'</code>: Monomials are chosen uniformly from all monomials of degree   at most max_degree. Because there are more such monomials at higher   degrees, the polynomial tends to have total degree equal to max_degree.</li> </ul> <p>Degree and number of terms: Every returned polynomial has total degree &gt;= min_degree. The guarantees on total degree and number of terms depend on <code>strictly_conditioned</code> and <code>nonzero_coeff</code>; see the constructor parameters for details.</p> <p>Parameters:</p> Name Type Description Default <code>symbols</code> <code>str | None</code> <p>Variable names for the polynomial ring (required if ring is None).</p> <code>None</code> <code>field_str</code> <code>str | None</code> <p>Base ring specifier: \"QQ\", \"RR\", \"ZZ\", or \"GF(p)\" for a prime finite field (required if ring is None).</p> <code>None</code> <code>order</code> <code>str | TermOrder | None</code> <p>Term order of the ring, e.g. \"degrevlex\" (required if ring is None).</p> <code>'degrevlex'</code> <code>ring</code> <code>Any</code> <p>Pre-built PolynomialRing (alternative to symbols/field_str/order).</p> <code>None</code> <code>max_num_terms</code> <code>int | None</code> <p>Upper bound on number of terms. If None, all monomials of the chosen degree are allowed.</p> <code>10</code> <code>max_degree</code> <code>int</code> <p>Maximum total degree of the polynomial.</p> <code>5</code> <code>min_degree</code> <code>int</code> <p>Minimum total degree; every returned polynomial has total degree &gt;= min_degree.</p> <code>0</code> <code>max_coeff</code> <code>int | None</code> <p>Bound on coefficient absolute value for RR and ZZ.</p> <code>None</code> <code>num_bound</code> <code>int | None</code> <p>Bound on numerator/denominator absolute value for QQ.</p> <code>None</code> <code>degree_sampling</code> <code>str</code> <p><code>'uniform'</code> or <code>'fixed'</code>; see class docstring (Behavior summary).</p> <code>'uniform'</code> <code>term_sampling</code> <code>str</code> <p><code>'uniform'</code>: number of terms chosen uniformly in [1, max_num_terms]; <code>'fixed'</code>: use max_num_terms.</p> <code>'uniform'</code> <code>strictly_conditioned</code> <code>bool</code> <p>Controls when a generated polynomial is accepted.</p> <ul> <li>If True:<ul> <li>Return only when total degree equals the degree   selected for this sample and number of terms   equals the number of terms selected for this   sample. (Those values are chosen by   degree_sampling and term_sampling; degree is in   [min_degree, max_degree], and number of terms is   at most max_num_terms.)</li> <li>If nonzero_coeff=False, some polynomials have   fewer than num_terms terms (zero coefficients);   those are rejected and generation is retried.   RuntimeError is raised if no success within   max_attempts.</li> </ul> </li> <li>If False:<ul> <li>Return the first polynomial with total degree &gt;=   min_degree and (if nonzero_instance) non-zero.</li> <li>Number of terms may be less than the chosen   num_terms when nonzero_coeff=False.</li> </ul> </li> </ul> <code>True</code> <code>nonzero_instance</code> <code>bool</code> <p>If True, the zero polynomial is never returned.</p> <code>True</code> <code>nonzero_coeff</code> <code>bool</code> <p>If True, no coefficient is zero (default); gives a predictable number of terms and fewer retries when strictly_conditioned is True.</p> <code>True</code> <code>max_attempts</code> <code>int</code> <p>Maximum trials per polynomial when strictly_conditioned is True; RuntimeError is raised if no success.</p> <code>1000</code> Source code in <code>src/calt/dataset/sagemath/utils/polynomial_sampler.py</code> <pre><code>def __init__(\n    self,\n    symbols: str | None = None,\n    field_str: str | None = None,\n    order: str | TermOrder | None = \"degrevlex\",\n    ring: Any = None,\n    max_num_terms: int | None = 10,\n    max_degree: int = 5,\n    min_degree: int = 0,\n    degree_sampling: str = \"uniform\",  # 'uniform' or 'fixed'\n    term_sampling: str = \"uniform\",  # 'uniform' or 'fixed'\n    max_coeff: int | None = None,  # Used for RR and ZZ\n    num_bound: int | None = None,  # Used for QQ\n    strictly_conditioned: bool = True,\n    nonzero_instance: bool = True,\n    nonzero_coeff: bool = True,\n    max_attempts: int = 1000,\n):\n    \"\"\"\n    Initialize polynomial sampler.\n\n    Args:\n        symbols: Variable names for the polynomial ring\n            (required if ring is None).\n        field_str: Base ring specifier: \"QQ\", \"RR\", \"ZZ\", or \"GF(p)\"\n            for a prime finite field (required if ring is None).\n        order: Term order of the ring, e.g. \"degrevlex\"\n            (required if ring is None).\n        ring: Pre-built PolynomialRing\n            (alternative to symbols/field_str/order).\n        max_num_terms: Upper bound on number of terms. If None, all\n            monomials of the chosen degree are allowed.\n        max_degree: Maximum total degree of the polynomial.\n        min_degree: Minimum total degree; every returned polynomial\n            has total degree &gt;= min_degree.\n        max_coeff: Bound on coefficient absolute value for RR and ZZ.\n        num_bound: Bound on numerator/denominator absolute value\n            for QQ.\n        degree_sampling: ``'uniform'`` or ``'fixed'``; see class\n            docstring (Behavior summary).\n        term_sampling: ``'uniform'``: number of terms chosen uniformly\n            in [1, max_num_terms]; ``'fixed'``: use max_num_terms.\n        strictly_conditioned: Controls when a generated polynomial\n            is accepted.\n\n            - If True:\n                - Return only when total degree equals the degree\n                  selected for this sample and number of terms\n                  equals the number of terms selected for this\n                  sample. (Those values are chosen by\n                  degree_sampling and term_sampling; degree is in\n                  [min_degree, max_degree], and number of terms is\n                  at most max_num_terms.)\n                - If nonzero_coeff=False, some polynomials have\n                  fewer than num_terms terms (zero coefficients);\n                  those are rejected and generation is retried.\n                  RuntimeError is raised if no success within\n                  max_attempts.\n            - If False:\n                - Return the first polynomial with total degree &gt;=\n                  min_degree and (if nonzero_instance) non-zero.\n                - Number of terms may be less than the chosen\n                  num_terms when nonzero_coeff=False.\n        nonzero_instance: If True, the zero polynomial is never\n            returned.\n        nonzero_coeff: If True, no coefficient is zero (default);\n            gives a predictable number of terms and fewer retries\n            when strictly_conditioned is True.\n        max_attempts: Maximum trials per polynomial when\n            strictly_conditioned is True; RuntimeError is raised if\n            no success.\n    \"\"\"\n    # Validate input parameters\n    if ring is not None:\n        if symbols is not None or field_str is not None or order is not None:\n            raise ValueError(\"Cannot specify both ring and symbols/field_str/order\")\n        self.ring = ring\n        self.symbols = None\n        self.field_str = None\n        self.order = None\n    else:\n        if symbols is None or field_str is None or order is None:\n            raise ValueError(\n                \"Must specify either ring or all of symbols/field_str/order\"\n            )\n        self.ring = None\n        self.symbols = symbols\n        self.field_str = field_str\n        # Map \"grevlex\" to \"degrevlex\" for SageMath compatibility\n        # SageMath uses \"degrevlex\" instead of \"grevlex\"\n        if isinstance(order, str) and order == \"grevlex\":\n            self.order = \"degrevlex\"\n        else:\n            self.order = order\n\n    self.max_num_terms = max_num_terms\n    self.max_degree = max_degree\n    self.min_degree = min_degree\n    self.max_coeff = max_coeff\n    self.num_bound = num_bound\n    self.degree_sampling = degree_sampling\n    self.term_sampling = term_sampling\n    self.strictly_conditioned = strictly_conditioned\n    self.nonzero_instance = nonzero_instance\n    self.nonzero_coeff = nonzero_coeff\n    self.max_attempts = max_attempts\n</code></pre>"},{"location":"dataset_sagemath/#calt.dataset.sagemath.utils.polynomial_sampler.PolynomialSampler.get_field","title":"get_field","text":"<pre><code>get_field()\n</code></pre> <p>Convert field_str to the SageMath base ring (QQ, RR, ZZ, or GF(p)).</p> Source code in <code>src/calt/dataset/sagemath/utils/polynomial_sampler.py</code> <pre><code>def get_field(self):\n    \"\"\"Convert field_str to the SageMath base ring (QQ, RR, ZZ, or GF(p)).\"\"\"\n    if self.ring is not None:\n        return self.ring.base_ring()\n\n    # Standard field mapping\n    standard_fields = {\"QQ\": QQ, \"RR\": RR, \"ZZ\": ZZ}\n    if self.field_str in standard_fields:\n        return standard_fields[self.field_str]\n\n    # Finite field handling\n    if not self.field_str.startswith(\"GF\"):\n        raise ValueError(f\"Unsupported field: {self.field_str}\")\n\n    try:\n        # Extract field size based on format\n        p = int(\n            self.field_str[3:-1]\n            if self.field_str.startswith(\"GF(\")\n            else self.field_str[2:]\n        )\n\n        if p &lt;= 1:\n            raise ValueError(f\"Field size must be greater than 1: {p}\")\n        return GF(p)\n    except ValueError as e:\n        raise ValueError(f\"Unsupported field: {self.field_str}\") from e\n</code></pre>"},{"location":"dataset_sagemath/#calt.dataset.sagemath.utils.polynomial_sampler.PolynomialSampler.get_ring","title":"get_ring","text":"<pre><code>get_ring() -&gt; PolynomialRing\n</code></pre> <p>Return the polynomial ring (the configured ring if set, otherwise one built from symbols/field_str/order).</p> <p>Returns:</p> Name Type Description <code>PolynomialRing</code> <code>PolynomialRing</code> <p>The polynomial ring.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If polynomial ring creation fails with informative error message.</p> Source code in <code>src/calt/dataset/sagemath/utils/polynomial_sampler.py</code> <pre><code>def get_ring(self) -&gt; PolynomialRing:\n    \"\"\"\n    Return the polynomial ring (the configured ring if set, otherwise one built from symbols/field_str/order).\n\n    Returns:\n        PolynomialRing: The polynomial ring.\n\n    Raises:\n        ValueError: If polynomial ring creation fails with informative error message.\n    \"\"\"\n    if self.ring is not None:\n        return self.ring\n\n    try:\n        field = self.get_field()\n        R = PolynomialRing(field, self.symbols, order=self.order)\n        return R\n    except (ValueError, TypeError, AttributeError) as e:\n        # Provide informative error message with the parameters used\n        field_str = self.field_str if self.field_str else \"unknown\"\n        order_str = (\n            str(self.order)\n            if isinstance(self.order, (str, TermOrder))\n            else self.order\n        )\n        raise ValueError(\n            f\"Failed to create polynomial ring with parameters: \"\n            f\"field={field_str}, symbols={self.symbols}, order={order_str}. \"\n            f\"Error details: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"dataset_sagemath/#calt.dataset.sagemath.utils.polynomial_sampler.PolynomialSampler.sample","title":"sample","text":"<pre><code>sample(\n    num_samples: int = 1,\n    size: tuple[int, int] | None = None,\n    density: float = 1.0,\n    matrix_type: str | None = None,\n) -&gt; list[MPolynomial_libsingular] | list[matrix]\n</code></pre> <p>Generate random polynomial samples</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>size</code> <code>tuple[int, int] | None</code> <p>If provided, generate matrix of polynomials with given size</p> <code>None</code> <code>density</code> <code>float</code> <p>Probability of non-zero entries in matrix</p> <code>1.0</code> <code>matrix_type</code> <code>str | None</code> <p>Special matrix type (e.g., 'unimodular_upper_triangular')</p> <code>None</code> <p>Returns:</p> Type Description <code>list[MPolynomial_libsingular] | list[matrix]</code> <p>List of polynomials or polynomial matrices</p> Source code in <code>src/calt/dataset/sagemath/utils/polynomial_sampler.py</code> <pre><code>def sample(\n    self,\n    num_samples: int = 1,\n    size: tuple[int, int] | None = None,\n    density: float = 1.0,\n    matrix_type: str | None = None,\n) -&gt; list[MPolynomial_libsingular] | list[matrix]:\n    \"\"\"\n    Generate random polynomial samples\n\n    Args:\n        num_samples: Number of samples to generate\n        size: If provided, generate matrix of polynomials with given size\n        density: Probability of non-zero entries in matrix\n        matrix_type: Special matrix type (e.g., 'unimodular_upper_triangular')\n\n    Returns:\n        List of polynomials or polynomial matrices\n    \"\"\"\n    if size is not None:\n        return [\n            self._sample_matrix(size, density, matrix_type)\n            for _ in range(num_samples)\n        ]\n    else:\n        return [self._sample_polynomial() for _ in range(num_samples)]\n</code></pre>"},{"location":"dataset_sympy/","title":"SymPy backend","text":"<p>When using <code>backend=\"sympy\"</code>, the following classes are used for generation and sampling. You can also use them directly without DatasetPipeline.</p> <p>See Dataset Generator (Overview) for the pipeline and <code>data.yaml</code> configuration.</p>"},{"location":"dataset_sympy/#calt.dataset.sympy.dataset_generator.DatasetGenerator","title":"DatasetGenerator","text":"<pre><code>DatasetGenerator(\n    backend: str = \"multiprocessing\",\n    n_jobs: int = -1,\n    verbose: bool = True,\n    root_seed: int = 42,\n)\n</code></pre> <p>Base class for instance generators</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Backend for parallel processing</p> <code>'multiprocessing'</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs (-1 for all cores)</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to display progress information</p> <code>True</code> <code>root_seed</code> <code>int</code> <p>Root seed for reproducibility</p> <code>42</code> Source code in <code>src/calt/dataset/sympy/dataset_generator.py</code> <pre><code>def __init__(\n    self,\n    backend: str = \"multiprocessing\",\n    n_jobs: int = -1,\n    verbose: bool = True,\n    root_seed: int = 42,\n):\n    \"\"\"\n    Initialize instance generator.\n\n    Args:\n        backend: Backend for parallel processing\n        n_jobs: Number of parallel jobs (-1 for all cores)\n        verbose: Whether to display progress information\n        root_seed: Root seed for reproducibility\n    \"\"\"\n\n    self.backend = backend\n    self.n_jobs = n_jobs\n    self.verbose = verbose\n    self.root_seed = root_seed\n\n    # Configure logging only once at initialization\n    self.logger = logger\n\n    # Configure joblib logging to show progress but not overwhelm\n    # Only set if not already configured\n    joblib_logger = logging.getLogger(\"joblib\")\n    if joblib_logger.level == logging.NOTSET:\n        joblib_logger.setLevel(logging.INFO)\n\n    parallel_logger = logging.getLogger(\"joblib.Parallel\")\n    if parallel_logger.level == logging.NOTSET:\n        parallel_logger.setLevel(logging.INFO)\n</code></pre>"},{"location":"dataset_sympy/#calt.dataset.sympy.dataset_generator.DatasetGenerator.run","title":"run","text":"<pre><code>run(\n    dataset_sizes: dict[str, int],\n    instance_generator: Callable,\n    statistics_calculator: Callable | None = None,\n    dataset_writer: DatasetWriter | None = None,\n    batch_size: int = 100000,\n    save_dir: str | None = None,\n    save_text: bool = True,\n    save_json: bool = True,\n)\n</code></pre> <p>Generate multiple datasets using parallel processing with batch writing.</p> <p>This is the main entry point for dataset generation. It supports generating multiple datasets (train/test) simultaneously or separately, with efficient memory management through batch processing and parallel execution.</p> <p>Key features:</p> <ul> <li>Parallel processing using joblib for high performance</li> <li>Batch-based memory management to handle large datasets</li> <li>Incremental statistics calculation to avoid memory issues</li> <li>Reproducible generation with unique seeds for each sample</li> <li>Support for nested data structures (up to 2 levels)</li> <li>Multiple output formats (text, JSON Lines) via DatasetWriter</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset_sizes</code> <code>dict[str, int]</code> <p>Dictionary mapping dataset names to number of samples.           Any string can be used as dataset name (e.g., \"train\", \"test\", \"validation\").           Duplicate names are not allowed.           Example: {\"train\": 100000, \"test\": 1000} or {\"train\": 100000, \"validation\": 5000}</p> required <code>instance_generator</code> <code>Callable</code> <p>Function that generates (problem, answer) pair given a seed.              Must accept a single integer seed parameter.</p> required <code>statistics_calculator</code> <code>Callable | None</code> <p>Optional function to calculate sample-specific statistics.                  Must accept (problem, answer) and return dict or None.</p> <code>None</code> <code>dataset_writer</code> <code>DatasetWriter | None</code> <p>DatasetWriter object for saving datasets to files.           If None, a new DatasetWriter will be created using save_dir, save_text, and save_json parameters.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of samples to process in each batch. Larger batches        use more memory but may be more efficient for I/O operations.</p> <code>100000</code> <code>save_dir</code> <code>str | None</code> <p>Base directory for saving datasets. Used only if dataset_writer is None.      If None, uses current working directory.</p> <code>None</code> <code>save_text</code> <code>bool</code> <p>Whether to save raw text files. Used only if dataset_writer is None.       Text files use \"#\" as separator between problem and answer.</p> <code>True</code> <code>save_json</code> <code>bool</code> <p>Whether to save JSON Lines files. Used only if dataset_writer is None.       JSON Lines files preserve the original nested structure format.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset_sizes is invalid or instance_generator is None</p> <code>Exception</code> <p>If parallel processing fails</p> Note <ul> <li>Each sample gets a unique seed for reproducibility</li> <li>Progress is logged if verbose=True (set in init)</li> <li>Memory usage scales with batch_size, not total dataset size</li> <li>Statistics are calculated incrementally to handle large datasets</li> <li>If dataset_writer is provided, save_dir, save_text, and save_json parameters are ignored</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Define instance generator function\n&gt;&gt;&gt; def instance_generator(seed):\n...     import random\n...     random.seed(seed)\n...     # Generate random polynomial problem\n...     problem = [random.randint(1, 1000) for _ in range(random.randint(1, 10))]\n...     answer = sum(problem)\n...     return problem, answer\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize dataset generator\n&gt;&gt;&gt; generator = DatasetGenerator(n_jobs=-1, verbose=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 1: Automatic DatasetWriter creation\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000, \"test\": 1000, \"validation\": 500},\n...     instance_generator=instance_generator,\n...     save_dir=\"./datasets\",\n...     save_text=True,\n...     save_json=True,\n...     batch_size=100\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 2: Manual DatasetWriter creation (for advanced use cases)\n&gt;&gt;&gt; from calt.dataset.sympy import DatasetWriter\n&gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000, \"test\": 1000},\n...     instance_generator=instance_generator,\n...     dataset_writer=writer,\n...     batch_size=100\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 3: Generate datasets separately (if needed)\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000},\n...     instance_generator=instance_generator,\n...     save_dir=\"./datasets\",\n...     batch_size=100\n... )\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"test\": 1000, \"validation\": 500},\n...     instance_generator=instance_generator,\n...     save_dir=\"./datasets\",\n...     batch_size=100\n... )\n</code></pre> Source code in <code>src/calt/dataset/sympy/dataset_generator.py</code> <pre><code>def run(\n    self,\n    dataset_sizes: dict[str, int],\n    instance_generator: Callable,\n    statistics_calculator: Callable | None = None,\n    dataset_writer: DatasetWriter | None = None,\n    batch_size: int = 100000,\n    save_dir: str | None = None,\n    save_text: bool = True,\n    save_json: bool = True,\n):\n    \"\"\"\n    Generate multiple datasets using parallel processing with batch writing.\n\n    This is the main entry point for dataset generation. It supports generating\n    multiple datasets (train/test) simultaneously or separately, with efficient\n    memory management through batch processing and parallel execution.\n\n    Key features:\n\n    - Parallel processing using joblib for high performance\n    - Batch-based memory management to handle large datasets\n    - Incremental statistics calculation to avoid memory issues\n    - Reproducible generation with unique seeds for each sample\n    - Support for nested data structures (up to 2 levels)\n    - Multiple output formats (text, JSON Lines) via DatasetWriter\n\n    Args:\n        dataset_sizes: Dictionary mapping dataset names to number of samples.\n                      Any string can be used as dataset name (e.g., \"train\", \"test\", \"validation\").\n                      Duplicate names are not allowed.\n                      Example: {\"train\": 100000, \"test\": 1000} or {\"train\": 100000, \"validation\": 5000}\n        instance_generator: Function that generates (problem, answer) pair given a seed.\n                         Must accept a single integer seed parameter.\n        statistics_calculator: Optional function to calculate sample-specific statistics.\n                             Must accept (problem, answer) and return dict or None.\n        dataset_writer: DatasetWriter object for saving datasets to files.\n                      If None, a new DatasetWriter will be created using save_dir, save_text, and save_json parameters.\n        batch_size: Number of samples to process in each batch. Larger batches\n                   use more memory but may be more efficient for I/O operations.\n        save_dir: Base directory for saving datasets. Used only if dataset_writer is None.\n                 If None, uses current working directory.\n        save_text: Whether to save raw text files. Used only if dataset_writer is None.\n                  Text files use \"#\" as separator between problem and answer.\n        save_json: Whether to save JSON Lines files. Used only if dataset_writer is None.\n                  JSON Lines files preserve the original nested structure format.\n\n    Raises:\n        ValueError: If dataset_sizes is invalid or instance_generator is None\n        Exception: If parallel processing fails\n\n    Note:\n        - Each sample gets a unique seed for reproducibility\n        - Progress is logged if verbose=True (set in __init__)\n        - Memory usage scales with batch_size, not total dataset size\n        - Statistics are calculated incrementally to handle large datasets\n        - If dataset_writer is provided, save_dir, save_text, and save_json parameters are ignored\n\n    Examples:\n        &gt;&gt;&gt; # Define instance generator function\n        &gt;&gt;&gt; def instance_generator(seed):\n        ...     import random\n        ...     random.seed(seed)\n        ...     # Generate random polynomial problem\n        ...     problem = [random.randint(1, 1000) for _ in range(random.randint(1, 10))]\n        ...     answer = sum(problem)\n        ...     return problem, answer\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Initialize dataset generator\n        &gt;&gt;&gt; generator = DatasetGenerator(n_jobs=-1, verbose=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 1: Automatic DatasetWriter creation\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000, \"test\": 1000, \"validation\": 500},\n        ...     instance_generator=instance_generator,\n        ...     save_dir=\"./datasets\",\n        ...     save_text=True,\n        ...     save_json=True,\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 2: Manual DatasetWriter creation (for advanced use cases)\n        &gt;&gt;&gt; from calt.dataset.sympy import DatasetWriter\n        &gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000, \"test\": 1000},\n        ...     instance_generator=instance_generator,\n        ...     dataset_writer=writer,\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 3: Generate datasets separately (if needed)\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000},\n        ...     instance_generator=instance_generator,\n        ...     save_dir=\"./datasets\",\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"test\": 1000, \"validation\": 500},\n        ...     instance_generator=instance_generator,\n        ...     save_dir=\"./datasets\",\n        ...     batch_size=100\n        ... )\n    \"\"\"\n    # Create DatasetWriter if not provided\n    if dataset_writer is None:\n        dataset_writer = DatasetWriter(\n            save_dir=save_dir,\n            save_text=save_text,\n            save_json=save_json,\n        )\n        self.logger.info(f\"save_dir: {dataset_writer.save_dir}\")\n        self.logger.info(f\"Text output: {save_text}\")\n        self.logger.info(f\"JSON output: {save_json}\")\n\n    # Prepare common arguments\n    common_args = {\n        \"instance_generator\": instance_generator,\n        \"statistics_calculator\": statistics_calculator,\n        \"dataset_writer\": dataset_writer,\n        \"batch_size\": batch_size,\n    }\n\n    # Validate dataset_sizes\n    if not isinstance(dataset_sizes, dict):\n        raise ValueError(\"dataset_sizes must be a dictionary\")\n\n    if not dataset_sizes:\n        raise ValueError(\"dataset_sizes cannot be empty\")\n\n    if instance_generator is None:\n        raise ValueError(\"instance_generator must be provided\")\n\n    # Check for duplicate dataset names\n    if len(dataset_sizes) != len(set(dataset_sizes.keys())):\n        raise ValueError(\"Duplicate dataset names are not allowed\")\n\n    for dataset_name, num_samples in dataset_sizes.items():\n        if not isinstance(num_samples, int) or num_samples &lt;= 0:\n            raise ValueError(\n                f\"Number of samples must be a positive integer, got {num_samples} for {dataset_name}\"\n            )\n\n    # Log overall generation start\n    self.logger.info(\n        \"=========================== Dataset generation ===========================\\n\"\n    )\n    self.logger.info(\n        f\"Starting dataset generation for {len(dataset_sizes)} dataset(s)\"\n    )\n    self.logger.info(f\"Dataset sizes: {dataset_sizes}\\n\")\n\n    # Generate each dataset\n    for dataset_name, num_samples in dataset_sizes.items():\n        self._generate_dataset(\n            tag=dataset_name, num_samples=num_samples, **common_args\n        )\n\n    self.logger.info(\"All datasets generated successfully!\")\n    self.logger.info(\n        \"==========================================================================\\n\"\n    )\n</code></pre>"},{"location":"dataset_sympy/#calt.dataset.sympy.utils.polynomial_sampler.PolynomialSampler","title":"PolynomialSampler","text":"<pre><code>PolynomialSampler(\n    symbols: str,\n    field_str: str,\n    order: str | MonomialOrder = \"grevlex\",\n    max_num_terms: int | None = 10,\n    max_degree: int = 5,\n    min_degree: int = 0,\n    degree_sampling: str = \"uniform\",\n    term_sampling: str = \"uniform\",\n    max_coeff: int | None = None,\n    num_bound: int | None = None,\n    strictly_conditioned: bool = True,\n    nonzero_instance: bool = True,\n    max_attempts: int = 1000,\n)\n</code></pre> <p>Generator for random polynomials with specific constraints (SymPy).</p> <p>The sampler builds polynomials by choosing a target degree and number of terms (within min/max bounds), then uses :class:<code>SinglePolynomialSampler</code> to select that many distinct monomials and assign random coefficients from the base ring. Ring is specified by symbols, field_str, and order.</p>"},{"location":"dataset_sympy/#calt.dataset.sympy.utils.polynomial_sampler.PolynomialSampler--behavior-summary","title":"Behavior summary","text":"<p>degree_sampling controls how monomial degrees are chosen (passed as <code>choose_degree</code> to the internal sampler):</p> <ul> <li><code>'uniform'</code>: For each term, a degree in [min_degree, max_degree]   is chosen uniformly at random, then a monomial of that degree is   chosen. The resulting polynomial's degree distribution is more   uniform over the range.</li> <li><code>'fixed'</code>: Monomials are chosen uniformly from all monomials of   degree at most max_degree. The polynomial tends to have total degree   equal to max_degree.</li> </ul> <p>Degree and number of terms: Every returned polynomial has total degree &gt;= min_degree. The guarantees on total degree and number of terms depend on <code>strictly_conditioned</code> and <code>nonzero_instance</code>; see the constructor parameters for details.</p> <p>Parameters:</p> Name Type Description Default <code>symbols</code> <code>str</code> <p>Variable names for the polynomial ring.</p> required <code>field_str</code> <code>str</code> <p>Base ring specifier: \"QQ\", \"RR\", \"ZZ\", or \"GF(p)\" for a prime finite field.</p> required <code>order</code> <code>str | MonomialOrder</code> <p>Term order of the ring, e.g. \"grevlex\".</p> <code>'grevlex'</code> <code>max_num_terms</code> <code>int | None</code> <p>Upper bound on number of terms. If None, all monomials of the chosen degree are allowed.</p> <code>10</code> <code>max_degree</code> <code>int</code> <p>Maximum total degree of the polynomial.</p> <code>5</code> <code>min_degree</code> <code>int</code> <p>Minimum total degree; every returned polynomial has total degree &gt;= min_degree.</p> <code>0</code> <code>max_coeff</code> <code>int | None</code> <p>Bound on coefficient absolute value for RR and ZZ.</p> <code>None</code> <code>num_bound</code> <code>int | None</code> <p>Bound on numerator/denominator absolute value for QQ.</p> <code>None</code> <code>degree_sampling</code> <code>str</code> <p><code>'uniform'</code> or <code>'fixed'</code>; see class docstring (Behavior summary).</p> <code>'uniform'</code> <code>term_sampling</code> <code>str</code> <p><code>'uniform'</code>: number of terms chosen uniformly in [1, max_terms] (max_terms bounded by max_num_terms); <code>'fixed'</code>: use max_terms.</p> <code>'uniform'</code> <code>strictly_conditioned</code> <code>bool</code> <p>Controls when a generated polynomial is accepted.</p> <ul> <li>If True:<ul> <li>Return only when total degree equals the degree   selected for this sample and number of terms   equals the number of terms selected for this   sample. (Those values are chosen by   degree_sampling and term_sampling; degree is in   [min_degree, max_degree], and number of terms is   at most max_num_terms.)</li> <li>RuntimeError is raised if no success within   max_attempts.</li> </ul> </li> <li>If False:<ul> <li>Return the first polynomial with total degree &gt;=   min_degree and (if nonzero_instance) non-zero.</li> <li>Number of terms may be less than the chosen value   when nonzero_instance is False.</li> </ul> </li> </ul> <code>True</code> <code>nonzero_instance</code> <code>bool</code> <p>If True, the zero polynomial is never returned and all coefficients are non-zero (predictable number of terms). If False, coefficients may be zero.</p> <code>True</code> <code>max_attempts</code> <code>int</code> <p>Maximum trials per polynomial when strictly_conditioned is True; RuntimeError is raised if no success.</p> <code>1000</code> Source code in <code>src/calt/dataset/sympy/utils/polynomial_sampler.py</code> <pre><code>def __init__(\n    self,\n    symbols: str,\n    field_str: str,\n    order: str | MonomialOrder = \"grevlex\",\n    max_num_terms: int | None = 10,\n    max_degree: int = 5,\n    min_degree: int = 0,\n    degree_sampling: str = \"uniform\",  # 'uniform' or 'fixed'\n    term_sampling: str = \"uniform\",  # 'uniform' or 'fixed'\n    max_coeff: int | None = None,  # Used for RR and ZZ\n    num_bound: int | None = None,  # Used for QQ\n    strictly_conditioned: bool = True,\n    nonzero_instance: bool = True,\n    max_attempts: int = 1000,\n) -&gt; None:\n    \"\"\"\n    Initialize polynomial sampler.\n\n    Args:\n        symbols: Variable names for the polynomial ring.\n        field_str: Base ring specifier: \"QQ\", \"RR\", \"ZZ\", or \"GF(p)\"\n            for a prime finite field.\n        order: Term order of the ring, e.g. \"grevlex\".\n        max_num_terms: Upper bound on number of terms. If None, all\n            monomials of the chosen degree are allowed.\n        max_degree: Maximum total degree of the polynomial.\n        min_degree: Minimum total degree; every returned polynomial\n            has total degree &gt;= min_degree.\n        max_coeff: Bound on coefficient absolute value for RR and ZZ.\n        num_bound: Bound on numerator/denominator absolute value\n            for QQ.\n        degree_sampling: ``'uniform'`` or ``'fixed'``; see class\n            docstring (Behavior summary).\n        term_sampling: ``'uniform'``: number of terms chosen uniformly\n            in [1, max_terms] (max_terms bounded by max_num_terms);\n            ``'fixed'``: use max_terms.\n        strictly_conditioned: Controls when a generated polynomial\n            is accepted.\n\n            - If True:\n                - Return only when total degree equals the degree\n                  selected for this sample and number of terms\n                  equals the number of terms selected for this\n                  sample. (Those values are chosen by\n                  degree_sampling and term_sampling; degree is in\n                  [min_degree, max_degree], and number of terms is\n                  at most max_num_terms.)\n                - RuntimeError is raised if no success within\n                  max_attempts.\n            - If False:\n                - Return the first polynomial with total degree &gt;=\n                  min_degree and (if nonzero_instance) non-zero.\n                - Number of terms may be less than the chosen value\n                  when nonzero_instance is False.\n        nonzero_instance: If True, the zero polynomial is never\n            returned and all coefficients are non-zero (predictable\n            number of terms). If False, coefficients may be zero.\n        max_attempts: Maximum trials per polynomial when\n            strictly_conditioned is True; RuntimeError is raised if\n            no success.\n    \"\"\"\n\n    self.symbols = symbols\n    self.field_str = field_str\n    self.order = order\n    self.max_num_terms = max_num_terms\n    self.max_degree = max_degree\n    self.min_degree = min_degree\n    self.max_coeff = max_coeff\n    self.num_bound = num_bound\n    self.degree_sampling = degree_sampling\n    self.term_sampling = term_sampling\n    self.strictly_conditioned = strictly_conditioned\n    self.nonzero_instance = nonzero_instance\n    self.max_attempts = max_attempts\n    self.single_poly_sampler = SinglePolynomialSampler()\n</code></pre>"},{"location":"dataset_sympy/#calt.dataset.sympy.utils.polynomial_sampler.PolynomialSampler.get_field","title":"get_field","text":"<pre><code>get_field() -&gt; Domain\n</code></pre> <p>Return the SymPy domain for field_str (QQ, RR, ZZ, or GF(p)).</p> Source code in <code>src/calt/dataset/sympy/utils/polynomial_sampler.py</code> <pre><code>def get_field(self) -&gt; Domain:\n    \"\"\"Return the SymPy domain for field_str (QQ, RR, ZZ, or GF(p)).\"\"\"\n    # Standard field mapping\n    standard_fields = {\"QQ\": QQ, \"RR\": RR, \"ZZ\": ZZ}\n    if self.field_str in standard_fields:\n        return standard_fields[self.field_str]\n\n    # Finite field handling\n    if not self.field_str.startswith(\"GF\"):\n        raise ValueError(f\"Unsupported field: {self.field_str}\")\n\n    try:\n        # Extract field size based on format\n        p = int(\n            self.field_str[3:-1]\n            if self.field_str.startswith(\"GF(\")\n            else self.field_str[2:]\n        )\n\n        if p &lt;= 1:\n            raise ValueError(f\"Field size must be greater than 1: {p}\")\n        return GF(p)\n    except ValueError as e:\n        raise ValueError(f\"Unsupported field: {self.field_str}\") from e\n</code></pre>"},{"location":"dataset_sympy/#calt.dataset.sympy.utils.polynomial_sampler.PolynomialSampler.get_ring","title":"get_ring","text":"<pre><code>get_ring() -&gt; PolyRing\n</code></pre> <p>Return the polynomial ring (PolyRing) for the configured symbols, field, and order.</p> Source code in <code>src/calt/dataset/sympy/utils/polynomial_sampler.py</code> <pre><code>def get_ring(self) -&gt; PolyRing:\n    \"\"\"Return the polynomial ring (PolyRing) for the configured symbols, field, and order.\"\"\"\n\n    R, *gens = ring(self.symbols, self.get_field(), self.order)\n    return R\n</code></pre>"},{"location":"dataset_sympy/#calt.dataset.sympy.utils.polynomial_sampler.PolynomialSampler.sample","title":"sample","text":"<pre><code>sample(\n    num_samples: int = 1,\n    size: tuple[int, int] | None = None,\n    density: float = 1.0,\n    matrix_type: str | None = None,\n) -&gt; list[PolyElement] | list[np.ndarray]\n</code></pre> <p>Generate random polynomial samples</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>size</code> <code>tuple[int, int] | None</code> <p>If provided, generate matrix of polynomials with given size</p> <code>None</code> <code>density</code> <code>float</code> <p>Probability of non-zero entries in matrix</p> <code>1.0</code> <code>matrix_type</code> <code>str | None</code> <p>Special matrix type (e.g., 'unimodular_upper_triangular')</p> <code>None</code> <p>Returns:</p> Type Description <code>list[PolyElement] | list[ndarray]</code> <p>List of polynomials, or list of polynomial matrices when</p> <code>list[PolyElement] | list[ndarray]</code> <p>size is provided.</p> Source code in <code>src/calt/dataset/sympy/utils/polynomial_sampler.py</code> <pre><code>def sample(\n    self,\n    num_samples: int = 1,\n    size: tuple[int, int] | None = None,\n    density: float = 1.0,\n    matrix_type: str | None = None,\n) -&gt; list[PolyElement] | list[np.ndarray]:\n    \"\"\"\n    Generate random polynomial samples\n\n    Args:\n        num_samples: Number of samples to generate\n        size: If provided, generate matrix of polynomials with given size\n        density: Probability of non-zero entries in matrix\n        matrix_type: Special matrix type (e.g., 'unimodular_upper_triangular')\n\n    Returns:\n        List of polynomials, or list of polynomial matrices when\n        size is provided.\n    \"\"\"\n    if size is not None:\n        return [\n            self._sample_matrix(size, density, matrix_type)\n            for _ in range(num_samples)\n        ]\n    else:\n        return [self._sample_polynomial() for _ in range(num_samples)]\n</code></pre>"},{"location":"dataset_sympy/#calt.dataset.sympy.utils.polynomial_sampler.PolynomialSampler.total_degree","title":"total_degree","text":"<pre><code>total_degree(poly: PolyElement) -&gt; int\n</code></pre> <p>Return the total degree of the polynomial.</p> Source code in <code>src/calt/dataset/sympy/utils/polynomial_sampler.py</code> <pre><code>def total_degree(self, poly: PolyElement) -&gt; int:\n    \"\"\"Return the total degree of the polynomial.\"\"\"\n    if poly.is_zero:\n        return 0\n    else:\n        return max(sum(monom) for monom in poly.monoms())\n</code></pre>"},{"location":"io_lexer/","title":"Lexer and vocabulary","text":"<p>The lexer and vocabulary are configured via <code>lexer.yaml</code>; the file format and top-level keys are described in Configuration. Below are the API references for the classes that consume that configuration.</p>"},{"location":"io_lexer/#calt.io.preprocessor.UnifiedLexer","title":"UnifiedLexer","text":"<pre><code>UnifiedLexer(\n    vocab_config: VocabConfig,\n    number_policy: Optional[NumberPolicy] = None,\n    strict: bool = True,\n    include_base_vocab: bool = True,\n)\n</code></pre> <p>               Bases: <code>AbstractPreProcessor</code></p> <p>Unified regex-based lexer for tokenizing input strings.</p> <p>This lexer converts raw input strings into token sequences based on vocabulary configuration. It follows longest-match principle for reserved tokens and supports configurable number tokenization.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from calt.io.vocabulary import VocabConfig\n&gt;&gt;&gt; from calt.io.preprocessor import UnifiedLexer, NumberPolicy\n&gt;&gt;&gt;\n&gt;&gt;&gt; vocab_config = VocabConfig([], {}).from_config(\"config/vocab.yaml\")\n&gt;&gt;&gt; number_policy = NumberPolicy(sign=False, digit_group=1)\n&gt;&gt;&gt; lexer = UnifiedLexer(vocab_config, number_policy=number_policy)\n&gt;&gt;&gt;\n&gt;&gt;&gt; tokens = lexer.tokenize(\"C-50*x1^2 + 3.14\")\n&gt;&gt;&gt; # Returns: [\"C-50\", \"*\", \"x1\", \"^\", \"2\", \"+\", \"3\", \".\", \"1\", \"4\"]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>vocab_config</code> <code>VocabConfig</code> <p>Vocabulary configuration.</p> required <code>number_policy</code> <code>Optional[NumberPolicy]</code> <p>Policy for tokenizing numbers. If None, uses default.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>If True, raise error on unknown characters. If False, emit . <code>True</code> <code>include_base_vocab</code> <code>bool</code> <p>Whether to include base vocabulary tokens.</p> <code>True</code> Source code in <code>src/calt/io/preprocessor/lexer.py</code> <pre><code>def __init__(\n    self,\n    vocab_config: VocabConfig,\n    number_policy: Optional[NumberPolicy] = None,\n    strict: bool = True,\n    include_base_vocab: bool = True,\n):\n    \"\"\"Initialize the unified lexer.\n\n    Args:\n        vocab_config: Vocabulary configuration.\n        number_policy: Policy for tokenizing numbers. If None, uses default.\n        strict: If True, raise error on unknown characters. If False, emit &lt;unk&gt;.\n        include_base_vocab: Whether to include base vocabulary tokens.\n    \"\"\"\n    # Initialize post-processor base class\n    super().__init__()\n\n    self.number_policy = number_policy or NumberPolicy()\n    self.strict = strict\n    self.include_base_vocab = include_base_vocab\n\n    # Extend vocab_config with required tokens based on number_policy\n    self.vocab_config = self._extend_vocab_for_number_policy(vocab_config)\n\n    # Build reserved tokens\n    self._build_reserved_tokens()\n\n    # Build regex patterns\n    self._build_patterns()\n</code></pre>"},{"location":"io_lexer/#calt.io.preprocessor.NumberPolicy","title":"NumberPolicy      <code>dataclass</code>","text":"<pre><code>NumberPolicy(\n    sign: bool = False, digit_group: int = 0, allow_float: bool = True\n)\n</code></pre> <p>Policy for tokenizing numbers.</p> <p>Attributes:</p> Name Type Description <code>sign</code> <code>bool</code> <p>How to handle sign. True (attach) means sign is part of the number, False (separate) means sign is a separate token.</p> <code>digit_group</code> <code>int</code> <p>Group digits. 0 = no split, d&gt;=1 = split every d digits.</p> <code>allow_float</code> <code>bool</code> <p>Whether to allow floating point numbers.</p>"},{"location":"io_lexer/#calt.io.vocabulary.config.VocabConfig","title":"VocabConfig","text":"<pre><code>VocabConfig(\n    vocab: list[str],\n    special_tokens: dict[str, str],\n    include_base_vocab=True,\n    include_base_special_tokens=True,\n)\n</code></pre> Source code in <code>src/calt/io/vocabulary/config.py</code> <pre><code>def __init__(\n    self,\n    vocab: list[str],\n    special_tokens: dict[str, str],\n    include_base_vocab=True,\n    include_base_special_tokens=True,\n):\n    self.vocab = vocab\n    self.special_tokens = special_tokens\n\n    if include_base_vocab:\n        self.vocab = BASE_VOCAB + self.vocab\n    if include_base_special_tokens:\n        self.special_tokens = BASE_SPECIAL_TOKENS | self.special_tokens\n</code></pre>"},{"location":"io_load_preprocessors/","title":"Load preprocessors","text":"<p>Load preprocessors run once at load time (before the lexer) to convert raw file content\u2014a text line, JSONL object, or pickle sample\u2014into <code>(input_text, target_text)</code> pairs. You can use the library-provided implementations or supply your own and pass them to IOPipeline via configuration.</p>"},{"location":"io_load_preprocessors/#calt.io.preprocessor.load_preprocessors.chain.ChainLoadPreprocessor","title":"ChainLoadPreprocessor","text":"<pre><code>ChainLoadPreprocessor(*preprocessors: Any)\n</code></pre> <p>Run multiple load preprocessors in sequence.</p> <p>The first receives the raw source (str or dict); each next receives the previous output. The last preprocessor must return (input_text, target_text). Use this to combine e.g. TextToSageLoadPreprocessor and ExpandedFormLoadPreprocessor.</p> Source code in <code>src/calt/io/preprocessor/load_preprocessors/chain.py</code> <pre><code>def __init__(self, *preprocessors: Any):\n    if not preprocessors:\n        raise ValueError(\"ChainLoadPreprocessor requires at least one preprocessor\")\n    self.preprocessors = list(preprocessors)\n</code></pre>"},{"location":"io_load_preprocessors/#calt.io.preprocessor.load_preprocessors.expanded_form.ExpandedFormLoadPreprocessor","title":"ExpandedFormLoadPreprocessor","text":"<pre><code>ExpandedFormLoadPreprocessor(delimiter: str = ' || ')\n</code></pre> <p>Convert pickle-loaded polynomials to C/E expanded form (input_text, target_text).</p> <p>Expects source to be a dict with \"problem\" and \"answer\" (or \"solution\") (as from pickle or JSONL that stored raw polynomial objects). Problem and answer can be a single polynomial or a list of polynomials. Each polynomial is converted to: \"C E E ... + C E ...\" Multiple polynomials are joined by delimiter (default \" || \"). Source code in <code>src/calt/io/preprocessor/load_preprocessors/expanded_form.py</code> <pre><code>def __init__(self, delimiter: str = \" || \"):\n    self.delimiter = delimiter\n</code></pre>"},{"location":"io_load_preprocessors/#calt.io.preprocessor.load_preprocessors.text_to_sage.TextToSageLoadPreprocessor","title":"TextToSageLoadPreprocessor","text":"<pre><code>TextToSageLoadPreprocessor(\n    delimiter: str = \" | \", ring: Callable[[str], Any] | None = None\n)\n</code></pre> <p>Parse text line into SageMath polynomial lists (dict for chaining).</p> <p>Expects source to be a string line: \"poly1 | poly2 # poly3 | poly4\" (problem # answer). Splits by delimiter to get polynomial strings, parses each with ring(poly_str), returns {\"problem\": [poly, ...], \"answer\": [poly, ...]} for use with ExpandedFormLoadPreprocessor (e.g. via ChainLoadPreprocessor).</p> <p>Parameters:</p> Name Type Description Default <code>delimiter</code> <code>str</code> <p>Separator between polynomials in input text (default \" | \").</p> <code>' | '</code> <code>ring</code> <code>Callable[[str], Any] | None</code> <p>Callable that takes a string and returns a polynomial (e.g. SageMath   polynomial ring R so that R(\"9x0 + 5x2 + 10\") works).</p> <code>None</code> Source code in <code>src/calt/io/preprocessor/load_preprocessors/text_to_sage.py</code> <pre><code>def __init__(\n    self,\n    delimiter: str = \" | \",\n    ring: Callable[[str], Any] | None = None,\n):\n    if ring is None:\n        raise ValueError(\"TextToSageLoadPreprocessor requires ring\")\n    self.delimiter = delimiter\n    self.ring = ring\n</code></pre>"},{"location":"io_load_preprocessors/#calt.io.preprocessor.load_preprocessors.reversed_order.ReversedOrderLoadPreprocessor","title":"ReversedOrderLoadPreprocessor","text":"<pre><code>ReversedOrderLoadPreprocessor(problem_to_str: Any = None, delimiter: str = ',')\n</code></pre> <p>Reverse the order of answer elements (split by delimiter, reverse, rejoin).</p> <ul> <li>Text line: <code>\"11,4,11,4 # 11,15,9,13\"</code> \u2192 input: <code>\"11,4,11,4\"</code>, target: <code>\"13,9,15,11\"</code></li> <li>JSONL: same for <code>{\"problem\": ..., \"answer\": ...}</code> (or \"solution\"); split answer by delimiter, reverse, rejoin.</li> </ul> Source code in <code>src/calt/io/preprocessor/load_preprocessors/reversed_order.py</code> <pre><code>def __init__(self, problem_to_str: Any = None, delimiter: str = \",\"):\n    self.problem_to_str = problem_to_str or _to_str\n    self.delimiter = delimiter\n</code></pre>"},{"location":"io_load_preprocessors/#calt.io.preprocessor.load_preprocessors.last_element.LastElementLoadPreprocessor","title":"LastElementLoadPreprocessor","text":"<pre><code>LastElementLoadPreprocessor(problem_to_str: Any = None, delimiter: str = ',')\n</code></pre> <p>Use only the last element of answer (e.g. cumulative-sum final value).</p> <ul> <li>Text line: single line like <code>\"11,4,11,4 # 11,15,9,13\"</code> (format: problem # answer)</li> <li>JSONL: dict with <code>{\"problem\": ..., \"answer\": ...}</code> (or \"solution\")</li> <li><code>answer</code> is one of:</li> <li>list (e.g. <code>[11, 15, 9, 13]</code>)</li> <li>delimiter-joined string (e.g. <code>\"11,15,9,13\"</code>)</li> <li>Output is <code>(input_text, last_answer_str)</code>; only the last element is used as target.   e.g. <code>\"11,4,11,4 # 11,15,9,13\"</code> \u2192 input: <code>\"11,4,11,4\"</code>, target: <code>\"13\"</code></li> </ul> Source code in <code>src/calt/io/preprocessor/load_preprocessors/last_element.py</code> <pre><code>def __init__(self, problem_to_str: Any = None, delimiter: str = \",\"):\n    # Problem formatting is delegated to existing _to_str\n    self.problem_to_str = problem_to_str or _to_str\n    self.delimiter = delimiter\n</code></pre>"},{"location":"io_pipeline/","title":"Overview","text":"<p>Utilities to prepare training/evaluation datasets, tokenizers, and data collators. They convert symbolic expressions (polynomials/integers) into internal token sequences and build batches suitable for training.</p> <ul> <li>Lexer and vocabulary \u2014 <code>lexer.yaml</code> configuration and tokenisation.</li> <li>Load preprocessors \u2014 optional load-time preprocessing.</li> <li>Visualization \u2014 visual diff of predictions vs references.</li> </ul>"},{"location":"io_pipeline/#iopipeline","title":"IOPipeline","text":"<p>The main entry point is <code>IOPipeline</code>. <code>IOPipeline.from_config</code> consumes a <code>omegaconf.DictConfig</code> with paths to the lexer/vocabulary configuration and dataset files, then builds tokenised training and test datasets (<code>train_dataset</code>, <code>test_dataset</code>), a <code>PreTrainedTokenizerFast</code> tokenizer, and a <code>StandardDataCollator</code>. These are returned as a dictionary (<code>io_dict</code>) and passed to the model and trainer pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset_path</code> <code>str | None</code> <p>Path to training dataset file (.txt, .jsonl, or .pkl)</p> <code>None</code> <code>test_dataset_path</code> <code>str | None</code> <p>Path to test dataset file</p> <code>None</code> <code>num_train_samples</code> <code>int | None</code> <p>Maximum number of training samples to load</p> <code>None</code> <code>num_test_samples</code> <code>int | None</code> <p>Maximum number of test samples to load</p> <code>None</code> <code>vocab_config</code> <code>VocabConfig | dict | str | None</code> <p>VocabConfig, dict, or path to YAML file</p> <code>None</code> <code>preprocessor</code> <code>AbstractPreProcessor | None</code> <p>Lexer/preprocessor instance (optional)</p> <code>None</code> <code>use_jsonl</code> <code>bool</code> <p>If True, read train/test as JSONL when path or jsonl path is set</p> <code>False</code> <code>use_pickle</code> <code>bool</code> <p>If True, read train/test as pickle (original math objects)</p> <code>False</code> <code>train_dataset_jsonl</code> <code>str | None</code> <p>Optional path to training JSONL</p> <code>None</code> <code>test_dataset_jsonl</code> <code>str | None</code> <p>Optional path to test JSONL</p> <code>None</code> <code>train_dataset_pickle</code> <code>str | None</code> <p>Optional path to training pickle</p> <code>None</code> <code>test_dataset_pickle</code> <code>str | None</code> <p>Optional path to test pickle</p> <code>None</code> <code>dataset_load_preprocessor</code> <code>DatasetLoadPreprocessor | None</code> <p>Optional load-time preprocessor (user-provided or library)</p> <code>None</code> <code>display_samples</code> <code>int | None</code> <p>If set and &gt; 0, print this many train samples: raw (before load preprocessor), which preprocessor is applied (if any), and after preprocessor. 0 or None to disable.</p> <code>None</code> Source code in <code>src/calt/io/pipeline.py</code> <pre><code>def __init__(\n    self,\n    train_dataset_path: str | None = None,\n    test_dataset_path: str | None = None,\n    num_train_samples: int | None = None,\n    num_test_samples: int | None = None,\n    vocab_config: VocabConfig | dict | str | None = None,\n    preprocessor: AbstractPreProcessor | None = None,\n    validate_train_tokens: bool = False,\n    validate_test_tokens: bool = False,\n    use_jsonl: bool = False,\n    use_pickle: bool = False,\n    train_dataset_jsonl: str | None = None,\n    test_dataset_jsonl: str | None = None,\n    train_dataset_pickle: str | None = None,\n    test_dataset_pickle: str | None = None,\n    dataset_load_preprocessor: DatasetLoadPreprocessor | None = None,\n    display_samples: int | None = None,\n):\n    \"\"\"Initialize IOPipeline.\n\n    Args:\n        train_dataset_path: Path to training dataset file (.txt, .jsonl, or .pkl)\n        test_dataset_path: Path to test dataset file\n        num_train_samples: Maximum number of training samples to load\n        num_test_samples: Maximum number of test samples to load\n        vocab_config: VocabConfig, dict, or path to YAML file\n        preprocessor: Lexer/preprocessor instance (optional)\n        use_jsonl: If True, read train/test as JSONL when path or jsonl path is set\n        use_pickle: If True, read train/test as pickle (original math objects)\n        train_dataset_jsonl: Optional path to training JSONL\n        test_dataset_jsonl: Optional path to test JSONL\n        train_dataset_pickle: Optional path to training pickle\n        test_dataset_pickle: Optional path to test pickle\n        dataset_load_preprocessor: Optional load-time preprocessor (user-provided or library)\n        display_samples: If set and &gt; 0, print this many train samples: raw (before load\n            preprocessor), which preprocessor is applied (if any), and after preprocessor.\n            0 or None to disable.\n    \"\"\"\n    self.train_dataset_path = train_dataset_path\n    self.test_dataset_path = test_dataset_path\n    self.num_train_samples = num_train_samples\n    self.num_test_samples = num_test_samples\n    self.vocab_config = self.get_vocab_config(vocab_config)\n    self.preprocessor = preprocessor\n    self.validate_train_tokens = validate_train_tokens\n    self.validate_test_tokens = validate_test_tokens\n    self.use_jsonl = use_jsonl\n    self.use_pickle = use_pickle\n    self.train_dataset_jsonl = train_dataset_jsonl\n    self.test_dataset_jsonl = test_dataset_jsonl\n    self.train_dataset_pickle = train_dataset_pickle\n    self.test_dataset_pickle = test_dataset_pickle\n    self.dataset_load_preprocessor = dataset_load_preprocessor\n    self.display_samples = display_samples\n    # Store config dicts for checkpoint saving\n    self.lexer_config_dict: dict | None = None\n    self.vocab_config_dict: dict | None = None\n</code></pre> <p>For visualization of evaluation results (predictions vs references), see Visualization.</p>"},{"location":"io_pipeline/#calt.io.IOPipeline.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: DictConfig) -&gt; IOPipeline\n</code></pre> <p>Create IOPipeline from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>Data configuration from cfg.data (OmegaConf). Must include: - lexer_config: str path to lexer.yaml file (required)</p> required <p>Returns:</p> Name Type Description <code>IOPipeline</code> <code>IOPipeline</code> <p>IOPipeline instance configured from the config.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from omegaconf import OmegaConf\n&gt;&gt;&gt; from calt.io import IOPipeline\n&gt;&gt;&gt;\n&gt;&gt;&gt; cfg = OmegaConf.load(\"config/train.yaml\")\n&gt;&gt;&gt; io_pipeline = IOPipeline.from_config(cfg.data)\n</code></pre> Source code in <code>src/calt/io/pipeline.py</code> <pre><code>@classmethod\ndef from_config(cls, config: DictConfig) -&gt; \"IOPipeline\":\n    \"\"\"Create IOPipeline from configuration.\n\n    Args:\n        config (DictConfig): Data configuration from cfg.data (OmegaConf).\n            Must include:\n            - lexer_config: str path to lexer.yaml file (required)\n\n    Returns:\n        IOPipeline: IOPipeline instance configured from the config.\n\n    Examples:\n        &gt;&gt;&gt; from omegaconf import OmegaConf\n        &gt;&gt;&gt; from calt.io import IOPipeline\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; cfg = OmegaConf.load(\"config/train.yaml\")\n        &gt;&gt;&gt; io_pipeline = IOPipeline.from_config(cfg.data)\n    \"\"\"\n    lexer_config_path = config.get(\"lexer_config\")\n    if lexer_config_path is None:\n        raise ValueError(\"lexer_config must be provided\")\n\n    # Resolve lexer config path (support relative paths)\n    lexer_config_path_obj = Path(lexer_config_path)\n    if not lexer_config_path_obj.is_absolute():\n        # Try to resolve relative to current working directory first\n        if not lexer_config_path_obj.exists():\n            # If not found, try relative to config file location if available\n            # (This is a best-effort approach)\n            pass\n    lexer_config_path = str(lexer_config_path_obj.resolve())\n\n    # Load lexer config\n    with open(lexer_config_path, \"r\") as f:\n        lexer_config = yaml.safe_load(f)\n\n    # Create VocabConfig from lexer config\n    vocab_config_dict = lexer_config.get(\"vocab\", {})\n    vocab_config = VocabConfig([], {}).from_config(vocab_config_dict)\n\n    # Create NumberPolicy from lexer config\n    number_policy_dict = lexer_config.get(\"number_policy\", {})\n    # attach_sign: true = attach sign to number, false = separate sign as token\n    attach_sign = number_policy_dict.get(\"attach_sign\", True)  # default: attach\n    number_policy = NumberPolicy(\n        sign=attach_sign,  # sign=True means attach, sign=False means separate\n        digit_group=number_policy_dict.get(\"digit_group\", 0),\n        allow_float=number_policy_dict.get(\"allow_float\", True),\n    )\n\n    # Create UnifiedLexer (vocab extension is handled inside UnifiedLexer.__init__)\n    preprocessor = UnifiedLexer(\n        vocab_config=vocab_config,\n        number_policy=number_policy,\n        strict=lexer_config.get(\"strict\", True),\n        include_base_vocab=lexer_config.get(\"include_base_vocab\", True),\n    )\n\n    # Use the extended vocab_config from lexer (includes auto-added tokens for floats)\n    vocab_config_path = preprocessor.vocab_config\n\n    use_jsonl = config.get(\"use_jsonl\", False)\n    use_pickle = config.get(\"use_pickle\", False)\n    train_jsonl = config.get(\"train_dataset_jsonl\")\n    test_jsonl = config.get(\"test_dataset_jsonl\")\n    train_pickle = config.get(\"train_dataset_pickle\")\n    test_pickle = config.get(\"test_dataset_pickle\")\n    dataset_load_preprocessor = config.get(\"dataset_load_preprocessor\")\n    display_samples = config.get(\"display_samples\")\n\n    # Create instance\n    instance = cls(\n        train_dataset_path=config.get(\"train_dataset_path\"),\n        test_dataset_path=config.get(\"test_dataset_path\"),\n        num_train_samples=config.get(\"num_train_samples\", -1),\n        num_test_samples=config.get(\"num_test_samples\", -1),\n        vocab_config=vocab_config_path,\n        preprocessor=preprocessor,\n        validate_train_tokens=config.get(\"validate_train_tokens\", False),\n        validate_test_tokens=config.get(\"validate_test_tokens\", True),\n        use_jsonl=use_jsonl,\n        use_pickle=use_pickle,\n        train_dataset_jsonl=train_jsonl,\n        test_dataset_jsonl=test_jsonl,\n        train_dataset_pickle=train_pickle,\n        test_dataset_pickle=test_pickle,\n        dataset_load_preprocessor=dataset_load_preprocessor,\n        display_samples=display_samples,\n    )\n\n    # Store config dicts for checkpoint saving\n    # Store the original lexer_config (includes vocab, number_policy, strict, etc.)\n    instance.lexer_config_dict = lexer_config\n    # Store vocab_config dict (from lexer config, before extension)\n    instance.vocab_config_dict = vocab_config_dict\n\n    return instance\n</code></pre>"},{"location":"io_pipeline/#calt.io.IOPipeline.validate_tokens","title":"validate_tokens","text":"<pre><code>validate_tokens(dataset: StandardDataset)\n</code></pre> <p>Validate tokens in a dataset and raise error if out-of-vocabulary tokens are found.</p> Source code in <code>src/calt/io/pipeline.py</code> <pre><code>def validate_tokens(self, dataset: StandardDataset):\n    \"\"\"Validate tokens in a dataset and raise error if out-of-vocabulary tokens are found.\"\"\"\n\n    out_of_vocab_tokens = validate_dataset_tokens(\n        lexer=self.preprocessor,\n        vocab_config=self.vocab_config,\n        input_texts=dataset.input_texts,\n        target_texts=dataset.target_texts,\n    )\n\n    if out_of_vocab_tokens:\n        token_list = \", \".join([f\"'{token}'\" for token in out_of_vocab_tokens])\n        error_msg = (\n            \"\\n--------------------------------\\n\"\n            f\"Vocabulary validation errors in dataset.\\n\"\n            f\"Out-of-vocabulary tokens: {token_list}\\n\"\n            f\"Please check your lexer.yaml configuration and dataset generation.\"\n            \"\\n--------------------------------\\n\"\n        )\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"io_pipeline/#calt.io.IOPipeline.build","title":"build","text":"<pre><code>build()\n</code></pre> <p>Build the data pipeline by loading the raw text data, applying the preprocessor, and setting the collator.</p> Source code in <code>src/calt/io/pipeline.py</code> <pre><code>def build(self):\n    \"\"\"\n    Build the data pipeline by loading the raw text data, applying the preprocessor, and setting the collator.\n    \"\"\"\n    # config = self.config\n\n    # Step 1: Load data (text, JSONL, or pickle) and apply load-time preprocessor if any\n    train_path = (\n        self.train_dataset_pickle\n        or self.train_dataset_jsonl\n        or self.train_dataset_path\n    )\n    test_path = (\n        self.test_dataset_pickle\n        or self.test_dataset_jsonl\n        or self.test_dataset_path\n    )\n    train_use_pickle = self.use_pickle or (\n        self.train_dataset_pickle is not None\n        or (train_path and str(train_path).endswith(\".pkl\"))\n    )\n    test_use_pickle = self.use_pickle or (\n        self.test_dataset_pickle is not None\n        or (test_path and str(test_path).endswith(\".pkl\"))\n    )\n    train_use_jsonl = self.use_jsonl or (\n        self.train_dataset_jsonl is not None\n        or (train_path and str(train_path).endswith(\".jsonl\"))\n    )\n    test_use_jsonl = self.use_jsonl or (\n        self.test_dataset_jsonl is not None\n        or (test_path and str(test_path).endswith(\".jsonl\"))\n    )\n    train_preprocessor = self.dataset_load_preprocessor\n    if train_preprocessor is None and (train_use_jsonl or train_use_pickle):\n        from .preprocessor.load_preprocessor import (\n            JsonlDefaultLoadPreprocessor,\n            PickleDefaultLoadPreprocessor,\n        )\n\n        train_preprocessor = (\n            PickleDefaultLoadPreprocessor()\n            if train_use_pickle\n            else JsonlDefaultLoadPreprocessor()\n        )\n    test_preprocessor = self.dataset_load_preprocessor\n    if test_preprocessor is None and (test_use_jsonl or test_use_pickle):\n        from .preprocessor.load_preprocessor import (\n            JsonlDefaultLoadPreprocessor,\n            PickleDefaultLoadPreprocessor,\n        )\n\n        test_preprocessor = (\n            PickleDefaultLoadPreprocessor()\n            if test_use_pickle\n            else JsonlDefaultLoadPreprocessor()\n        )\n    n_show = self.display_samples if self.display_samples is not None else 0\n    # When display_samples &gt; 0 and plain txt: load and show raw (before any load preprocessor)\n    if n_show &gt; 0 and not train_use_jsonl and not train_use_pickle and train_path:\n        raw_inputs, raw_targets = read_data_from_file(\n            train_path, max_samples=self.num_train_samples\n        )\n        n_raw = min(n_show, len(raw_inputs))\n        print(\n            f\"[Display] Raw (before any load preprocessor): {len(raw_inputs)} samples, \"\n            f\"showing first {n_raw}:\"\n        )\n        for i in range(n_raw):\n            inp = (\n                raw_inputs[i]\n                if len(raw_inputs[i]) &lt;= 50\n                else raw_inputs[i][:47] + \"...\"\n            )\n            tgt = (\n                raw_targets[i]\n                if len(raw_targets[i]) &lt;= 50\n                else raw_targets[i][:47] + \"...\"\n            )\n            print(f\"  [{i}] input:  {inp!r}\")\n            print(f\"      target: {tgt!r}\")\n        print()\n    train_dataset = StandardDataset.load_file(\n        train_path,\n        self.preprocessor,\n        self.num_train_samples,\n        use_jsonl=train_use_jsonl,\n        use_pickle=train_use_pickle,\n        dataset_load_preprocessor=train_preprocessor,\n    )\n    test_dataset = StandardDataset.load_file(\n        test_path,\n        self.preprocessor,\n        self.num_test_samples,\n        use_jsonl=test_use_jsonl,\n        use_pickle=test_use_pickle,\n        dataset_load_preprocessor=test_preprocessor,\n    )\n\n    if self.validate_train_tokens:\n        print(\"Validating training dataset tokens...\", end=\" \")\n        self.validate_tokens(train_dataset)\n        print(\"passed!\")\n    if self.validate_test_tokens:\n        print(\"Validating test dataset tokens...\", end=\" \")\n        self.validate_tokens(test_dataset)\n        print(\"passed!\")\n\n    # Display samples: preprocessor description (if any) and after-preprocessor samples\n    if n_show &gt; 0:\n        if train_preprocessor is not None:\n            name = type(train_preprocessor).__name__\n            if hasattr(train_preprocessor, \"preprocessors\"):\n                chain = \", \".join(\n                    type(p).__name__ for p in train_preprocessor.preprocessors\n                )\n                name = f\"{name}({chain})\"\n            print(f\"[Display] Load preprocessor: {name}\")\n        else:\n            print(\"[Display] No load preprocessor applied.\")\n        n_after = min(n_show, len(train_dataset.input_texts))\n        print(\n            f\"[Display] After load preprocessor: {len(train_dataset.input_texts)} samples, \"\n            f\"showing first {n_after}:\"\n        )\n        for i in range(n_after):\n            inp = train_dataset.input_texts[i]\n            tgt = train_dataset.target_texts[i]\n            inp_short = inp if len(inp) &lt;= 50 else inp[:47] + \"...\"\n            tgt_short = tgt if len(tgt) &lt;= 50 else tgt[:47] + \"...\"\n            print(f\"  [{i}] input:  {inp_short!r}\")\n            print(f\"      target: {tgt_short!r}\")\n        print()\n\n    # Step 2: Set collator that will transform the processed data into tokens (or token ids)\n    #         This will be called every time at the beginning of each epoch\n    # e.g.,\n    # processed data: \"C2 E1 E2 + C5 E1 E0 + C-3 E0 E0\"\n    # tokens: [\"C2\", \"E1\", \"E2\", \"C5\", \"E1\", \"E0\", \"C-3\", \"E0\", \"E0\"]\n    if self.vocab_config is None:\n        raise ValueError(\"vocab_config must be provided to build the tokenizer\")\n    tokenizer = get_tokenizer(vocab_config=self.vocab_config)\n    data_collator = StandardDataCollator(tokenizer=tokenizer)\n\n    self.train_dataset = train_dataset\n    self.test_dataset = test_dataset\n    self.tokenizer = tokenizer\n    self.data_collator = data_collator\n\n    self.io_dict = {\n        \"train_dataset\": train_dataset,\n        \"test_dataset\": test_dataset,\n        \"tokenizer\": tokenizer,\n        \"data_collator\": data_collator,\n    }\n\n    return self.io_dict\n</code></pre>"},{"location":"io_visualization/","title":"Visualization","text":"<p>Utilities to quickly render visual diffs between model predictions and references. Useful after evaluation to inspect outputs.</p>"},{"location":"io_visualization/#calt.io.visualization.comparison_vis.display_with_diff","title":"display_with_diff","text":"<pre><code>display_with_diff(\n    gold: Expr | str,\n    pred: Expr | str,\n    var_order: Sequence[Symbol] | None = None,\n) -&gt; None\n</code></pre> <p>Render \"gold\" vs. \"pred\" with strikethrough on mistakes in \"pred\".</p> <p>Parameters:</p> Name Type Description Default <code>gold</code> <code>Expr | str</code> <p>Ground-truth expression. If a string, it will be parsed as a token sequence (e.g., \"C1 E1 E1 C-3 E0 E7\") via <code>parse_poly</code>.</p> required <code>pred</code> <code>Expr | str</code> <p>Model-predicted expression. If a string, it will be parsed as a token sequence via <code>parse_poly</code>.</p> required <code>var_order</code> <code>Sequence[Symbol] | None</code> <p>Variable ordering (important for &gt;2 variables). Inferred if None. Also passed to <code>parse_poly</code> if inputs are strings. Defaults to None.</p> <code>None</code> Source code in <code>src/calt/io/visualization/comparison_vis.py</code> <pre><code>def display_with_diff(\n    gold: Expr | str,\n    pred: Expr | str,\n    var_order: Sequence[Symbol] | None = None,\n) -&gt; None:\n    \"\"\"Render \"gold\" vs. \"pred\" with strikethrough on mistakes in \"pred\".\n\n    Args:\n        gold (sympy.Expr | str):\n            Ground-truth expression. If a string, it will be parsed as a token\n            sequence (e.g., \"C1 E1 E1 C-3 E0 E7\") via ``parse_poly``.\n        pred (sympy.Expr | str):\n            Model-predicted expression. If a string, it will be parsed as a token\n            sequence via ``parse_poly``.\n        var_order (Sequence[sympy.Symbol] | None, optional):\n            Variable ordering (important for &gt;2 variables). Inferred if None. Also\n            passed to ``parse_poly`` if inputs are strings. Defaults to None.\n    \"\"\"\n\n    # --- input conversion ------------------------------------------------- #\n    if isinstance(gold, str):\n        gold = parse_poly(gold, var_names=var_order)\n    if isinstance(pred, str):\n        pred = parse_poly(pred, var_names=var_order)\n\n    # --- normalize -------------------------------------------------------- #\n    if var_order is None:\n        var_order = sorted(\n            gold.free_symbols.union(pred.free_symbols), key=lambda s: s.name\n        )\n    gold_poly = Poly(gold.expand(), *var_order)\n    pred_poly = Poly(pred.expand(), *var_order)\n\n    gdict = _poly_to_dict(gold_poly)\n    pdict = _poly_to_dict(pred_poly)\n\n    # --- diff detection --------------------------------------------------- #\n    diff: dict[tuple[int, ...], str] = {}\n    for exps in set(gdict) | set(pdict):\n        gcoeff = gdict.get(exps, 0)\n        pcoeff = pdict.get(exps, 0)\n        if pcoeff == 0 and gcoeff != 0:\n            continue  # missing term (not highlighted)\n        if gcoeff == 0 and pcoeff != 0:\n            diff[exps] = \"extra\"\n        elif gcoeff != pcoeff:\n            diff[exps] = \"coeff_wrong\"\n\n    # --- render ----------------------------------------------------------- #\n    gold_tex = latex(gold.expand())\n    pred_tex = _build_poly_latex(pdict, var_order, diff)\n\n    display(\n        Math(\n            r\"\"\"\\begin{aligned}\n        \\text{Ground truth\\,:}\\; &amp; {}\"\"\"\n            + gold_tex\n            + r\"\"\"\\\\\n        \\text{Prediction\\,:}\\;   &amp; {}\"\"\"\n            + pred_tex\n            + r\"\"\"\n        \\end{aligned}\"\"\"\n        )\n    )\n</code></pre>"},{"location":"io_visualization/#calt.io.visualization.comparison_vis.load_eval_results","title":"load_eval_results","text":"<pre><code>load_eval_results(file_path: str) -&gt; tuple[list[str], list[str]]\n</code></pre> <p>Load evaluation results from a JSON file.</p> <p>The JSON file should contain a list of objects with \"generated\" and \"reference\" keys.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the JSON file.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>tuple[list[str], list[str]]: A tuple containing two lists: - List of generated texts. - List of reference texts.</p> Source code in <code>src/calt/io/visualization/comparison_vis.py</code> <pre><code>def load_eval_results(file_path: str) -&gt; tuple[list[str], list[str]]:\n    \"\"\"Load evaluation results from a JSON file.\n\n    The JSON file should contain a list of objects with \"generated\" and \"reference\" keys.\n\n    Args:\n        file_path (str): Path to the JSON file.\n\n    Returns:\n        tuple[list[str], list[str]]: A tuple containing two lists:\n            - List of generated texts.\n            - List of reference texts.\n    \"\"\"\n    generated_texts = []\n    reference_texts = []\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    for item in data:\n        generated_texts.append(item.get(\"generated\", \"\"))\n        reference_texts.append(item.get(\"reference\", \"\"))\n\n    return generated_texts, reference_texts\n</code></pre>"},{"location":"model_pipeline/","title":"Model pipeline","text":"<p><code>ModelPipeline</code> builds a sequence-to-sequence model from the <code>model</code> block of your config and the tokenizer produced by IOPipeline. It is used after <code>IOPipeline.build()</code> and before TrainerPipeline.</p> <ul> <li>Overview \u2014 how the three pipelines (IO, Model, Trainer) fit together.</li> <li>Configuration \u2014 the <code>model</code> block in <code>train.yaml</code> and its keys.</li> </ul>"},{"location":"model_pipeline/#modelpipeline","title":"ModelPipeline","text":"<p>Use <code>ModelPipeline.from_io_dict(cfg.model, io_dict)</code> to create a pipeline from the result of <code>IOPipeline.from_config(cfg.data).build()</code>. The tokenizer is taken from <code>io_dict[\"tokenizer\"]</code>. Call <code>.build()</code> to obtain the <code>PreTrainedModel</code> instance.</p> <p>Pipeline for creating models from configuration using ModelRegistry.</p> <p>Similar to IOPipeline, this class provides a simple interface for creating model instances from config files. It uses ModelRegistry internally to handle model creation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from omegaconf import OmegaConf\n&gt;&gt;&gt; from calt.models import ModelPipeline\n&gt;&gt;&gt;\n&gt;&gt;&gt; cfg = OmegaConf.load(\"config/train.yaml\")\n&gt;&gt;&gt; tokenizer = ...  # Get tokenizer from IOPipeline\n&gt;&gt;&gt;\n&gt;&gt;&gt; model_pipeline = ModelPipeline(cfg.model, tokenizer)\n&gt;&gt;&gt; model = model_pipeline.build()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>calt_config</code> <code>DictConfig</code> <p>Model configuration from cfg.model (OmegaConf).</p> required <code>tokenizer</code> <code>PreTrainedTokenizerFast | None</code> <p>Tokenizer instance (required for some models).</p> <code>None</code> Source code in <code>src/calt/models/pipeline.py</code> <pre><code>def __init__(\n    self,\n    calt_config: DictConfig,\n    tokenizer: Optional[PreTrainedTokenizerFast] = None,\n):\n    \"\"\"Initialize the model pipeline.\n\n    Args:\n        calt_config (DictConfig): Model configuration from cfg.model (OmegaConf).\n        tokenizer (PreTrainedTokenizerFast | None): Tokenizer instance (required for some models).\n    \"\"\"\n    self.calt_config = calt_config\n    self.tokenizer = tokenizer\n    self.model: Optional[PreTrainedModel] = None\n    self._registry = ModelRegistry()\n</code></pre>"},{"location":"model_pipeline/#calt.models.pipeline.ModelPipeline.from_io_dict","title":"from_io_dict  <code>classmethod</code>","text":"<pre><code>from_io_dict(calt_config: DictConfig, io_dict: dict) -&gt; ModelPipeline\n</code></pre> <p>Create a ModelPipeline using the result dict from IOPipeline.build().</p> <p>Parameters:</p> Name Type Description Default <code>calt_config</code> <code>DictConfig</code> <p>Model configuration (cfg.model).</p> required <code>io_dict</code> <code>dict</code> <p>Result dict from <code>IOPipeline.build()</code>, expected to contain at least the <code>\"tokenizer\"</code> entry.</p> required Source code in <code>src/calt/models/pipeline.py</code> <pre><code>@classmethod\ndef from_io_dict(\n    cls,\n    calt_config: DictConfig,\n    io_dict: dict,\n) -&gt; \"ModelPipeline\":\n    \"\"\"Create a ModelPipeline using the result dict from IOPipeline.build().\n\n    Args:\n        calt_config: Model configuration (cfg.model).\n        io_dict: Result dict from ``IOPipeline.build()``, expected to contain\n            at least the ``\\\"tokenizer\\\"`` entry.\n    \"\"\"\n    return cls(\n        calt_config=calt_config,\n        tokenizer=io_dict[\"tokenizer\"],\n    )\n</code></pre>"},{"location":"model_pipeline/#calt.models.pipeline.ModelPipeline.build","title":"build","text":"<pre><code>build() -&gt; PreTrainedModel\n</code></pre> <p>Build the model from configuration using ModelRegistry.</p> <p>Returns:</p> Name Type Description <code>PreTrainedModel</code> <code>PreTrainedModel</code> <p>Model instance.</p> Source code in <code>src/calt/models/pipeline.py</code> <pre><code>def build(self) -&gt; PreTrainedModel:\n    \"\"\"Build the model from configuration using ModelRegistry.\n\n    Returns:\n        PreTrainedModel: Model instance.\n    \"\"\"\n    # Use ModelRegistry to create the model\n    self.model = self._registry.create_from_config(\n        model_config=self.calt_config,\n        tokenizer=self.tokenizer,\n    )\n    return self.model\n</code></pre>"},{"location":"model_pipeline/#supported-model-types","title":"Supported model types","text":"<p>Models are created via an internal <code>ModelRegistry</code>. The following types are registered by default:</p> <code>model_type</code> Description <code>generic</code>, <code>transformer</code>, <code>calt</code> CALT generic Transformer (encoder\u2013decoder). <code>bart</code> HuggingFace BART for conditional generation. <p>Set <code>model_type</code> in the <code>model</code> block of <code>train.yaml</code> (e.g. <code>model_type: generic</code>). Other keys in the <code>model</code> block (e.g. <code>num_encoder_layers</code>, <code>d_model</code>, <code>max_sequence_length</code>) are documented under Configuration \u2014 <code>model</code>.</p>"},{"location":"model_pipeline/#modelregistry","title":"ModelRegistry","text":"<p>To create a model without using the pipeline (e.g. with a custom config), you can use the registry or helpers from <code>calt.models</code>: <code>ModelRegistry</code>, <code>get_model_from_config</code>. See the API reference below.</p> <p>Registry for creating model instances based on model type.</p> <p>This class provides a unified interface for creating different types of models. Models can be registered and retrieved by name or inferred from config.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create model with explicit name and config\n&gt;&gt;&gt; from calt.models.generic import TransformerConfig\n&gt;&gt;&gt; registry = ModelRegistry()\n&gt;&gt;&gt; config = TransformerConfig(vocab_size=1000, d_model=128)\n&gt;&gt;&gt; model = registry.create(\"transformer\", config)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model from config only (model_type inferred from config)\n&gt;&gt;&gt; model = registry.create(model_config=config)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Register custom model\n&gt;&gt;&gt; registry.register(\"custom_model\", CustomModel, CustomModelConfig)\n</code></pre> Source code in <code>src/calt/models/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the registry with default model types.\"\"\"\n    self._registry: dict[\n        str, tuple[Type[PreTrainedModel], Type[PretrainedConfig]]\n    ] = {}\n    self._config_mappings: dict[str, Callable] = {}\n    self._register_defaults()\n</code></pre>"},{"location":"model_pipeline/#calt.models.base.ModelRegistry.create_from_config","title":"create_from_config","text":"<pre><code>create_from_config(\n    model_config: DictConfig,\n    tokenizer: Optional[PreTrainedTokenizerFast] = None,\n    model_name: Optional[str] = None,\n) -&gt; PreTrainedModel\n</code></pre> <p>Create a model instance from OmegaConf config (cfg.model).</p> <p>This method automatically converts the unified config format to model-specific configs using registered config mapping functions.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>DictConfig</code> <p>Model configuration from cfg.model (OmegaConf).</p> required <code>tokenizer</code> <code>PreTrainedTokenizerFast | None</code> <p>Tokenizer instance (required for some models like BART).</p> <code>None</code> <code>model_name</code> <code>str | None</code> <p>Name of the model type. If None, will be inferred from model_config.model_type.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>PreTrainedModel</code> <code>PreTrainedModel</code> <p>Model instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_name is not supported or cannot be inferred from config.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create model from OmegaConf config\n&gt;&gt;&gt; from omegaconf import OmegaConf\n&gt;&gt;&gt; cfg = OmegaConf.load(\"config/train.yaml\")\n&gt;&gt;&gt; registry = ModelRegistry()\n&gt;&gt;&gt; model = registry.create_from_config(cfg.model, tokenizer)\n</code></pre> Source code in <code>src/calt/models/base.py</code> <pre><code>def create_from_config(\n    self,\n    model_config: DictConfig,\n    tokenizer: Optional[PreTrainedTokenizerFast] = None,\n    model_name: Optional[str] = None,\n) -&gt; PreTrainedModel:\n    \"\"\"Create a model instance from OmegaConf config (cfg.model).\n\n    This method automatically converts the unified config format to model-specific configs\n    using registered config mapping functions.\n\n    Args:\n        model_config (DictConfig): Model configuration from cfg.model (OmegaConf).\n        tokenizer (PreTrainedTokenizerFast | None): Tokenizer instance (required for some models like BART).\n        model_name (str | None): Name of the model type. If None, will be inferred from model_config.model_type.\n\n    Returns:\n        PreTrainedModel: Model instance.\n\n    Raises:\n        ValueError: If model_name is not supported or cannot be inferred from config.\n\n    Examples:\n        &gt;&gt;&gt; # Create model from OmegaConf config\n        &gt;&gt;&gt; from omegaconf import OmegaConf\n        &gt;&gt;&gt; cfg = OmegaConf.load(\"config/train.yaml\")\n        &gt;&gt;&gt; registry = ModelRegistry()\n        &gt;&gt;&gt; model = registry.create_from_config(cfg.model, tokenizer)\n    \"\"\"\n    # Determine model name\n    if model_name is None:\n        if hasattr(model_config, \"model_type\"):\n            model_name = model_config.model_type\n        else:\n            raise ValueError(\n                \"Cannot infer model_name from model_config. \"\n                \"Please provide model_name explicitly or set model_config.model_type.\"\n            )\n\n    model_name = model_name.lower() if isinstance(model_name, str) else model_name\n\n    # Get model class from registry\n    if model_name not in self._registry:\n        supported_types = list(self._registry.keys())\n        raise ValueError(\n            f\"Unsupported model type: {model_name}. \"\n            f\"Supported types: {supported_types}\"\n        )\n\n    model_class, config_class = self._registry[model_name]\n\n    # Get config mapping function\n    if model_name not in self._config_mappings:\n        raise ValueError(\n            f\"No config mapping registered for model type: {model_name}. \"\n            f\"Please register a config mapping using register_config_mapping().\"\n        )\n\n    mapping_func = self._config_mappings[model_name]\n\n    # Convert OmegaConf config to model-specific config\n    converted_config = mapping_func(model_config, tokenizer)\n\n    # Create and return model\n    return model_class(config=converted_config)\n</code></pre>"},{"location":"model_pipeline/#calt.models.base.ModelRegistry.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; list[str]\n</code></pre> <p>List all registered model types.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of registered model type names.</p> Source code in <code>src/calt/models/base.py</code> <pre><code>def list_models(self) -&gt; list[str]:\n    \"\"\"List all registered model types.\n\n    Returns:\n        list[str]: List of registered model type names.\n    \"\"\"\n    return list(self._registry.keys())\n</code></pre>"},{"location":"model_pipeline/#calt.models.base.ModelRegistry.register","title":"register","text":"<pre><code>register(\n    model_name: str,\n    model_class: Type[PreTrainedModel],\n    config_class: Type[PretrainedConfig],\n)\n</code></pre> <p>Register a model class with the registry.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name to register the model under.</p> required <code>model_class</code> <code>Type[PreTrainedModel]</code> <p>Model class to register.</p> required <code>config_class</code> <code>Type[PretrainedConfig]</code> <p>Config class for the model.</p> required Source code in <code>src/calt/models/base.py</code> <pre><code>def register(\n    self,\n    model_name: str,\n    model_class: Type[PreTrainedModel],\n    config_class: Type[PretrainedConfig],\n):\n    \"\"\"Register a model class with the registry.\n\n    Args:\n        model_name (str): Name to register the model under.\n        model_class (Type[PreTrainedModel]): Model class to register.\n        config_class (Type[PretrainedConfig]): Config class for the model.\n    \"\"\"\n    self._registry[model_name.lower()] = (model_class, config_class)\n</code></pre>"},{"location":"model_pipeline/#calt.models.base.ModelRegistry.register_config_mapping","title":"register_config_mapping","text":"<pre><code>register_config_mapping(\n    model_name: str,\n    mapping_func: Callable[\n        [DictConfig, Optional[PreTrainedTokenizerFast]], PretrainedConfig\n    ],\n)\n</code></pre> <p>Register a config mapping function for converting OmegaConf to model config.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model type.</p> required <code>mapping_func</code> <code>Callable</code> <p>Function that takes (model_config: DictConfig, tokenizer: Optional) and returns PretrainedConfig.</p> required Source code in <code>src/calt/models/base.py</code> <pre><code>def register_config_mapping(\n    self,\n    model_name: str,\n    mapping_func: Callable[\n        [DictConfig, Optional[PreTrainedTokenizerFast]], PretrainedConfig\n    ],\n):\n    \"\"\"Register a config mapping function for converting OmegaConf to model config.\n\n    Args:\n        model_name (str): Name of the model type.\n        mapping_func (Callable): Function that takes (model_config: DictConfig, tokenizer: Optional)\n            and returns PretrainedConfig.\n    \"\"\"\n    self._config_mappings[model_name.lower()] = mapping_func\n</code></pre>"},{"location":"trainer/","title":"Overview","text":"<p>A convenient extension of the HuggingFace <code>Trainer</code> and utility helpers for training and evaluation. It streamlines device placement, metrics computation, and generation result saving.</p> <ul> <li>Model pipeline \u2014 builds the model from configuration; <code>cfg.model</code> and supported model types.</li> <li>Configuration \u2014 <code>data.yaml</code>, <code>lexer.yaml</code>, and <code>train.yaml</code>.</li> </ul>"},{"location":"trainer/#calt.trainer.trainer.Trainer","title":"Trainer","text":"<pre><code>Trainer(*args, **kwargs)\n</code></pre> <p>               Bases: <code>Trainer</code></p> <p>Extension of HuggingFace :class:<code>~transformers.Trainer</code>.</p> <p>The trainer adds task-specific helpers that simplify training generative Transformer models. It accepts all the usual <code>HTrainer</code> keyword arguments and does not introduce new parameters - the default constructor is therefore forwarded verbatim.</p> Source code in <code>src/calt/trainer/trainer.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    # Keeps a chronological list of metric dictionaries that WandB has\n    # seen.  This enables the caller to inspect the *complete* training\n    # history after the run has finished without having to query WandB.\n    self.log_history = []\n\n    if self.compute_metrics is None:\n        self.compute_metrics = self._compute_metrics\n</code></pre>"},{"location":"trainer/#calt.trainer.trainer.Trainer.evaluate","title":"evaluate","text":"<pre><code>evaluate(eval_dataset=None, ignore_keys=None, metric_key_prefix='eval')\n</code></pre> <p>Override evaluate to also save generation results during training.</p> <p>This method is called during training evaluation steps and after training. It runs the standard evaluation and then saves generation results.</p> Source code in <code>src/calt/trainer/trainer.py</code> <pre><code>def evaluate(\n    self,\n    eval_dataset=None,\n    ignore_keys=None,\n    metric_key_prefix=\"eval\",\n):\n    \"\"\"Override evaluate to also save generation results during training.\n\n    This method is called during training evaluation steps and after training.\n    It runs the standard evaluation and then saves generation results.\n    \"\"\"\n    # Run standard evaluation\n    metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n\n    # Also save generation results during evaluation\n    # Get current step number if available\n    step = getattr(self.state, \"global_step\", None)\n    logger.info(\n        f\"Running evaluate_and_save_generation (step={step}, metric_key_prefix={metric_key_prefix})\"\n    )\n    try:\n        # Pass step number to evaluate_and_save_generation\n        success_rate = self.evaluate_and_save_generation(step=step)\n        # Add generation metrics to the returned metrics dict\n        generation_metrics = {\n            f\"{metric_key_prefix}_generation_success_rate\": success_rate,\n        }\n        if step is not None:\n            generation_metrics[f\"{metric_key_prefix}_generation_step\"] = step\n        # Update the metrics dict\n        metrics.update(generation_metrics)\n        # Explicitly log only the generation metrics to ensure they are recorded\n        self.log(generation_metrics)\n        logger.info(\n            f\"Successfully saved generation results (step={step}, success_rate={success_rate:.4f})\"\n        )\n    except Exception as e:\n        # Log error but don't fail the evaluation\n        logger.warning(\n            f\"Failed to save generation results during evaluation: {e}\",\n            exc_info=True,\n        )\n\n    return metrics\n</code></pre>"},{"location":"trainer/#calt.trainer.trainer.Trainer.evaluate_and_save_generation","title":"evaluate_and_save_generation","text":"<pre><code>evaluate_and_save_generation(max_length: int = 512, step: int | None = None)\n</code></pre> <p>Run greedy/beam-search generation on the evaluation set.</p> <p>The helper decodes the model outputs into strings, stores the results in <code>eval_results.json</code> inside the trainer's output directory and finally computes exact-match accuracy between the generated and reference sequences.</p> <p>Parameters:</p> Name Type Description Default <code>max_length</code> <code>int</code> <p>Maximum generation length. Defaults to 512.</p> <code>512</code> <code>step</code> <code>int</code> <p>Current training step number. If None, tries to get from self.state.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>Exact-match accuracy in the [0, 1] interval.</p> Source code in <code>src/calt/trainer/trainer.py</code> <pre><code>def evaluate_and_save_generation(\n    self, max_length: int = 512, step: int | None = None\n):\n    \"\"\"Run greedy/beam-search generation on the evaluation set.\n\n    The helper decodes the model outputs into strings, stores the results in\n    ``eval_results.json`` inside the trainer's output directory and finally computes\n    exact-match accuracy between the generated and reference sequences.\n\n    Args:\n        max_length (int, optional): Maximum generation length. Defaults to 512.\n        step (int, optional): Current training step number. If None, tries to get from self.state.\n\n    Returns:\n        float: Exact-match accuracy in the [0, 1] interval.\n    \"\"\"\n    if self.eval_dataset is None:\n        raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n\n    if len(self.eval_dataset) == 0:\n        logger.warning(\n            \"eval_dataset is empty; skipping evaluate_and_save_generation.\"\n        )\n        return 0.0\n\n    all_generated_texts = []\n    all_reference_texts = []\n\n    eval_dataloader = self.get_eval_dataloader(self.eval_dataset)\n\n    self.model.eval()\n    tokenizer = self.processing_class\n\n    for batch in eval_dataloader:\n        if batch is None:\n            continue\n        inputs = self._prepare_inputs(batch)\n        if inputs is None:\n            continue\n        input_ids = inputs.get(\"input_ids\")\n        attention_mask = inputs.get(\"attention_mask\")\n        labels = inputs.get(\"labels\")\n\n        if input_ids is None:\n            continue\n\n        with torch.no_grad():\n            generated_ids = self.model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                # Optional: specify ``pad_token_id`` / ``eos_token_id`` as\n                # keyword arguments if the model configuration requires.\n            )\n\n        # generated_ids shape (batch_size, sequence_length)\n        current_generated_texts = tokenizer.batch_decode(\n            generated_ids,\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        )\n        all_generated_texts.extend(current_generated_texts)\n\n        if labels is not None:\n            labels_for_decode = labels.clone()\n            labels_for_decode[labels_for_decode == -100] = tokenizer.pad_token_id\n            current_reference_texts = tokenizer.batch_decode(\n                labels_for_decode,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=True,\n            )\n            all_reference_texts.extend(current_reference_texts)\n        else:\n            # Keep placeholder when reference labels are missing.\n            all_reference_texts.extend([\"\" for _ in current_generated_texts])\n\n    # Include step number in filename if available during training\n    if step is None:\n        step = getattr(self.state, \"global_step\", None)\n    if step is not None:\n        # Save step-wise results in a subdirectory\n        eval_results_dir = os.path.join(self.args.output_dir, \"eval_results\")\n        os.makedirs(eval_results_dir, exist_ok=True)\n        output_eval_file = os.path.join(\n            eval_results_dir,\n            f\"step_{step}.json\",\n        )\n    else:\n        output_eval_file = os.path.join(\n            self.args.output_dir,\n            \"eval_results.json\",\n        )\n    results = []\n    for gen_text, ref_text in zip(all_generated_texts, all_reference_texts):\n        results.append(\n            {\n                \"generated\": gen_text,\n                \"reference\": ref_text,\n            }\n        )\n\n    with open(output_eval_file, \"w\") as writer:\n        json.dump(\n            results,\n            writer,\n            indent=4,\n            ensure_ascii=False,\n        )\n\n    correct_predictions = 0\n    total_predictions = len(all_generated_texts)\n\n    if total_predictions == 0:\n        return 0.0\n\n    for gen_text, ref_text in zip(all_generated_texts, all_reference_texts):\n        if gen_text.strip() == ref_text.strip():\n            correct_predictions += 1\n\n    success_rate = correct_predictions / total_predictions\n\n    return success_rate\n</code></pre>"},{"location":"trainer/#trainerpipeline","title":"TrainerPipeline","text":"<p>The main entry point for building a trainer from config is <code>TrainerPipeline</code>. Use <code>TrainerPipeline.from_io_dict(cfg.train, model, io_dict)</code> then <code>.build()</code> to obtain a pipeline; call <code>.train()</code> to run training and <code>.evaluate_and_save_generation()</code> for evaluation.</p> <p>Pipeline for creating trainers from configuration.</p> <p>Similar to IOPipeline, this class provides a simple interface for creating trainer instances from config files. It automatically selects the appropriate TrainerLoader based on the config.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from omegaconf import OmegaConf\n&gt;&gt;&gt; from calt.trainer import TrainerPipeline\n&gt;&gt;&gt;\n&gt;&gt;&gt; cfg = OmegaConf.load(\"config/train.yaml\")\n&gt;&gt;&gt; model = ...  # Get model from ModelPipeline\n&gt;&gt;&gt; tokenizer = ...  # Get tokenizer from IOPipeline\n&gt;&gt;&gt; train_dataset = ...  # Get from IOPipeline\n&gt;&gt;&gt; eval_dataset = ...  # Get from IOPipeline\n&gt;&gt;&gt; data_collator = ...  # Get from IOPipeline\n&gt;&gt;&gt;\n&gt;&gt;&gt; trainer_pipeline = TrainerPipeline(\n...     cfg.train,\n...     model=model,\n...     tokenizer=tokenizer,\n...     train_dataset=train_dataset,\n...     eval_dataset=eval_dataset,\n...     data_collator=data_collator,\n... )\n&gt;&gt;&gt; trainer = trainer_pipeline.build()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>Training configuration from cfg.train (OmegaConf).</p> required <code>model</code> <code>PreTrainedModel | None</code> <p>Model instance.</p> <code>None</code> <code>tokenizer</code> <code>PreTrainedTokenizerFast | None</code> <p>Tokenizer instance.</p> <code>None</code> <code>train_dataset</code> <code>Dataset | None</code> <p>Training dataset.</p> <code>None</code> <code>eval_dataset</code> <code>Dataset | None</code> <p>Evaluation dataset.</p> <code>None</code> <code>data_collator</code> <code>StandardDataCollator | None</code> <p>Data collator.</p> <code>None</code> Source code in <code>src/calt/trainer/pipeline.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    model: Optional[PreTrainedModel] = None,\n    tokenizer: Optional[PreTrainedTokenizerFast] = None,\n    train_dataset: Optional[Dataset] = None,\n    eval_dataset: Optional[Dataset] = None,\n    data_collator: Optional[StandardDataCollator] = None,\n    wandb_config: Optional[DictConfig] = None,\n    io_dict: Optional[dict] = None,\n):\n    \"\"\"Initialize the trainer pipeline.\n\n    Args:\n        config (DictConfig): Training configuration from cfg.train (OmegaConf).\n        model (PreTrainedModel | None): Model instance.\n        tokenizer (PreTrainedTokenizerFast | None): Tokenizer instance.\n        train_dataset (Dataset | None): Training dataset.\n        eval_dataset (Dataset | None): Evaluation dataset.\n        data_collator (StandardDataCollator | None): Data collator.\n    \"\"\"\n    self.config = config\n    self.model = model\n    # Prefer explicit arguments, but allow filling from io_dict when provided\n    if io_dict is not None:\n        tokenizer = tokenizer or io_dict.get(\"tokenizer\")\n        train_dataset = train_dataset or io_dict.get(\"train_dataset\")\n        eval_dataset = eval_dataset or io_dict.get(\"test_dataset\")\n        data_collator = data_collator or io_dict.get(\"data_collator\")\n\n    self.tokenizer = tokenizer\n    self.train_dataset = train_dataset\n    self.eval_dataset = eval_dataset\n    self.data_collator = data_collator\n    self.wandb_config = wandb_config\n    self.trainer: Optional[Trainer] = None\n    self._loader = None\n</code></pre>"},{"location":"trainer/#calt.trainer.pipeline.TrainerPipeline.build","title":"build","text":"<pre><code>build() -&gt; TrainerPipeline\n</code></pre> <p>Build the trainer from configuration.</p> <p>Returns:</p> Name Type Description <code>TrainerPipeline</code> <code>TrainerPipeline</code> <p>Returns self for method chaining.</p> Source code in <code>src/calt/trainer/pipeline.py</code> <pre><code>def build(self) -&gt; \"TrainerPipeline\":\n    \"\"\"Build the trainer from configuration.\n\n    Returns:\n        TrainerPipeline: Returns self for method chaining.\n    \"\"\"\n    # Configure wandb before building TrainingArguments / Trainer\n    self._configure_wandb()\n\n    # Import here to avoid circular import\n    from .loader import StandardTrainerLoader\n\n    # Create trainer loader\n    self._loader = StandardTrainerLoader(\n        calt_config=self.config,\n        model=self.model,\n        tokenizer=self.tokenizer,\n        train_dataset=self.train_dataset,\n        eval_dataset=self.eval_dataset,\n        data_collator=self.data_collator,\n    )\n\n    # Load the trainer\n    self.trainer = self._loader.load()\n    return self\n</code></pre>"},{"location":"trainer/#calt.trainer.pipeline.TrainerPipeline.train","title":"train","text":"<pre><code>train(resume_from_checkpoint: str | bool | None = None) -&gt; None\n</code></pre> <p>Train the model.</p> <p>This method calls trainer.train(). If resume_from_checkpoint is provided, training will resume from the specified checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from_checkpoint</code> <code>str | bool | None</code> <p>If True, resume from the latest checkpoint in output_dir.                    If str, resume from the specified checkpoint path.                    If None, start training from scratch.</p> <code>None</code> Source code in <code>src/calt/trainer/pipeline.py</code> <pre><code>def train(self, resume_from_checkpoint: str | bool | None = None) -&gt; None:\n    \"\"\"Train the model.\n\n    This method calls trainer.train(). If resume_from_checkpoint is provided,\n    training will resume from the specified checkpoint.\n\n    Args:\n        resume_from_checkpoint: If True, resume from the latest checkpoint in output_dir.\n                               If str, resume from the specified checkpoint path.\n                               If None, start training from scratch.\n    \"\"\"\n    if self.trainer is None:\n        raise ValueError(\"Trainer not built. Call build() first.\")\n\n    # Train the model\n    self.trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n</code></pre>"},{"location":"trainer/#calt.trainer.pipeline.TrainerPipeline.save_model","title":"save_model","text":"<pre><code>save_model(output_dir: str | None = None) -&gt; None\n</code></pre> <p>Save the model and tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | None</code> <p>Directory to save the model and tokenizer. If None, uses trainer's output_dir.</p> <code>None</code> Source code in <code>src/calt/trainer/pipeline.py</code> <pre><code>def save_model(self, output_dir: str | None = None) -&gt; None:\n    \"\"\"Save the model and tokenizer.\n\n    Args:\n        output_dir: Directory to save the model and tokenizer. If None, uses trainer's output_dir.\n    \"\"\"\n    if self.trainer is None:\n        raise ValueError(\"Trainer not built. Call build() first.\")\n    if output_dir is None:\n        output_dir = self.trainer.args.output_dir\n    self.trainer.save_model(output_dir=output_dir)\n    # Also save tokenizer\n    if self.tokenizer is not None:\n        self.tokenizer.save_pretrained(output_dir)\n</code></pre>"},{"location":"trainer/#calt.trainer.pipeline.TrainerPipeline.evaluate_and_save_generation","title":"evaluate_and_save_generation","text":"<pre><code>evaluate_and_save_generation(max_length: int = 512) -&gt; float\n</code></pre> <p>Evaluate and save generation results.</p> Source code in <code>src/calt/trainer/pipeline.py</code> <pre><code>def evaluate_and_save_generation(self, max_length: int = 512) -&gt; float:\n    \"\"\"Evaluate and save generation results.\"\"\"\n    if self.trainer is None:\n        raise ValueError(\"Trainer not built. Call build() first.\")\n    return self.trainer.evaluate_and_save_generation(max_length=max_length)\n</code></pre>"},{"location":"trainer/#calt.trainer.pipeline.TrainerPipeline.from_io_dict","title":"from_io_dict  <code>classmethod</code>","text":"<pre><code>from_io_dict(\n    config: DictConfig,\n    model: PreTrainedModel,\n    io_dict: dict,\n    wandb_config: Optional[DictConfig] = None,\n) -&gt; TrainerPipeline\n</code></pre> <p>Create a TrainerPipeline from a dict returned by IOPipeline.build().</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig</code> <p>Training configuration (cfg.train). May contain wandb config as config.wandb.</p> required <code>model</code> <code>PreTrainedModel</code> <p>Model instance from ModelPipeline.</p> required <code>io_dict</code> <code>dict</code> <p>IOPipeline.build() result.</p> required <code>wandb_config</code> <code>Optional[DictConfig]</code> <p>Optional wandb configuration block. If None, tries to get from config.wandb.</p> <code>None</code> Source code in <code>src/calt/trainer/pipeline.py</code> <pre><code>@classmethod\ndef from_io_dict(\n    cls,\n    config: DictConfig,\n    model: PreTrainedModel,\n    io_dict: dict,\n    wandb_config: Optional[DictConfig] = None,\n) -&gt; \"TrainerPipeline\":\n    \"\"\"Create a TrainerPipeline from a dict returned by IOPipeline.build().\n\n    Args:\n        config: Training configuration (cfg.train). May contain wandb config as config.wandb.\n        model: Model instance from ModelPipeline.\n        io_dict: IOPipeline.build() result.\n        wandb_config: Optional wandb configuration block. If None, tries to get from config.wandb.\n    \"\"\"\n    # Get wandb_config from config.wandb if not provided\n    if wandb_config is None and hasattr(config, \"wandb\"):\n        wandb_config = config.wandb\n\n    instance = cls(\n        config=config,\n        model=model,\n        tokenizer=io_dict[\"tokenizer\"],\n        train_dataset=io_dict[\"train_dataset\"],\n        eval_dataset=io_dict[\"test_dataset\"],\n        data_collator=io_dict[\"data_collator\"],\n        wandb_config=wandb_config,\n        io_dict=io_dict,\n    )\n    return instance\n</code></pre>"},{"location":"trainer/#calt.trainer.pipeline.TrainerPipeline.resume_from_checkpoint","title":"resume_from_checkpoint  <code>classmethod</code>","text":"<pre><code>resume_from_checkpoint(\n    save_dir: str, resume_from_checkpoint: bool = True\n) -&gt; TrainerPipeline\n</code></pre> <p>Resume training from a saved checkpoint directory.</p> <p>This method loads train.yaml from save_dir, reconstructs IOPipeline, ModelPipeline, and TrainerPipeline, and optionally loads the saved model and tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str</code> <p>Directory containing train.yaml, model/, and tokenizer/.</p> required <code>resume_from_checkpoint</code> <code>bool</code> <p>If True, load saved model and tokenizer. If False, create new model.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>TrainerPipeline</code> <code>TrainerPipeline</code> <p>TrainerPipeline instance ready for training continuation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from calt.trainer import TrainerPipeline\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load from checkpoint and continue training\n&gt;&gt;&gt; trainer_pipeline = TrainerPipeline.resume_from_checkpoint(\"./results\")\n&gt;&gt;&gt; trainer_pipeline.build()\n&gt;&gt;&gt; trainer_pipeline.train()  # Continue training\n</code></pre> Source code in <code>src/calt/trainer/pipeline.py</code> <pre><code>@classmethod\ndef resume_from_checkpoint(\n    cls,\n    save_dir: str,\n    resume_from_checkpoint: bool = True,\n) -&gt; \"TrainerPipeline\":\n    \"\"\"Resume training from a saved checkpoint directory.\n\n    This method loads train.yaml from save_dir, reconstructs IOPipeline, ModelPipeline,\n    and TrainerPipeline, and optionally loads the saved model and tokenizer.\n\n    Args:\n        save_dir: Directory containing train.yaml, model/, and tokenizer/.\n        resume_from_checkpoint: If True, load saved model and tokenizer. If False, create new model.\n\n    Returns:\n        TrainerPipeline: TrainerPipeline instance ready for training continuation.\n\n    Examples:\n        &gt;&gt;&gt; from calt.trainer import TrainerPipeline\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load from checkpoint and continue training\n        &gt;&gt;&gt; trainer_pipeline = TrainerPipeline.resume_from_checkpoint(\"./results\")\n        &gt;&gt;&gt; trainer_pipeline.build()\n        &gt;&gt;&gt; trainer_pipeline.train()  # Continue training\n    \"\"\"\n    from .utils import load_from_checkpoint\n\n    _, _, trainer_pipeline = load_from_checkpoint(save_dir, resume_from_checkpoint)\n    return trainer_pipeline\n</code></pre>"},{"location":"trainer/#pipelines-and-configuration","title":"Pipelines and configuration","text":"<p>High-level example scripts (under <code>calt/examples/*</code>) use class-based pipelines to keep configuration and wiring simple:</p> <ul> <li>:class:<code>calt.io.IOPipeline</code> \u2013 builds datasets, tokenizer, and collator.</li> <li>:class:<code>calt.models.ModelPipeline</code> \u2013 builds the model from configuration.</li> <li>:class:<code>calt.trainer.TrainerPipeline</code> \u2013 builds the HuggingFace <code>Trainer</code>.</li> </ul> <p>A typical training script looks like:</p> <pre><code>from omegaconf import OmegaConf\nfrom calt.io import IOPipeline\nfrom calt.models import ModelPipeline\nfrom calt.trainer import TrainerPipeline\n\ncfg = OmegaConf.load(\"configs/train.yaml\")\n\nio_dict = IOPipeline.from_config(cfg.data).build()\nmodel = ModelPipeline.from_io_dict(cfg.model, io_dict).build()\n# wandb_config is optional - if not provided, TrainerPipeline will try to get it from cfg.train.wandb\ntrainer_pipeline = TrainerPipeline.from_io_dict(cfg.train, model, io_dict).build()\n\ntrainer_pipeline.train()\nsuccess_rate = trainer_pipeline.evaluate_and_save_generation()\nprint(f\"Success rate: {100 * success_rate:.1f}%\")\n</code></pre>"},{"location":"trainer/#resuming-training-from-checkpoint","title":"Resuming training from checkpoint","text":"<p>You can resume training from a saved checkpoint using <code>TrainerPipeline.resume_from_checkpoint</code>:</p> <pre><code>from calt.trainer import TrainerPipeline\n\n# Resume training from a saved directory\ntrainer_pipeline = TrainerPipeline.resume_from_checkpoint(\n    save_dir=\"./results/my_experiment\",\n    resume_from_checkpoint=True  # Load saved model weights\n)\ntrainer_pipeline.build()\ntrainer_pipeline.train()  # Continue training\n</code></pre> <p>For details on the three configuration files used in these examples (<code>data.yaml</code>, <code>lexer.yaml</code>, <code>train.yaml</code>), see Configuration.</p>"}]}