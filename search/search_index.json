{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CALT: Computer ALgebra with Transformer","text":"<p>CALT is a simple Python library for learning arithmetic and symbolic computation with Transformer models. It offers a basic Transformer model and training, and non-experts of deep learning (e.g., mathematicians) can focus on constructing datasets to train and evaluate the model. Particularly, users only need to implement an instance generator for their own task.</p>"},{"location":"#installation","title":"Installation","text":"<p>CALT can be installed via <code>pip</code>.  <pre><code>pip install calt-x\n</code></pre></p> <p>We highly recommend the users to use CALT codebase - a comprehensive template repository to build up your own projects using CALT. The quickstart guide can be found in CALT codebase documentation.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this code in your research, please cite our paper:</p> <pre><code>@misc{kera2025calt,\n  title={CALT: A Library for Computer Algebra with Transformer},\n  author={Hiroshi Kera and Shun Arawaka and Yuta Sato},\n  year={2025},\n  archivePrefix={arXiv},\n  eprint={2506.08600}\n}\n</code></pre> <p>The following is a small list of such studies from our group. </p> <ul> <li>\"Learning to Compute Gr\u00f6bner Bases,\" Kera et al., 2024</li> <li>\"Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms,\" Kera and Pelleriti et al., 2025</li> <li>\"Geometric Generality of Transformer-Based Gr\u00f6bner Basis Computation,\" Kambe et al., 2025</li> </ul> <p>Refer to our paper \"CALT: A Library for Computer Algebra with Transformer,\" Kera et al., 2025 for a comprehensive overview.</p>"},{"location":"data_loader/","title":"Data Loader","text":""},{"location":"data_loader/#data-loader","title":"Data Loader","text":"<p>Utilities to prepare training/evaluation datasets, tokenizers, and data collators. They convert symbolic expressions (polynomials/integers) into internal token sequences and build batches suitable for training.</p>"},{"location":"data_loader/#entry-point","title":"Entry point","text":"<p>Create dataset, tokenizer and data-collator objects.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset_path</code> <code>str</code> <p>Path to the file that stores the \"training\" samples.</p> required <code>test_dataset_path</code> <code>str</code> <p>Path to the file that stores the \"evaluation\" samples.</p> required <code>field</code> <code>str</code> <p>Finite-field identifier (e.g. <code>\"Q\"</code> for the rationals or <code>\"Zp\"</code> for a prime field) used to generate the vocabulary.</p> required <code>num_variables</code> <code>int</code> <p>Maximum number of symbolic variables ((x_1, \\dots, x_n)) that can appear in a polynomial.</p> required <code>max_degree</code> <code>int</code> <p>Maximum total degree allowed for any monomial term.</p> required <code>max_coeff</code> <code>int</code> <p>Maximum absolute value of the coefficients appearing in the data.</p> required <code>max_length</code> <code>int</code> <p>Hard upper bound on the token sequence length. Longer sequences will be right-truncated. Defaults to 512.</p> <code>512</code> <code>processor_name</code> <code>str</code> <p>Name of the processor to use for converting symbolic expressions into internal token IDs. The default processor is <code>\"polynomial\"</code>, which handles polynomial expressions. The alternative processor is <code>\"integer\"</code>, which handles integer expressions. Defaults to <code>\"polynomial\"</code>.</p> <code>'polynomial'</code> <code>vocab_path</code> <code>str | None</code> <p>Path to the vocabulary configuration file. If None, a default vocabulary will be generated based on the field, max_degree, and max_coeff parameters. Defaults to None.</p> <code>None</code> <code>num_train_samples</code> <code>int | None</code> <p>Maximum number of training samples to load. If None or -1, all available training samples will be loaded. Defaults to None.</p> <code>None</code> <code>num_test_samples</code> <code>int | None</code> <p>Maximum number of test samples to load. If None or -1, all available test samples will be loaded. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict[str, StandardDataset], PreTrainedTokenizerFast, StandardDataCollator]</code> <p>tuple[dict[str, StandardDataset], StandardTokenizer, StandardDataCollator]: 1. <code>dataset</code> - a <code>dict</code> with <code>\"train\"</code> and <code>\"test\"</code> splits    containing <code>StandardDataset</code> instances. 2. <code>tokenizer</code> - a <code>PreTrainedTokenizerFast</code> capable of encoding    symbolic expressions into token IDs and vice versa. 3. <code>data_collator</code> - a callable that assembles batches and applies    dynamic padding so they can be fed to a HuggingFace <code>Trainer</code>.</p> Source code in <code>src/calt/data_loader/data_loader.py</code> <pre><code>def load_data(\n    train_dataset_path: str,\n    test_dataset_path: str,\n    field: str,\n    num_variables: int,\n    max_degree: int,\n    max_coeff: int,\n    max_length: int = 512,\n    processor_name: str = \"polynomial\",\n    vocab_path: str | None = None,\n    num_train_samples: int | None = None,\n    num_test_samples: int | None = None,\n) -&gt; tuple[dict[str, StandardDataset], StandardTokenizer, StandardDataCollator]:\n    \"\"\"Create dataset, tokenizer and data-collator objects.\n\n    Args:\n        train_dataset_path (str):\n            Path to the file that stores the \"training\" samples.\n        test_dataset_path (str):\n            Path to the file that stores the \"evaluation\" samples.\n        field (str):\n            Finite-field identifier (e.g. ``\"Q\"`` for the rationals or ``\"Zp\"``\n            for a prime field) used to generate the vocabulary.\n        num_variables (int):\n            Maximum number of symbolic variables (\\(x_1, \\dots, x_n\\)) that can\n            appear in a polynomial.\n        max_degree (int):\n            Maximum total degree allowed for any monomial term.\n        max_coeff (int):\n            Maximum absolute value of the coefficients appearing in the data.\n        max_length (int, optional):\n            Hard upper bound on the token sequence length. Longer sequences will\n            be right-truncated. Defaults to 512.\n        processor_name (str, optional):\n            Name of the processor to use for converting symbolic expressions into\n            internal token IDs. The default processor is ``\"polynomial\"``, which\n            handles polynomial expressions. The alternative processor is\n            ``\"integer\"``, which handles integer expressions. Defaults to\n            ``\"polynomial\"``.\n        vocab_path (str | None, optional):\n            Path to the vocabulary configuration file. If None, a default vocabulary\n            will be generated based on the field, max_degree, and max_coeff parameters.\n            Defaults to None.\n        num_train_samples (int | None, optional):\n            Maximum number of training samples to load. If None or -1, all available\n            training samples will be loaded. Defaults to None.\n        num_test_samples (int | None, optional):\n            Maximum number of test samples to load. If None or -1, all available\n            test samples will be loaded. Defaults to None.\n\n    Returns:\n        tuple[dict[str, StandardDataset], StandardTokenizer, StandardDataCollator]:\n            1. ``dataset`` - a ``dict`` with ``\"train\"`` and ``\"test\"`` splits\n               containing ``StandardDataset`` instances.\n            2. ``tokenizer`` - a ``PreTrainedTokenizerFast`` capable of encoding\n               symbolic expressions into token IDs and vice versa.\n            3. ``data_collator`` - a callable that assembles batches and applies\n               dynamic padding so they can be fed to a HuggingFace ``Trainer``.\n    \"\"\"\n    if processor_name == \"polynomial\":\n        preprocessor = PolynomialToInternalProcessor(\n            num_variables=num_variables,\n            max_degree=max_degree,\n            max_coeff=max_coeff,\n        )\n    elif processor_name == \"integer\":\n        preprocessor = IntegerToInternalProcessor(max_coeff=max_coeff)\n    else:\n        raise ValueError(f\"Unknown processor: {processor_name}\")\n\n    train_input_texts, train_target_texts = _read_data_from_file(\n        train_dataset_path, max_samples=num_train_samples\n    )\n    train_dataset = StandardDataset(\n        input_texts=train_input_texts,\n        target_texts=train_target_texts,\n        preprocessor=preprocessor,\n    )\n\n    test_input_texts, test_target_texts = _read_data_from_file(\n        test_dataset_path, max_samples=num_test_samples\n    )\n    test_dataset = StandardDataset(\n        input_texts=test_input_texts,\n        target_texts=test_target_texts,\n        preprocessor=preprocessor,\n    )\n\n    vocab_config: VocabConfig | None = None\n    if vocab_path:\n        with open(vocab_path, \"r\") as f:\n            vocab_config = yaml.safe_load(f)\n\n    tokenizer = set_tokenizer(\n        field=field,\n        max_degree=max_degree,\n        max_coeff=max_coeff,\n        max_length=max_length,\n        vocab_config=vocab_config,\n    )\n    data_collator = StandardDataCollator(tokenizer)\n    dataset = {\"train\": train_dataset, \"test\": test_dataset}\n    return dataset, tokenizer, data_collator\n</code></pre>"},{"location":"data_loader/#dataset-and-collator","title":"Dataset and collator","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>src/calt/data_loader/utils/data_collator.py</code> <pre><code>def __init__(\n    self,\n    input_texts: list[str],\n    target_texts: list[str],\n    preprocessor: AbstractPreprocessor,\n    **extra_fields,\n) -&gt; None:\n    self.input_texts = input_texts\n    self.target_texts = target_texts\n    self.preprocessor = preprocessor\n    self.extra_fields = extra_fields\n\n    num_samples = len(self.input_texts)\n    if len(self.target_texts) != num_samples:\n        raise ValueError(\n            \"input_texts and target_texts must have the same number of samples.\"\n        )\n\n    for name, data in self.extra_fields.items():\n        if len(data) != num_samples:\n            raise ValueError(\n                f\"Extra field '{name}' has {len(data)} samples, but {num_samples} were expected.\"\n            )\n</code></pre> Source code in <code>src/calt/data_loader/utils/data_collator.py</code> <pre><code>def __init__(self, tokenizer: Tokenizer = None) -&gt; None:\n    self.tokenizer = tokenizer\n</code></pre>"},{"location":"data_loader/#calt.data_loader.utils.data_collator.StandardDataset.load_file","title":"load_file  <code>classmethod</code>","text":"<pre><code>load_file(\n    data_path: str,\n    preprocessor: AbstractPreprocessor,\n    max_samples: int | None = None,\n) -&gt; StandardDataset\n</code></pre> <p>Load data from a file and create a <code>StandardDataset</code> instance.</p> <p>This method maintains backward compatibility with the previous file-based initialization.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the data file.</p> required <code>preprocessor</code> <code>AbstractPreprocessor</code> <p>Preprocessor instance.</p> required <code>max_samples</code> <code>int | None</code> <p>Maximum number of samples to load. Use -1 or None to load all samples. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>StandardDataset</code> <code>StandardDataset</code> <p>Loaded dataset instance.</p> Source code in <code>src/calt/data_loader/utils/data_collator.py</code> <pre><code>@classmethod\ndef load_file(\n    cls,\n    data_path: str,\n    preprocessor: AbstractPreprocessor,\n    max_samples: int | None = None,\n) -&gt; \"StandardDataset\":\n    \"\"\"Load data from a file and create a ``StandardDataset`` instance.\n\n    This method maintains backward compatibility with the previous file-based initialization.\n\n    Args:\n        data_path (str): Path to the data file.\n        preprocessor (AbstractPreprocessor): Preprocessor instance.\n        max_samples (int | None, optional): Maximum number of samples to load.\n            Use -1 or None to load all samples. Defaults to None.\n\n    Returns:\n        StandardDataset: Loaded dataset instance.\n    \"\"\"\n    input_texts, target_texts = _read_data_from_file(data_path, max_samples)\n    return cls(\n        input_texts=input_texts,\n        target_texts=target_texts,\n        preprocessor=preprocessor,\n    )\n</code></pre>"},{"location":"data_loader/#calt.data_loader.utils.data_collator.StandardDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; dict[str, str]\n</code></pre> <p>Get dataset item and convert to internal representation.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the item to retrieve.</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: A mapping with keys <code>\"input\"</code> and <code>\"target\"</code>.</p> Source code in <code>src/calt/data_loader/utils/data_collator.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict[str, str]:\n    \"\"\"Get dataset item and convert to internal representation.\n\n    Args:\n        idx (int): Index of the item to retrieve.\n\n    Returns:\n        dict[str, str]: A mapping with keys ``\"input\"`` and ``\"target\"``.\n    \"\"\"\n    src = self.preprocessor(self.input_texts[idx])\n    tgt = self.preprocessor(self.target_texts[idx])\n    return {\"input\": src, \"target\": tgt}\n</code></pre>"},{"location":"data_loader/#calt.data_loader.utils.data_collator.StandardDataCollator.__call__","title":"__call__","text":"<pre><code>__call__(batch)\n</code></pre> <p>Collate a batch of data samples.</p> <p>If a tokenizer is provided, it tokenizes <code>input</code> and <code>target</code> attributes. Other attributes starting with <code>target_</code> are prefixed with <code>decoder_</code> and padded.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[dict[str, Any]]</code> <p>Mini-batch samples.</p> required <p>Returns:</p> Type Description <p>dict[str, torch.Tensor | list[str]]: Batched tensors and/or lists.</p> Source code in <code>src/calt/data_loader/utils/data_collator.py</code> <pre><code>def __call__(self, batch):\n    \"\"\"Collate a batch of data samples.\n\n    If a tokenizer is provided, it tokenizes ``input`` and ``target`` attributes.\n    Other attributes starting with ``target_`` are prefixed with ``decoder_`` and padded.\n\n    Args:\n        batch (list[dict[str, Any]]): Mini-batch samples.\n\n    Returns:\n        dict[str, torch.Tensor | list[str]]: Batched tensors and/or lists.\n    \"\"\"\n    batch_dict = {}\n\n    # Get the attributes from the first item in the batch.\n    attributes = batch[0].keys()\n\n    if self.tokenizer is None:\n        # If no tokenizer is provided, return the batch as is.\n        for attribute in attributes:\n            attribute_batch = [item[attribute] for item in batch]\n            batch_dict[attribute] = attribute_batch\n\n        return batch_dict\n\n    for attribute in attributes:\n        attribute_batch = [item[attribute] for item in batch]\n\n        if attribute == \"input\":\n            # Tokenize the input sequences.\n            inputs = self.tokenizer(\n                attribute_batch, padding=\"longest\", return_tensors=\"pt\"\n            )\n            batch_dict[\"input_ids\"] = inputs[\"input_ids\"]\n            batch_dict[\"attention_mask\"] = inputs[\"attention_mask\"]\n\n        elif attribute == \"target\":\n            # Tokenize the target sequences.\n            targets = self.tokenizer(\n                attribute_batch, padding=\"longest\", return_tensors=\"pt\"\n            )\n            # Prepare decoder input ids (remove the last token, usually EOS).\n            batch_dict[\"decoder_input_ids\"] = targets[\"input_ids\"][\n                :, :-1\n            ].contiguous()\n            # Prepare decoder attention mask accordingly.\n            batch_dict[\"decoder_attention_mask\"] = targets[\"attention_mask\"][\n                :, :-1\n            ].contiguous()\n\n            # Prepare labels for the loss calculation (shift by one, usually remove BOS).\n            labels = targets[\"input_ids\"][:, 1:].contiguous()\n            label_attention_mask = (\n                targets[\"attention_mask\"][:, 1:].contiguous().bool()\n            )\n            # Set padding tokens in labels to -100 to be ignored by the loss function.\n            labels[~label_attention_mask] = -100\n            batch_dict[\"labels\"] = labels\n\n        else:\n            # For other attributes, if they start with 'target_',\n            # prefix them with 'decoder_' (e.g., 'target_aux' becomes 'decoder_aux').\n            if attribute.startswith(\"target_\"):\n                attribute_key = (\n                    \"decoder_\" + attribute[7:]\n                )  #  Corrected key for batch_dict\n            else:\n                attribute_key = (\n                    attribute  # Use original attribute name if no prefix\n                )\n            # Pad the sequences for these attributes.\n            batch_dict[attribute_key] = self._pad_sequences(\n                attribute_batch, padding_value=0\n            )\n\n    return batch_dict\n</code></pre>"},{"location":"data_loader/#preprocessing-expression-internal-tokens","title":"Preprocessing (expression \u2192 internal tokens)","text":"<p>               Bases: <code>AbstractPreprocessor</code></p> <p>Convert SageMath-style expressions to/from internal token representation.</p> <p>Example (to_internal):     \"2x1^2x0 + 5*x0 - 3\" -&gt; \"C2 E1 E2 C5 E1 E0 C-3 E0 E0\" (for <code>num_vars=2</code>)</p> <p>Example (to_original):     \"C2 E2 E1 C5 E1 E0 C-3 E0 E0\" -&gt; \"2x0^2x1 + 5*x0 - 3\"</p> The internal representation uses <ul> <li><code>C{n}</code> tokens for coefficients (e.g., <code>C2</code>, <code>C-3</code>)</li> <li><code>E{n}</code> tokens for exponents (e.g., <code>E1</code>, <code>E2</code>, <code>E0</code>)</li> </ul> <p>Each term is represented as a coefficient token followed by exponent tokens for each variable.</p> Source code in <code>src/calt/data_loader/utils/preprocessor.py</code> <pre><code>def __init__(self, num_variables: int, max_degree: int, max_coeff: int):\n    \"\"\"Initialize preprocessor parameters.\n\n    Args:\n        num_variables (int): Number of variables in the polynomial (e.g., x0, x1, ...).\n        max_degree (int): Maximum degree of the polynomial.\n        max_coeff (int): Maximum coefficient value in the polynomial.\n    \"\"\"\n    if num_variables &lt; 0:\n        raise ValueError(\"num_variables must be positive\")\n    if max_degree &lt; 0:\n        raise ValueError(\"max_degree must be non-negative\")\n    if max_coeff &lt;= 0:\n        raise ValueError(\"max_coeff must be positive\")\n\n    self.num_variables = num_variables\n    self.max_degree = max_degree\n    self.max_coeff = max_coeff\n    self.var_name_to_index = {f\"x{i}\": i for i in range(num_variables)}\n</code></pre> <p>               Bases: <code>AbstractPreprocessor</code></p> <p>Convert an integer string to/from internal token representation.</p> <p>Input format examples (to_internal):     - \"12345\"     - \"123|45|678\" Output format examples (from_internal):     - \"C1 C2 C3 C4 C5\"     - \"C1 C2 C3 [SEP] C4 C5 [SEP] C6 C7 C8\"</p> <p>The internal representation uses <code>C{n}</code> tokens for digits. Parts separated by '|' are converted individually and joined by <code>[SEP]</code>. Note: <code>num_variables</code>, <code>max_degree</code>, <code>max_coeff</code> are inherited but not directly used.</p> <p>Parameters:</p> Name Type Description Default <code>max_coeff</code> <code>int</code> <p>The maximum digit value (typically 9). Passed to superclass but primarily used for validation context.</p> <code>9</code> Source code in <code>src/calt/data_loader/utils/preprocessor.py</code> <pre><code>def __init__(self, max_coeff: int = 9):\n    \"\"\"Initialize the processor.\n\n    Args:\n        max_coeff (int): The maximum digit value (typically 9). Passed to superclass but primarily used for validation context.\n    \"\"\"\n    # Use dummy values for num_variables and max_degree as they are not relevant\n    super().__init__(num_variables=0, max_degree=0, max_coeff=max_coeff)\n</code></pre>"},{"location":"data_loader/#calt.data_loader.utils.preprocessor.PolynomialToInternalProcessor.encode","title":"encode","text":"<pre><code>encode(text: str) -&gt; str\n</code></pre> <p>Process a symbolic text into internal token representation.</p> <p>If the text contains the '|' separator character, each part is processed separately and joined with '[SEP]' token.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input symbolic text to process.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String in the internal token representation.</p> Source code in <code>src/calt/data_loader/utils/preprocessor.py</code> <pre><code>def encode(self, text: str) -&gt; str:\n    \"\"\"Process a symbolic text into internal token representation.\n\n    If the text contains the '|' separator character, each part is processed\n    separately and joined with '[SEP]' token.\n\n    Args:\n        text (str): Input symbolic text to process.\n\n    Returns:\n        str: String in the internal token representation.\n    \"\"\"\n    # If text contains '|', process each part separately and join with [SEP]\n    if \"|\" in text:\n        parts = [p.strip() for p in text.split(\"|\")]\n        internals = [self._poly_to_encode(p) for p in parts]\n        processed_string = \" [SEP] \".join(internals)\n    else:\n        processed_string = self._poly_to_encode(text)\n\n    return processed_string\n</code></pre>"},{"location":"data_loader/#calt.data_loader.utils.preprocessor.PolynomialToInternalProcessor.decode","title":"decode","text":"<pre><code>decode(tokens: str) -&gt; str\n</code></pre> <p>Convert an internal token string back to a symbolic polynomial expression.</p> Source code in <code>src/calt/data_loader/utils/preprocessor.py</code> <pre><code>def decode(self, tokens: str) -&gt; str:\n    \"\"\"Convert an internal token string back to a symbolic polynomial expression.\"\"\"\n    if \"[SEP]\" in tokens:\n        parts = tokens.split(\"[SEP]\")\n        original_parts = [self._internal_to_poly(p.strip()) for p in parts]\n        return \" | \".join(original_parts)\n    else:\n        return self._internal_to_poly(tokens)\n</code></pre>"},{"location":"data_loader/#calt.data_loader.utils.preprocessor.IntegerToInternalProcessor.encode","title":"encode","text":"<pre><code>encode(text: str) -&gt; str\n</code></pre> <p>Process an integer string into internal token representation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input string representing one or more integers separated by '|'.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Internal token representation (e.g., \"C1 C2 [SEP] C3 C4\"), or \"[ERROR_FORMAT]\" if any part is not a valid integer string.</p> Source code in <code>src/calt/data_loader/utils/preprocessor.py</code> <pre><code>def encode(self, text: str) -&gt; str:\n    \"\"\"Process an integer string into internal token representation.\n\n    Args:\n        text (str): Input string representing one or more integers separated by '|'.\n\n    Returns:\n        str: Internal token representation (e.g., \"C1 C2 [SEP] C3 C4\"), or \"[ERROR_FORMAT]\" if any part is not a valid integer string.\n    \"\"\"\n    if \"|\" in text:\n        parts = [p.strip() for p in text.split(\"|\")]\n        tokenized_parts = []\n        for part in parts:\n            tokens = self._number_to_tokens(part)\n            if tokens == \"[ERROR_FORMAT]\":\n                # If any part fails, return error for the whole input\n                return \"[ERROR_FORMAT]\"\n            tokenized_parts.append(tokens)\n        # Join the tokenized parts with [SEP]\n        return \" [SEP] \".join(tokenized_parts)\n    else:\n        # If no separator, process the whole string\n        return self._number_to_tokens(text.strip())\n</code></pre>"},{"location":"data_loader/#calt.data_loader.utils.preprocessor.IntegerToInternalProcessor.decode","title":"decode","text":"<pre><code>decode(tokens: str) -&gt; str\n</code></pre> <p>Convert an internal token representation back to an integer string.</p> Source code in <code>src/calt/data_loader/utils/preprocessor.py</code> <pre><code>def decode(self, tokens: str) -&gt; str:\n    \"\"\"Convert an internal token representation back to an integer string.\"\"\"\n    if \"[SEP]\" in tokens:\n        parts = tokens.split(\"[SEP]\")\n        # Process each part and join with '|'\n        number_parts = [self._tokens_to_number(p.strip()) for p in parts]\n        return \"|\".join(number_parts)\n    else:\n        # Process the whole string if no separator\n        return self._tokens_to_number(tokens.strip())\n</code></pre>"},{"location":"data_loader/#tokenizer","title":"Tokenizer","text":"<p>Build or load a tokenizer for polynomial expressions and configure the vocabulary. </p> <p>Create or load a tokenizer for polynomial expressions.</p> <p>If a <code>vocab_config</code> is provided, it builds a tokenizer from the config. Otherwise, it creates a new tokenizer based on the provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field specification (\"QQ\"/\"ZZ\" for rational/integer, or \"GF<p>\" for finite field). Used if <code>vocab_config</code> is not provided.</p> <code>'GF'</code> <code>max_coeff</code> <code>int</code> <p>Maximum absolute value for coefficients. Used if <code>vocab_config</code> is not provided.</p> <code>100</code> <code>max_degree</code> <code>int</code> <p>Maximum degree for any variable. Used if <code>vocab_config</code> is not provided.</p> <code>10</code> <code>max_length</code> <code>int</code> <p>Maximum sequence length the tokenizer will process.</p> <code>512</code> <code>vocab_config</code> <code>Optional[VocabConfig]</code> <p>Optional dictionary with \"vocab\" and \"special_vocab\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>PreTrainedTokenizerFast</code> <code>PreTrainedTokenizerFast</code> <p>A pre-configured HuggingFace tokenizer for polynomial expressions.</p> Source code in <code>src/calt/data_loader/utils/tokenizer.py</code> <pre><code>def set_tokenizer(\n    field: str = \"GF\",\n    max_coeff: int = 100,\n    max_degree: int = 10,\n    max_length: int = 512,\n    vocab_config: Optional[VocabConfig] = None,\n) -&gt; PreTrainedTokenizerFast:\n    \"\"\"Create or load a tokenizer for polynomial expressions.\n\n    If a ``vocab_config`` is provided, it builds a tokenizer from the config.\n    Otherwise, it creates a new tokenizer based on the provided parameters.\n\n    Args:\n        field (str): Field specification (\"QQ\"/\"ZZ\" for rational/integer, or \"GF&lt;p&gt;\"\n            for finite field). Used if ``vocab_config`` is not provided.\n        max_coeff (int): Maximum absolute value for coefficients. Used if\n            ``vocab_config`` is not provided.\n        max_degree (int): Maximum degree for any variable. Used if ``vocab_config`` is\n            not provided.\n        max_length (int): Maximum sequence length the tokenizer will process.\n        vocab_config (Optional[VocabConfig]): Optional dictionary with \"vocab\" and \"special_vocab\".\n\n    Returns:\n        PreTrainedTokenizerFast: A pre-configured HuggingFace tokenizer for polynomial expressions.\n    \"\"\"\n    if vocab_config:\n        vocab_list = vocab_config[\"vocab\"]\n        special_token_map = vocab_config[\"special_vocab\"]\n        special_tokens = list(special_token_map.values())\n\n    else:\n        # Create tokenizer from scratch\n        special_tokens = [\"[PAD]\", \"&lt;s&gt;\", \"&lt;/s&gt;\", \"[CLS]\"]\n        special_token_map = dict(\n            zip(\n                [\"pad_token\", \"bos_token\", \"eos_token\", \"cls_token\"],\n                special_tokens,\n            )\n        )\n\n        CONSTS = [\"[C]\"]\n        if field in \"ZZ\":\n            CONSTS += [f\"C{i}\" for i in range(-max_coeff, max_coeff + 1)]\n        elif field.startswith(\"GF\"):\n            try:\n                p = int(field[2:])\n                if p &lt;= 0:\n                    raise ValueError()\n            except (ValueError, IndexError):\n                msg = f\"Invalid field specification for GF(p): {field}\"\n                raise ValueError(msg)\n            CONSTS += [f\"C{i}\" for i in range(-p + 1, p)]\n        else:\n            raise ValueError(f\"unknown field: {field}\")\n\n        ECONSTS = [f\"E{i}\" for i in range(max_degree + 1)]\n        vocab_list = CONSTS + ECONSTS + [\"[SEP]\"]\n\n    vocab = dict(zip(vocab_list, range(len(vocab_list))))\n\n    tok = Tokenizer(WordLevel(vocab))\n    tok.pre_tokenizer = CharDelimiterSplit(\" \")\n    tok.add_special_tokens(special_tokens)\n    tok.enable_padding()\n    tok.no_truncation()\n\n    bos_token = special_token_map[\"bos_token\"]\n    eos_token = special_token_map[\"eos_token\"]\n    tok.post_processor = TemplateProcessing(\n        single=f\"{bos_token} $A {eos_token}\",\n        special_tokens=[\n            (bos_token, tok.token_to_id(bos_token)),\n            (eos_token, tok.token_to_id(eos_token)),\n        ],\n    )\n\n    tokenizer = PreTrainedTokenizerFast(\n        tokenizer_object=tok,\n        model_max_length=max_length,\n        **special_token_map,\n    )\n    return tokenizer\n</code></pre> <p>               Bases: <code>TypedDict</code></p>"},{"location":"data_loader/#visualization-utilities-optional","title":"Visualization utilities (optional)","text":"<p>Quickly render visual diffs between predictions and references. </p> <p>Render \"gold\" vs. \"pred\" with strikethrough on mistakes in \"pred\".</p> <p>Parameters:</p> Name Type Description Default <code>gold</code> <code>Expr | str</code> <p>Ground-truth expression. If a string, it will be parsed as a token sequence (e.g., \"C1 E1 E1 C-3 E0 E7\") via <code>parse_poly</code>.</p> required <code>pred</code> <code>Expr | str</code> <p>Model-predicted expression. If a string, it will be parsed as a token sequence via <code>parse_poly</code>.</p> required <code>var_order</code> <code>Sequence[Symbol] | None</code> <p>Variable ordering (important for &gt;2 variables). Inferred if None. Also passed to <code>parse_poly</code> if inputs are strings. Defaults to None.</p> <code>None</code> Source code in <code>src/calt/data_loader/utils/comparison_vis.py</code> <pre><code>def display_with_diff(\n    gold: Expr | str,\n    pred: Expr | str,\n    var_order: Sequence[Symbol] | None = None,\n) -&gt; None:\n    \"\"\"Render \"gold\" vs. \"pred\" with strikethrough on mistakes in \"pred\".\n\n    Args:\n        gold (sympy.Expr | str):\n            Ground-truth expression. If a string, it will be parsed as a token\n            sequence (e.g., \"C1 E1 E1 C-3 E0 E7\") via ``parse_poly``.\n        pred (sympy.Expr | str):\n            Model-predicted expression. If a string, it will be parsed as a token\n            sequence via ``parse_poly``.\n        var_order (Sequence[sympy.Symbol] | None, optional):\n            Variable ordering (important for &gt;2 variables). Inferred if None. Also\n            passed to ``parse_poly`` if inputs are strings. Defaults to None.\n    \"\"\"\n\n    # --- input conversion ------------------------------------------------- #\n    if isinstance(gold, str):\n        gold = parse_poly(gold, var_names=var_order)\n    if isinstance(pred, str):\n        pred = parse_poly(pred, var_names=var_order)\n\n    # --- normalize -------------------------------------------------------- #\n    if var_order is None:\n        var_order = sorted(\n            gold.free_symbols.union(pred.free_symbols), key=lambda s: s.name\n        )\n    gold_poly = Poly(gold.expand(), *var_order)\n    pred_poly = Poly(pred.expand(), *var_order)\n\n    gdict = _poly_to_dict(gold_poly)\n    pdict = _poly_to_dict(pred_poly)\n\n    # --- diff detection --------------------------------------------------- #\n    diff: dict[tuple[int, ...], str] = {}\n    for exps in set(gdict) | set(pdict):\n        gcoeff = gdict.get(exps, 0)\n        pcoeff = pdict.get(exps, 0)\n        if pcoeff == 0 and gcoeff != 0:\n            continue  # missing term (not highlighted)\n        if gcoeff == 0 and pcoeff != 0:\n            diff[exps] = \"extra\"\n        elif gcoeff != pcoeff:\n            diff[exps] = \"coeff_wrong\"\n\n    # --- render ----------------------------------------------------------- #\n    gold_tex = latex(gold.expand())\n    pred_tex = _build_poly_latex(pdict, var_order, diff)\n\n    display(\n        Math(\n            r\"\"\"\\begin{aligned}\n        \\text{Ground truth\\,:}\\; &amp; {}\"\"\"\n            + gold_tex\n            + r\"\"\"\\\\\n        \\text{Prediction\\,:}\\;   &amp; {}\"\"\"\n            + pred_tex\n            + r\"\"\"\n        \\end{aligned}\"\"\"\n        )\n    )\n</code></pre> <p>Load evaluation results from a JSON file.</p> <p>The JSON file should contain a list of objects with \"generated\" and \"reference\" keys.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the JSON file.</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>tuple[list[str], list[str]]: A tuple containing two lists: - List of generated texts. - List of reference texts.</p> Source code in <code>src/calt/data_loader/utils/comparison_vis.py</code> <pre><code>def load_eval_results(file_path: str) -&gt; tuple[list[str], list[str]]:\n    \"\"\"Load evaluation results from a JSON file.\n\n    The JSON file should contain a list of objects with \"generated\" and \"reference\" keys.\n\n    Args:\n        file_path (str): Path to the JSON file.\n\n    Returns:\n        tuple[list[str], list[str]]: A tuple containing two lists:\n            - List of generated texts.\n            - List of reference texts.\n    \"\"\"\n    generated_texts = []\n    reference_texts = []\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    for item in data:\n        generated_texts.append(item.get(\"generated\", \"\"))\n        reference_texts.append(item.get(\"reference\", \"\"))\n\n    return generated_texts, reference_texts\n</code></pre> <p>Convert a math expression string or token sequence to a SymPy polynomial.</p> <p>This function handles: 1. Standard mathematical notation (e.g., \"4x0 + 4x1\"). 2. SageMath-style power notation (e.g., \"3x0^2 + 3x0\"). 3. Internal token format (e.g., \"C4 E1 E0 C4 E0 E1\").</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The mathematical expression or token sequence to parse.</p> required <code>var_names</code> <code>Sequence[str | Symbol] | None</code> <p>Variable names. Primarily used for the token sequence format to ensure the correct number of variables. For expression strings, variables are inferred, but providing them can ensure they are treated as symbols.</p> <code>None</code> <p>Returns:</p> Type Description <code>Expr</code> <p>sympy.Expr: A SymPy expression for the polynomial.</p> Source code in <code>src/calt/data_loader/utils/comparison_vis.py</code> <pre><code>def parse_poly(text: str, var_names: Sequence[str | Symbol] | None = None) -&gt; Expr:\n    \"\"\"Convert a math expression string or token sequence to a SymPy polynomial.\n\n    This function handles:\n    1. Standard mathematical notation (e.g., \"4*x0 + 4*x1\").\n    2. SageMath-style power notation (e.g., \"3*x0^2 + 3*x0\").\n    3. Internal token format (e.g., \"C4 E1 E0 C4 E0 E1\").\n\n    Args:\n        text (str):\n            The mathematical expression or token sequence to parse.\n        var_names (Sequence[str | sympy.Symbol] | None, optional):\n            Variable names. Primarily used for the token sequence format to ensure\n            the correct number of variables. For expression strings, variables are\n            inferred, but providing them can ensure they are treated as symbols.\n\n    Returns:\n        sympy.Expr: A SymPy expression for the polynomial.\n    \"\"\"\n    text = text.strip()\n\n    # Heuristic: if the text starts with a 'C' token, attempt to parse it\n    # using the token-based parser first.\n    if text.startswith(\"C\"):\n        try:\n            return _parse_poly_from_tokens(text, var_names)\n        except (ValueError, IndexError):\n            # Fallback to standard expression parsing if token parsing fails.\n            # This allows parsing expressions that happen to start with 'C'\n            # (e.g., if 'C' is a variable name).\n            pass\n\n    # Standard expression parsing\n    # Replace SageMath-style power operator '^' with SymPy's '**'\n    text_sympy = text.replace(\"^\", \"**\")\n\n    # Prepare a local dictionary of symbols if var_names are provided\n    local_dict = {}\n    if var_names:\n        if all(isinstance(v, Symbol) for v in var_names):\n            symbols_map = {s.name: s for s in var_names}\n        else:\n            symbols_map = {str(name): Symbol(str(name)) for name in var_names}\n        local_dict.update(symbols_map)\n\n    return parse_expr(text_sympy, local_dict=local_dict, evaluate=True)\n</code></pre>"},{"location":"dataset_generator/","title":"Dataset Generator","text":"<p>A unified interface with SageMath and SymPy backends for large-scale dataset generation. It produces paired problems and solutions, supports batch writing, and computes incremental statistics.</p>"},{"location":"dataset_generator/#common-sagemath-backend-example","title":"Common (SageMath backend example)","text":""},{"location":"dataset_generator/#generation-flow","title":"Generation flow","text":"<p>Base class for problem generators</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Backend for parallel processing</p> <code>'multiprocessing'</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs (-1 for all cores)</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to display progress information</p> <code>True</code> <code>root_seed</code> <code>int</code> <p>Root seed for reproducibility</p> <code>42</code> Source code in <code>src/calt/dataset_generator/sagemath/dataset_generator.py</code> <pre><code>def __init__(\n    self,\n    backend: str = \"multiprocessing\",\n    n_jobs: int = -1,\n    verbose: bool = True,\n    root_seed: int = 42,\n):\n    \"\"\"\n    Initialize problem generator.\n\n    Args:\n        backend: Backend for parallel processing\n        n_jobs: Number of parallel jobs (-1 for all cores)\n        verbose: Whether to display progress information\n        root_seed: Root seed for reproducibility\n    \"\"\"\n\n    self.backend = backend\n    self.n_jobs = n_jobs\n    self.verbose = verbose\n    self.root_seed = root_seed\n\n    # Configure logging only once at initialization\n    self.logger = logger\n\n    # Configure joblib logging to show progress but not overwhelm\n    # Only set if not already configured\n    joblib_logger = logging.getLogger(\"joblib\")\n    if joblib_logger.level == logging.NOTSET:\n        joblib_logger.setLevel(logging.INFO)\n\n    parallel_logger = logging.getLogger(\"joblib.Parallel\")\n    if parallel_logger.level == logging.NOTSET:\n        parallel_logger.setLevel(logging.INFO)\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.dataset_generator.DatasetGenerator.run","title":"run","text":"<pre><code>run(\n    dataset_sizes: dict[str, int],\n    problem_generator: Callable,\n    statistics_calculator: Callable | None = None,\n    dataset_writer: DatasetWriter | None = None,\n    batch_size: int = 100000,\n    save_dir: str | None = None,\n    save_text: bool = True,\n    save_json: bool = True,\n)\n</code></pre> <p>Generate multiple datasets using parallel processing with batch writing.</p> <p>This is the main entry point for dataset generation. It supports generating multiple datasets (train/test) simultaneously or separately, with efficient memory management through batch processing and parallel execution.</p> <p>Key features: - Parallel processing using joblib for high performance - Batch-based memory management to handle large datasets - Incremental statistics calculation to avoid memory issues - Reproducible generation with unique seeds for each sample - Support for nested data structures (up to 2 levels) - Multiple output formats (pickle, text, JSON) via DatasetWriter</p> <p>Parameters:</p> Name Type Description Default <code>dataset_sizes</code> <code>dict[str, int]</code> <p>Dictionary mapping dataset names to number of samples.           Any string can be used as dataset name (e.g., \"train\", \"test\", \"validation\").           Duplicate names are not allowed.           Example: {\"train\": 100000, \"test\": 1000} or {\"train\": 100000, \"validation\": 5000}</p> required <code>problem_generator</code> <code>Callable</code> <p>Function that generates (problem, solution) pair given a seed.              Must accept a single integer seed parameter.</p> required <code>statistics_calculator</code> <code>Callable | None</code> <p>Optional function to calculate sample-specific statistics.                  Must accept (problem, solution) and return dict or None.</p> <code>None</code> <code>dataset_writer</code> <code>DatasetWriter | None</code> <p>DatasetWriter object for saving datasets to files.           If None, a new DatasetWriter will be created using save_dir, save_text, and save_json parameters.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of samples to process in each batch. Larger batches        use more memory but may be more efficient for I/O operations.</p> <code>100000</code> <code>save_dir</code> <code>str | None</code> <p>Base directory for saving datasets. Used only if dataset_writer is None.      If None, uses current working directory.</p> <code>None</code> <code>save_text</code> <code>bool</code> <p>Whether to save raw text files. Used only if dataset_writer is None.       Text files use \"#\" as separator between problem and solution.</p> <code>True</code> <code>save_json</code> <code>bool</code> <p>Whether to save JSON Lines files. Used only if dataset_writer is None.       JSON Lines files preserve the original nested structure format.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset_sizes is invalid or problem_generator is None</p> <code>Exception</code> <p>If parallel processing fails</p> Note <ul> <li>Each sample gets a unique seed for reproducibility</li> <li>Progress is logged if verbose=True (set in init)</li> <li>Memory usage scales with batch_size, not total dataset size</li> <li>Statistics are calculated incrementally to handle large datasets</li> <li>If dataset_writer is provided, save_dir, save_text, and save_json parameters are ignored</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Define problem generator function\n&gt;&gt;&gt; def polynomial_generator(seed):\n...     import random\n...     random.seed(seed)\n...     # Generate random polynomial problem\n...     problem = [random.randint(1, 1000) for _ in range(random.randint(1, 10))]\n...     solution = sum(problem)\n...     return problem, solution\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize dataset generator\n&gt;&gt;&gt; generator = DatasetGenerator(n_jobs=-1, verbose=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 1: Automatic DatasetWriter creation\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000, \"test\": 1000, \"validation\": 500},\n...     problem_generator=polynomial_generator,\n...     save_dir=\"./datasets\",\n...     save_text=True,\n...     save_json=True,\n...     batch_size=100\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 2: Manual DatasetWriter creation (for advanced use cases)\n&gt;&gt;&gt; from calt.dataset_generator.sagemath import DatasetWriter\n&gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000, \"test\": 1000},\n...     problem_generator=polynomial_generator,\n...     dataset_writer=writer,\n...     batch_size=100\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 3: Generate datasets separately (if needed)\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000},\n...     problem_generator=polynomial_generator,\n...     save_dir=\"./datasets\",\n...     batch_size=100\n... )\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"test\": 1000, \"validation\": 500},\n...     problem_generator=polynomial_generator,\n...     save_dir=\"./datasets\",\n...     batch_size=100\n... )\n</code></pre> Source code in <code>src/calt/dataset_generator/sagemath/dataset_generator.py</code> <pre><code>def run(\n    self,\n    dataset_sizes: dict[str, int],\n    problem_generator: Callable,\n    statistics_calculator: Callable | None = None,\n    dataset_writer: DatasetWriter | None = None,\n    batch_size: int = 100000,\n    save_dir: str | None = None,\n    save_text: bool = True,\n    save_json: bool = True,\n):\n    \"\"\"\n    Generate multiple datasets using parallel processing with batch writing.\n\n    This is the main entry point for dataset generation. It supports generating\n    multiple datasets (train/test) simultaneously or separately, with efficient\n    memory management through batch processing and parallel execution.\n\n    Key features:\n    - Parallel processing using joblib for high performance\n    - Batch-based memory management to handle large datasets\n    - Incremental statistics calculation to avoid memory issues\n    - Reproducible generation with unique seeds for each sample\n    - Support for nested data structures (up to 2 levels)\n    - Multiple output formats (pickle, text, JSON) via DatasetWriter\n\n    Args:\n        dataset_sizes: Dictionary mapping dataset names to number of samples.\n                      Any string can be used as dataset name (e.g., \"train\", \"test\", \"validation\").\n                      Duplicate names are not allowed.\n                      Example: {\"train\": 100000, \"test\": 1000} or {\"train\": 100000, \"validation\": 5000}\n        problem_generator: Function that generates (problem, solution) pair given a seed.\n                         Must accept a single integer seed parameter.\n        statistics_calculator: Optional function to calculate sample-specific statistics.\n                             Must accept (problem, solution) and return dict or None.\n        dataset_writer: DatasetWriter object for saving datasets to files.\n                      If None, a new DatasetWriter will be created using save_dir, save_text, and save_json parameters.\n        batch_size: Number of samples to process in each batch. Larger batches\n                   use more memory but may be more efficient for I/O operations.\n        save_dir: Base directory for saving datasets. Used only if dataset_writer is None.\n                 If None, uses current working directory.\n        save_text: Whether to save raw text files. Used only if dataset_writer is None.\n                  Text files use \"#\" as separator between problem and solution.\n        save_json: Whether to save JSON Lines files. Used only if dataset_writer is None.\n                  JSON Lines files preserve the original nested structure format.\n\n    Raises:\n        ValueError: If dataset_sizes is invalid or problem_generator is None\n        Exception: If parallel processing fails\n\n    Note:\n        - Each sample gets a unique seed for reproducibility\n        - Progress is logged if verbose=True (set in __init__)\n        - Memory usage scales with batch_size, not total dataset size\n        - Statistics are calculated incrementally to handle large datasets\n        - If dataset_writer is provided, save_dir, save_text, and save_json parameters are ignored\n\n    Examples:\n        &gt;&gt;&gt; # Define problem generator function\n        &gt;&gt;&gt; def polynomial_generator(seed):\n        ...     import random\n        ...     random.seed(seed)\n        ...     # Generate random polynomial problem\n        ...     problem = [random.randint(1, 1000) for _ in range(random.randint(1, 10))]\n        ...     solution = sum(problem)\n        ...     return problem, solution\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Initialize dataset generator\n        &gt;&gt;&gt; generator = DatasetGenerator(n_jobs=-1, verbose=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 1: Automatic DatasetWriter creation\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000, \"test\": 1000, \"validation\": 500},\n        ...     problem_generator=polynomial_generator,\n        ...     save_dir=\"./datasets\",\n        ...     save_text=True,\n        ...     save_json=True,\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 2: Manual DatasetWriter creation (for advanced use cases)\n        &gt;&gt;&gt; from calt.dataset_generator.sagemath import DatasetWriter\n        &gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000, \"test\": 1000},\n        ...     problem_generator=polynomial_generator,\n        ...     dataset_writer=writer,\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 3: Generate datasets separately (if needed)\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000},\n        ...     problem_generator=polynomial_generator,\n        ...     save_dir=\"./datasets\",\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"test\": 1000, \"validation\": 500},\n        ...     problem_generator=polynomial_generator,\n        ...     save_dir=\"./datasets\",\n        ...     batch_size=100\n        ... )\n    \"\"\"\n    # Create DatasetWriter if not provided\n    if dataset_writer is None:\n        dataset_writer = DatasetWriter(\n            save_dir=save_dir,\n            save_text=save_text,\n            save_json=save_json,\n        )\n        self.logger.info(f\"save_dir: {dataset_writer.save_dir}\")\n        self.logger.info(f\"Text output: {save_text}\")\n        self.logger.info(f\"JSON output: {save_json}\")\n\n    # Prepare common arguments\n    common_args = {\n        \"problem_generator\": problem_generator,\n        \"statistics_calculator\": statistics_calculator,\n        \"dataset_writer\": dataset_writer,\n        \"batch_size\": batch_size,\n    }\n\n    # Validate dataset_sizes\n    if not isinstance(dataset_sizes, dict):\n        raise ValueError(\"dataset_sizes must be a dictionary\")\n\n    if not dataset_sizes:\n        raise ValueError(\"dataset_sizes cannot be empty\")\n\n    if problem_generator is None:\n        raise ValueError(\"problem_generator must be provided\")\n\n    # Check for duplicate dataset names\n    if len(dataset_sizes) != len(set(dataset_sizes.keys())):\n        raise ValueError(\"Duplicate dataset names are not allowed\")\n\n    for dataset_name, num_samples in dataset_sizes.items():\n        if not isinstance(num_samples, int) or num_samples &lt;= 0:\n            raise ValueError(\n                f\"Number of samples must be a positive integer, got {num_samples} for {dataset_name}\"\n            )\n\n    # Log overall generation start\n    self.logger.info(\n        \"=========================== Dataset generation ===========================\\n\"\n    )\n    self.logger.info(\n        f\"Starting dataset generation for {len(dataset_sizes)} dataset(s)\"\n    )\n    self.logger.info(f\"Dataset sizes: {dataset_sizes}\\n\")\n\n    # Generate each dataset\n    for dataset_name, num_samples in dataset_sizes.items():\n        self._generate_dataset(\n            tag=dataset_name, num_samples=num_samples, **common_args\n        )\n\n    self.logger.info(\"All datasets generated successfully!\")\n    self.logger.info(\n        \"==========================================================================\\n\"\n    )\n</code></pre>"},{"location":"dataset_generator/#writing-and-statistics","title":"Writing and statistics","text":"<p>Dataset writer for saving problem-solution pairs in multiple formats.</p> <p>This class handles saving datasets with nested structure support up to 2 levels. It can save data in pickle (binary), raw text, and JSON Lines formats.</p> <p>Attributes:</p> Name Type Description <code>INNER_SEP</code> <code>str</code> <p>Separator for single-level lists (\" | \")</p> <code>OUTER_SEP</code> <code>str</code> <p>Separator for nested lists (\" || \")</p> <code>save_dir</code> <code>Path</code> <p>Base directory for saving datasets</p> <code>save_text</code> <code>bool</code> <p>Whether to save raw text files</p> <code>save_json</code> <code>bool</code> <p>Whether to save JSON Lines files</p> <code>_file_handles</code> <code>dict</code> <p>Dictionary to store open file handles</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str | None</code> <p>Base directory for saving datasets. If None, uses current working directory.</p> <code>None</code> <code>save_text</code> <code>bool</code> <p>Whether to save raw text files. Text files use \"#\" as separator       between problem and solution, with nested structures joined by separators.</p> <code>True</code> <code>save_json</code> <code>bool</code> <p>Whether to save JSON Lines files. JSON Lines files preserve the original       nested structure format, with one sample per line.</p> <code>True</code> Note <p>Pickle files are always saved as they are the primary format for data loading. Text and JSON Lines files are optional and controlled by save_text and save_json flags.</p> Usage Source code in <code>src/calt/dataset_generator/sagemath/utils/dataset_writer.py</code> <pre><code>def __init__(\n    self,\n    save_dir: str | None = None,\n    save_text: bool = True,\n    save_json: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize dataset writer.\n\n    Args:\n        save_dir: Base directory for saving datasets. If None, uses current working directory.\n        save_text: Whether to save raw text files. Text files use \"#\" as separator\n                  between problem and solution, with nested structures joined by separators.\n        save_json: Whether to save JSON Lines files. JSON Lines files preserve the original\n                  nested structure format, with one sample per line.\n\n    Note:\n        Pickle files are always saved as they are the primary format for data loading.\n        Text and JSON Lines files are optional and controlled by save_text and save_json flags.\n\n    Usage:\n        # Efficient batch processing with file handle management\n        writer = DatasetWriter(save_dir=\"./datasets\")\n        writer.open(\"train\")  # Open file handles once\n        try:\n            for batch_idx, samples in enumerate(batches):\n                writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx)\n        finally:\n            writer.close(\"train\")  # Close file handles\n\n        # Or use context manager\n        with DatasetWriter(save_dir=\"./datasets\") as writer:\n            writer.open(\"train\")\n            for batch_idx, samples in enumerate(batches):\n                writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx)\n            writer.close(\"train\")\n\n        # Support for various dataset splits\n        writer.open(\"validation\")  # Validation set\n        writer.open(\"dev\")         # Development set\n        writer.open(\"eval\")        # Evaluation set\n    \"\"\"\n    self.save_dir = Path(save_dir) if save_dir else Path.cwd()\n    self.save_text = save_text\n    self.save_json = save_json\n    self.logger = logging.getLogger(__name__)\n    self._file_handles: dict[\n        str, dict[str, any]\n    ] = {}  # {tag: {file_type: file_handle}}\n    TimedeltaDumper.add_representer(timedelta, timedelta_representer)\n</code></pre> <p>Memory-efficient statistics calculator that uses incremental computation.</p> <p>This calculator avoids storing all data in memory by computing statistics incrementally as batches are processed using Welford's online algorithm for numerical stability and memory efficiency. All standard deviations are calculated as population standard deviations.</p> Source code in <code>src/calt/dataset_generator/sagemath/utils/statistics_calculator.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize incremental sample statistics calculator.\"\"\"\n    self.runtime_stats = IncrementalStatistics()\n    self.sample_stats = {}  # Store aggregated sample statistics by category\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter--efficient-batch-processing-with-file-handle-management","title":"Efficient batch processing with file handle management","text":"<p>writer = DatasetWriter(save_dir=\"./datasets\") writer.open(\"train\")  # Open file handles once try:     for batch_idx, samples in enumerate(batches):         writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx) finally:     writer.close(\"train\")  # Close file handles</p>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter--or-use-context-manager","title":"Or use context manager","text":"<p>with DatasetWriter(save_dir=\"./datasets\") as writer:     writer.open(\"train\")     for batch_idx, samples in enumerate(batches):         writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx)     writer.close(\"train\")</p>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter--support-for-various-dataset-splits","title":"Support for various dataset splits","text":"<p>writer.open(\"validation\")  # Validation set writer.open(\"dev\")         # Development set writer.open(\"eval\")        # Evaluation set</p>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter.open","title":"open","text":"<pre><code>open(tag: str) -&gt; None\n</code></pre> <p>Open file handles for the specified tag.</p> <p>This method should be called before starting batch processing to avoid repeated file open/close operations.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid</p> Source code in <code>src/calt/dataset_generator/sagemath/utils/dataset_writer.py</code> <pre><code>def open(self, tag: str) -&gt; None:\n    \"\"\"\n    Open file handles for the specified tag.\n\n    This method should be called before starting batch processing to avoid\n    repeated file open/close operations.\n\n    Args:\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n\n    Raises:\n        ValueError: If tag is invalid\n    \"\"\"\n    self._validate_tag(tag)\n\n    if tag in self._file_handles:\n        self.logger.warning(f\"File handles for tag '{tag}' are already open\")\n        return\n\n    dataset_dir = self._create_dataset_dir()\n    self._file_handles[tag] = {}\n\n    # Create batch directory for pickle files\n    batch_dir = dataset_dir / f\"{tag}_batches\"\n    batch_dir.mkdir(exist_ok=True)\n    self._file_handles[tag][\"batch_dir\"] = batch_dir\n    self._file_handles[tag][\"batch_count\"] = 0\n\n    # Open text file if enabled\n    if self.save_text:\n        raw_path = dataset_dir / f\"{tag}_raw.txt\"\n        self._file_handles[tag][\"text\"] = open(raw_path, \"w\")\n\n    # Open JSON Lines file if enabled\n    if self.save_json:\n        json_path = dataset_dir / f\"{tag}_data.jsonl\"\n        self._file_handles[tag][\"json\"] = open(json_path, \"w\")\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter.close","title":"close","text":"<pre><code>close(tag: str) -&gt; None\n</code></pre> <p>Close file handles for the specified tag.</p> <p>This method should be called after finishing batch processing.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid</p> Source code in <code>src/calt/dataset_generator/sagemath/utils/dataset_writer.py</code> <pre><code>def close(self, tag: str) -&gt; None:\n    \"\"\"\n    Close file handles for the specified tag.\n\n    This method should be called after finishing batch processing.\n\n    Args:\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n\n    Raises:\n        ValueError: If tag is invalid\n    \"\"\"\n    self._validate_tag(tag)\n\n    if tag not in self._file_handles:\n        self.logger.warning(f\"No open file handles found for tag '{tag}'\")\n        return\n\n    # Combine batch files into final pickle file\n    if \"batch_dir\" in self._file_handles[tag]:\n        self._combine_batch_files(tag)\n\n    # Close all open file handles\n    for file_type, file_handle in self._file_handles[tag].items():\n        if hasattr(file_handle, \"close\"):  # Only close actual file handles\n            try:\n                file_handle.close()\n            except Exception as e:\n                self.logger.error(\n                    f\"Error closing {file_type} file for tag '{tag}': {e}\"\n                )\n\n    del self._file_handles[tag]\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter.close_all","title":"close_all","text":"<pre><code>close_all() -&gt; None\n</code></pre> <p>Close all open file handles.</p> <p>This method should be called when the writer is no longer needed.</p> Source code in <code>src/calt/dataset_generator/sagemath/utils/dataset_writer.py</code> <pre><code>def close_all(self) -&gt; None:\n    \"\"\"\n    Close all open file handles.\n\n    This method should be called when the writer is no longer needed.\n    \"\"\"\n    for tag in list(self._file_handles.keys()):\n        self.close(tag)\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Context manager entry.</p> Source code in <code>src/calt/dataset_generator/sagemath/utils/dataset_writer.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Context manager exit - close all files.</p> Source code in <code>src/calt/dataset_generator/sagemath/utils/dataset_writer.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit - close all files.\"\"\"\n    self.close_all()\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter.save_batch","title":"save_batch","text":"<pre><code>save_batch(\n    samples: StringSampleList, tag: str = \"train\", batch_idx: int = 0\n) -&gt; None\n</code></pre> <p>Save a batch of samples to files in multiple formats.</p> <p>This method saves samples in three formats: 1. Pickle (.pkl) - Binary format, always saved, used for loading 2. Raw text (.txt) - Human-readable format with separators (if save_text=True) 3. JSON Lines (.jsonl) - Structured format preserving nested structure (if save_json=True)</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>StringSampleList</code> <p>List of (problem, solution) pairs in string format</p> required <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> <code>'train'</code> <code>batch_idx</code> <code>int</code> <p>Batch index for incremental saving. Use 0 for first batch,       subsequent batches will append to existing files.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid or samples contain invalid nested structures</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Simple string samples (single problem-solution pairs)\n&gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n&gt;&gt;&gt; samples = [\n...     (\"x^2 + 2*x + 1\", \"(x + 1)^2\"),\n...     (\"2*x^3 - 3*x^2\", \"x^2*(2*x - 3)\"),\n... ]\n&gt;&gt;&gt; # Creates: train_data.pkl, train_raw.txt, train_data.jsonl\n&gt;&gt;&gt; writer.save_batch(samples, tag=\"train\", batch_idx=0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 1 level nested structure samples (multiple problems/solutions)\n&gt;&gt;&gt; samples = [\n...     ([\"x + y\", \"x - y\"], [\"2*x\", \"2*y\"]),\n...     ([\"x^2 + y^2\", \"x^2 - y^2\"], [\"2*x^2\", \"2*y^2\"]),\n... ]\n&gt;&gt;&gt; # Text output: \"x + y | x - y # 2*x | 2*y\"\n&gt;&gt;&gt; writer.save_batch(samples, tag=\"test\", batch_idx=0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 2 level nested structure samples (complex nested problems)\n&gt;&gt;&gt; samples = [\n...     ([[\"x\", \"y\"], [\"z\", \"w\"]], [[\"x\", \"z\"], [\"y\", \"w\"]]),\n...     ([[\"x + y\", \"x - y\"], [\"z + w\", \"z - w\"]], [[\"x + y\", \"z + w\"], [\"x - y\", \"z - w\"]]),\n... ]\n&gt;&gt;&gt; # Text output: \"x | y || z | w # x | z || y | w\"\n&gt;&gt;&gt; writer.save_batch(samples, tag=\"test\", batch_idx=0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Append more samples to existing dataset\n&gt;&gt;&gt; more_samples = [\n...     ([[\"a\", \"b\"], [\"c\", \"d\"]], [[\"a\", \"c\"], [\"b\", \"d\"]]),\n...     ([[\"e\", \"f\"], [\"g\", \"h\"]], [[\"e\", \"g\"], [\"f\", \"h\"]]),\n... ]\n&gt;&gt;&gt; # Appends to existing files instead of overwriting\n&gt;&gt;&gt; writer.save_batch(more_samples, tag=\"train\", batch_idx=1)\n</code></pre> Source code in <code>src/calt/dataset_generator/sagemath/utils/dataset_writer.py</code> <pre><code>def save_batch(\n    self,\n    samples: StringSampleList,\n    tag: str = \"train\",\n    batch_idx: int = 0,\n) -&gt; None:\n    \"\"\"\n    Save a batch of samples to files in multiple formats.\n\n    This method saves samples in three formats:\n    1. Pickle (.pkl) - Binary format, always saved, used for loading\n    2. Raw text (.txt) - Human-readable format with separators (if save_text=True)\n    3. JSON Lines (.jsonl) - Structured format preserving nested structure (if save_json=True)\n\n    Args:\n        samples: List of (problem, solution) pairs in string format\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n        batch_idx: Batch index for incremental saving. Use 0 for first batch,\n                  subsequent batches will append to existing files.\n\n    Raises:\n        ValueError: If tag is invalid or samples contain invalid nested structures\n\n    Examples:\n        &gt;&gt;&gt; # Simple string samples (single problem-solution pairs)\n        &gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n        &gt;&gt;&gt; samples = [\n        ...     (\"x^2 + 2*x + 1\", \"(x + 1)^2\"),\n        ...     (\"2*x^3 - 3*x^2\", \"x^2*(2*x - 3)\"),\n        ... ]\n        &gt;&gt;&gt; # Creates: train_data.pkl, train_raw.txt, train_data.jsonl\n        &gt;&gt;&gt; writer.save_batch(samples, tag=\"train\", batch_idx=0)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 1 level nested structure samples (multiple problems/solutions)\n        &gt;&gt;&gt; samples = [\n        ...     ([\"x + y\", \"x - y\"], [\"2*x\", \"2*y\"]),\n        ...     ([\"x^2 + y^2\", \"x^2 - y^2\"], [\"2*x^2\", \"2*y^2\"]),\n        ... ]\n        &gt;&gt;&gt; # Text output: \"x + y | x - y # 2*x | 2*y\"\n        &gt;&gt;&gt; writer.save_batch(samples, tag=\"test\", batch_idx=0)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 2 level nested structure samples (complex nested problems)\n        &gt;&gt;&gt; samples = [\n        ...     ([[\"x\", \"y\"], [\"z\", \"w\"]], [[\"x\", \"z\"], [\"y\", \"w\"]]),\n        ...     ([[\"x + y\", \"x - y\"], [\"z + w\", \"z - w\"]], [[\"x + y\", \"z + w\"], [\"x - y\", \"z - w\"]]),\n        ... ]\n        &gt;&gt;&gt; # Text output: \"x | y || z | w # x | z || y | w\"\n        &gt;&gt;&gt; writer.save_batch(samples, tag=\"test\", batch_idx=0)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Append more samples to existing dataset\n        &gt;&gt;&gt; more_samples = [\n        ...     ([[\"a\", \"b\"], [\"c\", \"d\"]], [[\"a\", \"c\"], [\"b\", \"d\"]]),\n        ...     ([[\"e\", \"f\"], [\"g\", \"h\"]], [[\"e\", \"g\"], [\"f\", \"h\"]]),\n        ... ]\n        &gt;&gt;&gt; # Appends to existing files instead of overwriting\n        &gt;&gt;&gt; writer.save_batch(more_samples, tag=\"train\", batch_idx=1)\n    \"\"\"\n    self._validate_tag(tag)\n\n    # Validate samples\n    if not samples:\n        self.logger.warning(\n            \"Empty samples list provided - no files will be created\"\n        )\n        return\n\n    # Check if file handles are open for this tag\n    if tag not in self._file_handles:\n        # Fallback to old method if file handles are not open\n        self._save_batch_legacy(samples, tag, batch_idx)\n        return\n\n    # Save binary data (pickle format) - save batch individually\n    batch_file = (\n        self._file_handles[tag][\"batch_dir\"]\n        / f\"batch_{self._file_handles[tag]['batch_count']:06d}.pkl\"\n    )\n    with open(batch_file, \"wb\") as f:\n        pickle.dump(samples, f)\n    self._file_handles[tag][\"batch_count\"] += 1\n\n    # Save raw text data (optional)\n    if self.save_text:\n        for problem_str, solution_str in samples:\n            formatted_line = self._format_sample_strings(problem_str, solution_str)\n            self._file_handles[tag][\"text\"].write(f\"{formatted_line}\\n\")\n        self._file_handles[tag][\"text\"].flush()\n\n    # Save JSON Lines data (optional)\n    if self.save_json:\n        for problem_str, solution_str in samples:\n            json_data = self._get_json_data(problem_str, solution_str)\n            json_line = json.dumps(json_data, ensure_ascii=False)\n            self._file_handles[tag][\"json\"].write(f\"{json_line}\\n\")\n        self._file_handles[tag][\"json\"].flush()\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter.save_final_statistics","title":"save_final_statistics","text":"<pre><code>save_final_statistics(statistics: StatisticsDict, tag: str = 'train') -&gt; None\n</code></pre> <p>Save final overall statistics to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>statistics</code> <code>StatisticsDict</code> <p>Dictionary containing dataset statistics</p> required <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> <code>'train'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid</p> Note <p>Statistics are saved in YAML format for human readability. The file is named \"{tag}_stats.yaml\" in the dataset directory.</p> Source code in <code>src/calt/dataset_generator/sagemath/utils/dataset_writer.py</code> <pre><code>def save_final_statistics(\n    self,\n    statistics: StatisticsDict,\n    tag: str = \"train\",\n) -&gt; None:\n    \"\"\"\n    Save final overall statistics to YAML file.\n\n    Args:\n        statistics: Dictionary containing dataset statistics\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n\n    Raises:\n        ValueError: If tag is invalid\n\n    Note:\n        Statistics are saved in YAML format for human readability.\n        The file is named \"{tag}_stats.yaml\" in the dataset directory.\n    \"\"\"\n    self._validate_tag(tag)\n    dataset_dir = self._create_dataset_dir()\n\n    stats_path = dataset_dir / f\"{tag}_stats.yaml\"\n    with open(stats_path, \"w\") as f:\n        yaml.dump(\n            statistics,\n            f,\n            Dumper=TimedeltaDumper,\n            default_flow_style=False,\n            sort_keys=False,\n            indent=4,\n        )\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter.load_dataset","title":"load_dataset","text":"<pre><code>load_dataset(tag: str) -&gt; StringSampleList\n</code></pre> <p>Load dataset from pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> required <p>Returns:</p> Type Description <code>StringSampleList</code> <p>List of (problem, solution) pairs in string format</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid</p> <code>FileNotFoundError</code> <p>If the pickle file doesn't exist</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; samples = writer.load_dataset(\"train\")\n&gt;&gt;&gt; print(f\"Loaded {len(samples)} samples\")\n&gt;&gt;&gt; for problem, solution in samples[:3]:\n...     print(f\"Problem: {problem}, Solution: {solution}\")\n</code></pre> Source code in <code>src/calt/dataset_generator/sagemath/utils/dataset_writer.py</code> <pre><code>def load_dataset(self, tag: str) -&gt; StringSampleList:\n    \"\"\"\n    Load dataset from pickle file.\n\n    Args:\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n\n    Returns:\n        List of (problem, solution) pairs in string format\n\n    Raises:\n        ValueError: If tag is invalid\n        FileNotFoundError: If the pickle file doesn't exist\n\n    Examples:\n        &gt;&gt;&gt; samples = writer.load_dataset(\"train\")\n        &gt;&gt;&gt; print(f\"Loaded {len(samples)} samples\")\n        &gt;&gt;&gt; for problem, solution in samples[:3]:\n        ...     print(f\"Problem: {problem}, Solution: {solution}\")\n    \"\"\"\n    self._validate_tag(tag)\n    pickle_path = self.save_dir / f\"{tag}_data.pkl\"\n\n    if not pickle_path.exists():\n        raise FileNotFoundError(f\"Dataset file not found: {pickle_path}\")\n\n    with open(pickle_path, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.dataset_writer.DatasetWriter.load_dataset_jsonl","title":"load_dataset_jsonl","text":"<pre><code>load_dataset_jsonl(tag: str) -&gt; StringSampleList\n</code></pre> <p>Load dataset from JSON Lines file.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> required <p>Returns:</p> Type Description <code>StringSampleList</code> <p>List of (problem, solution) pairs in string format</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid</p> <code>FileNotFoundError</code> <p>If the JSON Lines file doesn't exist</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; samples = writer.load_dataset_jsonl(\"train\")\n&gt;&gt;&gt; print(f\"Loaded {len(samples)} samples\")\n&gt;&gt;&gt; for problem, solution in samples[:3]:\n...     print(f\"Problem: {problem}, Solution: {solution}\")\n</code></pre> Source code in <code>src/calt/dataset_generator/sagemath/utils/dataset_writer.py</code> <pre><code>def load_dataset_jsonl(self, tag: str) -&gt; StringSampleList:\n    \"\"\"\n    Load dataset from JSON Lines file.\n\n    Args:\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n\n    Returns:\n        List of (problem, solution) pairs in string format\n\n    Raises:\n        ValueError: If tag is invalid\n        FileNotFoundError: If the JSON Lines file doesn't exist\n\n    Examples:\n        &gt;&gt;&gt; samples = writer.load_dataset_jsonl(\"train\")\n        &gt;&gt;&gt; print(f\"Loaded {len(samples)} samples\")\n        &gt;&gt;&gt; for problem, solution in samples[:3]:\n        ...     print(f\"Problem: {problem}, Solution: {solution}\")\n    \"\"\"\n    self._validate_tag(tag)\n    jsonl_path = self.save_dir / f\"{tag}_data.jsonl\"\n\n    if not jsonl_path.exists():\n        raise FileNotFoundError(f\"JSON Lines file not found: {jsonl_path}\")\n\n    samples = []\n    with open(jsonl_path, \"r\") as f:\n        for line_num, line in enumerate(f, 1):\n            line = line.strip()\n            if not line:  # Skip empty lines\n                continue\n            try:\n                data = json.loads(line)\n                problem = data[\"problem\"]\n                solution = data[\"solution\"]\n                samples.append((problem, solution))\n            except (json.JSONDecodeError, KeyError) as e:\n                self.logger.warning(f\"Error parsing line {line_num}: {e}\")\n                continue\n\n    return samples\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.statistics_calculator.MemoryEfficientStatisticsCalculator.update_batch","title":"update_batch","text":"<pre><code>update_batch(\n    runtimes: list[float],\n    batch_sample_stats: list[dict[str, dict[str, int | float]]],\n) -&gt; None\n</code></pre> <p>Update statistics with a batch of results using Welford's online algorithm.</p> <p>This method processes each sample individually, updating both runtime statistics and sample-specific statistics incrementally for better control and efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>runtimes</code> <code>list[float]</code> <p>List of runtime values for each sample in the batch</p> required <code>batch_sample_stats</code> <code>list[dict[str, dict[str, int | float]]]</code> <p>List of sample statistics dictionaries for the current batch.                Each dictionary has the structure:                {\"category1\": {\"metric1\": value1, ...},                 \"category2\": {\"metric1\": value1, ...}}                Example:                [{\"problem\": {\"total_degree\": 2, \"num_polynomials\": 3},                  \"solution\": {\"total_degree\": 3, \"num_polynomials\": 3}},                 {\"problem\": {\"total_degree\": 5, \"num_polynomials\": 4},                  \"solution\": {\"total_degree\": 8, \"num_polynomials\": 4}},                 ...]</p> required Source code in <code>src/calt/dataset_generator/sagemath/utils/statistics_calculator.py</code> <pre><code>def update_batch(\n    self,\n    runtimes: list[float],\n    batch_sample_stats: list[dict[str, dict[str, int | float]]],\n) -&gt; None:\n    \"\"\"\n    Update statistics with a batch of results using Welford's online algorithm.\n\n    This method processes each sample individually, updating both runtime\n    statistics and sample-specific statistics incrementally for better\n    control and efficiency.\n\n    Args:\n        runtimes: List of runtime values for each sample in the batch\n        batch_sample_stats: List of sample statistics dictionaries for the current batch.\n                           Each dictionary has the structure:\n                           {\"category1\": {\"metric1\": value1, ...},\n                            \"category2\": {\"metric1\": value1, ...}}\n                           Example:\n                           [{\"problem\": {\"total_degree\": 2, \"num_polynomials\": 3},\n                             \"solution\": {\"total_degree\": 3, \"num_polynomials\": 3}},\n                            {\"problem\": {\"total_degree\": 5, \"num_polynomials\": 4},\n                             \"solution\": {\"total_degree\": 8, \"num_polynomials\": 4}},\n                            ...]\n    \"\"\"\n    # Update runtime statistics\n    for runtime in runtimes:\n        self.runtime_stats.update(runtime)\n\n    # Update sample statistics\n    for stats in batch_sample_stats:\n        # Update each numeric sample statistic incrementally\n        for category, category_stats in stats.items():\n            if isinstance(category_stats, dict):\n                # Handle nested structure like {\"problem\": {...}, \"solution\": {...}}\n                if category not in self.sample_stats:\n                    self.sample_stats[category] = {}\n\n                for stat_name, value in category_stats.items():\n                    if isinstance(value, (int, float)):\n                        if stat_name not in self.sample_stats[category]:\n                            self.sample_stats[category][stat_name] = (\n                                IncrementalStatistics()\n                            )\n                        self.sample_stats[category][stat_name].update(float(value))\n\n            elif isinstance(category_stats, (int, float)):\n                # Handle flat structure\n                if category not in self.sample_stats:\n                    self.sample_stats[category] = IncrementalStatistics()\n                self.sample_stats[category].update(float(category_stats))\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.statistics_calculator.MemoryEfficientStatisticsCalculator.get_overall_statistics","title":"get_overall_statistics","text":"<pre><code>get_overall_statistics(total_time: float, num_samples: int) -&gt; dict[str, Any]\n</code></pre> <p>Get overall statistics.</p> <p>Parameters:</p> Name Type Description Default <code>total_time</code> <code>float</code> <p>Total processing time</p> required <code>num_samples</code> <code>int</code> <p>Total number of samples</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing overall statistics with the structure:</p> <code>dict[str, Any]</code> <p>{ \"total_time\": float, \"num_samples\": int, \"samples_per_second\": float, \"generation_time\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float}, \"problem_stats\": {\"metric1\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float}, ...}, \"solution_stats\": {\"metric1\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float}, ...}</p> <code>dict[str, Any]</code> <p>}</p> Source code in <code>src/calt/dataset_generator/sagemath/utils/statistics_calculator.py</code> <pre><code>def get_overall_statistics(\n    self, total_time: float, num_samples: int\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Get overall statistics.\n\n    Args:\n        total_time: Total processing time\n        num_samples: Total number of samples\n\n    Returns:\n        Dictionary containing overall statistics with the structure:\n        {\n            \"total_time\": float,\n            \"num_samples\": int,\n            \"samples_per_second\": float,\n            \"generation_time\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float},\n            \"problem_stats\": {\"metric1\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float}, ...},\n            \"solution_stats\": {\"metric1\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float}, ...}\n        }\n    \"\"\"\n    runtime_stats = self.runtime_stats.get_statistics()\n\n    overall_stats = {\n        \"total_time\": total_time,\n        \"num_samples\": num_samples,\n        \"samples_per_second\": num_samples / total_time if total_time &gt; 0 else 0.0,\n        \"generation_time\": {\n            \"mean\": runtime_stats[\"mean\"],\n            \"std\": runtime_stats[\"std\"],\n            \"min\": runtime_stats[\"min\"],\n            \"max\": runtime_stats[\"max\"],\n        },\n    }\n\n    # Add sample statistics by category\n    for category, category_stats in self.sample_stats.items():\n        if isinstance(category_stats, dict):\n            # Handle nested structure like {\"problem\": {...}, \"solution\": {...}}\n            overall_stats[f\"{category}_stats\"] = {\n                stat_name: stat_calc.get_statistics()\n                for stat_name, stat_calc in category_stats.items()\n            }\n        else:\n            # Handle flat structure\n            overall_stats[f\"{category}_stats\"] = category_stats.get_statistics()\n\n    return overall_stats\n</code></pre>"},{"location":"dataset_generator/#sampling","title":"Sampling","text":"<p>Generator for random polynomials with specific constraints</p> <p>Parameters:</p> Name Type Description Default <code>symbols</code> <code>str | None</code> <p>Symbols of polynomial ring (required if ring is None)</p> <code>None</code> <code>field_str</code> <code>str | None</code> <p>Field of polynomial ring (required if ring is None)</p> <code>None</code> <code>order</code> <code>str | TermOrder | None</code> <p>Order of polynomial ring (required if ring is None)</p> <code>None</code> <code>ring</code> <code>Any</code> <p>PolynomialRing object (alternative to symbols/field_str/order)</p> <code>None</code> <code>max_num_terms</code> <code>int | None</code> <p>Maximum number of terms in polynomial. If None, all possible terms are allowed.</p> <code>10</code> <code>max_degree</code> <code>int</code> <p>Maximum degree of polynomial</p> <code>5</code> <code>min_degree</code> <code>int</code> <p>Minimum degree of polynomial</p> <code>0</code> <code>max_coeff</code> <code>int | None</code> <p>Maximum coefficient value (used for RR and ZZ)</p> <code>None</code> <code>num_bound</code> <code>int | None</code> <p>Maximum absolute value of coefficients (used for QQ)</p> <code>None</code> <code>degree_sampling</code> <code>str</code> <p>How to sample degree ('uniform' or 'fixed')</p> <code>'uniform'</code> <code>term_sampling</code> <code>str</code> <p>How to sample number of terms ('uniform' or 'fixed')</p> <code>'uniform'</code> <code>strictly_conditioned</code> <code>bool</code> <p>Whether to strictly enforce conditions</p> <code>True</code> <code>nonzero_instance</code> <code>bool</code> <p>Whether to enforce non-zero instance</p> <code>True</code> <code>nonzero_coeff</code> <code>bool</code> <p>Whether to exclude zero coefficients during coefficient generation</p> <code>False</code> <code>max_attempts</code> <code>int</code> <p>Maximum number of attempts to generate a polynomial satisfying conditions</p> <code>1000</code> Source code in <code>src/calt/dataset_generator/sagemath/utils/polynomial_sampler.py</code> <pre><code>def __init__(\n    self,\n    symbols: str | None = None,\n    field_str: str | None = None,\n    order: str | TermOrder | None = None,\n    ring: Any = None,\n    max_num_terms: int | None = 10,\n    max_degree: int = 5,\n    min_degree: int = 0,\n    degree_sampling: str = \"uniform\",  # 'uniform' or 'fixed'\n    term_sampling: str = \"uniform\",  # 'uniform' or 'fixed'\n    max_coeff: int | None = None,  # Used for RR and ZZ\n    num_bound: int | None = None,  # Used for QQ\n    strictly_conditioned: bool = True,\n    nonzero_instance: bool = True,\n    nonzero_coeff: bool = False,  # Whether to exclude zero coefficients\n    max_attempts: int = 1000,\n):\n    \"\"\"\n    Initialize polynomial sampler\n\n    Args:\n        symbols: Symbols of polynomial ring (required if ring is None)\n        field_str: Field of polynomial ring (required if ring is None)\n        order: Order of polynomial ring (required if ring is None)\n        ring: PolynomialRing object (alternative to symbols/field_str/order)\n        max_num_terms: Maximum number of terms in polynomial. If None, all possible terms are allowed.\n        max_degree: Maximum degree of polynomial\n        min_degree: Minimum degree of polynomial\n        max_coeff: Maximum coefficient value (used for RR and ZZ)\n        num_bound: Maximum absolute value of coefficients (used for QQ)\n        degree_sampling: How to sample degree ('uniform' or 'fixed')\n        term_sampling: How to sample number of terms ('uniform' or 'fixed')\n        strictly_conditioned: Whether to strictly enforce conditions\n        nonzero_instance: Whether to enforce non-zero instance\n        nonzero_coeff: Whether to exclude zero coefficients during coefficient generation\n        max_attempts: Maximum number of attempts to generate a polynomial satisfying conditions\n    \"\"\"\n    # Validate input parameters\n    if ring is not None:\n        if symbols is not None or field_str is not None or order is not None:\n            raise ValueError(\"Cannot specify both ring and symbols/field_str/order\")\n        self.ring = ring\n        self.symbols = None\n        self.field_str = None\n        self.order = None\n    else:\n        if symbols is None or field_str is None or order is None:\n            raise ValueError(\n                \"Must specify either ring or all of symbols/field_str/order\"\n            )\n        self.ring = None\n        self.symbols = symbols\n        self.field_str = field_str\n        self.order = order\n\n    self.max_num_terms = max_num_terms\n    self.max_degree = max_degree\n    self.min_degree = min_degree\n    self.max_coeff = max_coeff\n    self.num_bound = num_bound\n    self.degree_sampling = degree_sampling\n    self.term_sampling = term_sampling\n    self.strictly_conditioned = strictly_conditioned\n    self.nonzero_instance = nonzero_instance\n    self.nonzero_coeff = nonzero_coeff\n    self.max_attempts = max_attempts\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.polynomial_sampler.PolynomialSampler.get_field","title":"get_field","text":"<pre><code>get_field()\n</code></pre> <p>Convert field_str to actual sympy domain object</p> Source code in <code>src/calt/dataset_generator/sagemath/utils/polynomial_sampler.py</code> <pre><code>def get_field(self):\n    \"\"\"Convert field_str to actual sympy domain object\"\"\"\n    if self.ring is not None:\n        return self.ring.base_ring()\n\n    # Standard field mapping\n    standard_fields = {\"QQ\": QQ, \"RR\": RR, \"ZZ\": ZZ}\n    if self.field_str in standard_fields:\n        return standard_fields[self.field_str]\n\n    # Finite field handling\n    if not self.field_str.startswith(\"GF\"):\n        raise ValueError(f\"Unsupported field: {self.field_str}\")\n\n    try:\n        # Extract field size based on format\n        p = int(\n            self.field_str[3:-1]\n            if self.field_str.startswith(\"GF(\")\n            else self.field_str[2:]\n        )\n\n        if p &lt;= 1:\n            raise ValueError(f\"Field size must be greater than 1: {p}\")\n        return GF(p)\n    except ValueError as e:\n        raise ValueError(f\"Unsupported field: {self.field_str}\") from e\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.polynomial_sampler.PolynomialSampler.get_ring","title":"get_ring","text":"<pre><code>get_ring() -&gt; PolynomialRing\n</code></pre> <p>Generate polynomial ring</p> <p>Returns:</p> Name Type Description <code>PolynomialRing</code> <code>PolynomialRing</code> <p>Generated polynomial ring</p> Source code in <code>src/calt/dataset_generator/sagemath/utils/polynomial_sampler.py</code> <pre><code>def get_ring(self) -&gt; PolynomialRing:\n    \"\"\"\n    Generate polynomial ring\n\n    Returns:\n        PolynomialRing: Generated polynomial ring\n    \"\"\"\n    if self.ring is not None:\n        return self.ring\n\n    R = PolynomialRing(self.get_field(), self.symbols, order=self.order)\n    return R\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sagemath.utils.polynomial_sampler.PolynomialSampler.sample","title":"sample","text":"<pre><code>sample(\n    num_samples: int = 1,\n    size: tuple[int, int] | None = None,\n    density: float = 1.0,\n    matrix_type: str | None = None,\n) -&gt; list[MPolynomial_libsingular] | list[matrix]\n</code></pre> <p>Generate random polynomial samples</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>size</code> <code>tuple[int, int] | None</code> <p>If provided, generate matrix of polynomials with given size</p> <code>None</code> <code>density</code> <code>float</code> <p>Probability of non-zero entries in matrix</p> <code>1.0</code> <code>matrix_type</code> <code>str | None</code> <p>Special matrix type (e.g., 'unimodular_upper_triangular')</p> <code>None</code> <p>Returns:</p> Type Description <code>list[MPolynomial_libsingular] | list[matrix]</code> <p>List of polynomials or polynomial matrices</p> Source code in <code>src/calt/dataset_generator/sagemath/utils/polynomial_sampler.py</code> <pre><code>def sample(\n    self,\n    num_samples: int = 1,\n    size: tuple[int, int] | None = None,\n    density: float = 1.0,\n    matrix_type: str | None = None,\n) -&gt; list[MPolynomial_libsingular] | list[matrix]:\n    \"\"\"\n    Generate random polynomial samples\n\n    Args:\n        num_samples: Number of samples to generate\n        size: If provided, generate matrix of polynomials with given size\n        density: Probability of non-zero entries in matrix\n        matrix_type: Special matrix type (e.g., 'unimodular_upper_triangular')\n\n    Returns:\n        List of polynomials or polynomial matrices\n    \"\"\"\n    if size is not None:\n        return [\n            self._sample_matrix(size, density, matrix_type)\n            for _ in range(num_samples)\n        ]\n    else:\n        return [self._sample_polynomial() for _ in range(num_samples)]\n</code></pre>"},{"location":"dataset_generator/#common-sympy-backend-example","title":"Common (SymPy backend example)","text":""},{"location":"dataset_generator/#generation-flow_1","title":"Generation flow","text":"<p>Base class for problem generators</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Backend for parallel processing</p> <code>'multiprocessing'</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs (-1 for all cores)</p> <code>-1</code> <code>verbose</code> <code>bool</code> <p>Whether to display progress information</p> <code>True</code> <code>root_seed</code> <code>int</code> <p>Root seed for reproducibility</p> <code>42</code> Source code in <code>src/calt/dataset_generator/sympy/dataset_generator.py</code> <pre><code>def __init__(\n    self,\n    backend: str = \"multiprocessing\",\n    n_jobs: int = -1,\n    verbose: bool = True,\n    root_seed: int = 42,\n):\n    \"\"\"\n    Initialize problem generator.\n\n    Args:\n        backend: Backend for parallel processing\n        n_jobs: Number of parallel jobs (-1 for all cores)\n        verbose: Whether to display progress information\n        root_seed: Root seed for reproducibility\n    \"\"\"\n\n    self.backend = backend\n    self.n_jobs = n_jobs\n    self.verbose = verbose\n    self.root_seed = root_seed\n\n    # Configure logging only once at initialization\n    self.logger = logger\n\n    # Configure joblib logging to show progress but not overwhelm\n    # Only set if not already configured\n    joblib_logger = logging.getLogger(\"joblib\")\n    if joblib_logger.level == logging.NOTSET:\n        joblib_logger.setLevel(logging.INFO)\n\n    parallel_logger = logging.getLogger(\"joblib.Parallel\")\n    if parallel_logger.level == logging.NOTSET:\n        parallel_logger.setLevel(logging.INFO)\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.dataset_generator.DatasetGenerator.run","title":"run","text":"<pre><code>run(\n    dataset_sizes: dict[str, int],\n    problem_generator: Callable,\n    statistics_calculator: Callable | None = None,\n    dataset_writer: DatasetWriter | None = None,\n    batch_size: int = 100000,\n    save_dir: str | None = None,\n    save_text: bool = True,\n    save_json: bool = True,\n)\n</code></pre> <p>Generate multiple datasets using parallel processing with batch writing.</p> <p>This is the main entry point for dataset generation. It supports generating multiple datasets (train/test) simultaneously or separately, with efficient memory management through batch processing and parallel execution.</p> <p>Key features: - Parallel processing using joblib for high performance - Batch-based memory management to handle large datasets - Incremental statistics calculation to avoid memory issues - Reproducible generation with unique seeds for each sample - Support for nested data structures (up to 2 levels) - Multiple output formats (pickle, text, JSON) via DatasetWriter</p> <p>Parameters:</p> Name Type Description Default <code>dataset_sizes</code> <code>dict[str, int]</code> <p>Dictionary mapping dataset names to number of samples.           Any string can be used as dataset name (e.g., \"train\", \"test\", \"validation\").           Duplicate names are not allowed.           Example: {\"train\": 100000, \"test\": 1000} or {\"train\": 100000, \"validation\": 5000}</p> required <code>problem_generator</code> <code>Callable</code> <p>Function that generates (problem, solution) pair given a seed.              Must accept a single integer seed parameter.</p> required <code>statistics_calculator</code> <code>Callable | None</code> <p>Optional function to calculate sample-specific statistics.                  Must accept (problem, solution) and return dict or None.</p> <code>None</code> <code>dataset_writer</code> <code>DatasetWriter | None</code> <p>DatasetWriter object for saving datasets to files.           If None, a new DatasetWriter will be created using save_dir, save_text, and save_json parameters.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of samples to process in each batch. Larger batches        use more memory but may be more efficient for I/O operations.</p> <code>100000</code> <code>save_dir</code> <code>str | None</code> <p>Base directory for saving datasets. Used only if dataset_writer is None.      If None, uses current working directory.</p> <code>None</code> <code>save_text</code> <code>bool</code> <p>Whether to save raw text files. Used only if dataset_writer is None.       Text files use \"#\" as separator between problem and solution.</p> <code>True</code> <code>save_json</code> <code>bool</code> <p>Whether to save JSON Lines files. Used only if dataset_writer is None.       JSON Lines files preserve the original nested structure format.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset_sizes is invalid or problem_generator is None</p> <code>Exception</code> <p>If parallel processing fails</p> Note <ul> <li>Each sample gets a unique seed for reproducibility</li> <li>Progress is logged if verbose=True (set in init)</li> <li>Memory usage scales with batch_size, not total dataset size</li> <li>Statistics are calculated incrementally to handle large datasets</li> <li>If dataset_writer is provided, save_dir, save_text, and save_json parameters are ignored</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Define problem generator function\n&gt;&gt;&gt; def polynomial_generator(seed):\n...     import random\n...     random.seed(seed)\n...     # Generate random polynomial problem\n...     problem = [random.randint(1, 1000) for _ in range(random.randint(1, 10))]\n...     solution = sum(problem)\n...     return problem, solution\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize dataset generator\n&gt;&gt;&gt; generator = DatasetGenerator(n_jobs=-1, verbose=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 1: Automatic DatasetWriter creation\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000, \"test\": 1000, \"validation\": 500},\n...     problem_generator=polynomial_generator,\n...     save_dir=\"./datasets\",\n...     save_text=True,\n...     save_json=True,\n...     batch_size=100\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 2: Manual DatasetWriter creation (for advanced use cases)\n&gt;&gt;&gt; from calt.dataset_generator.sympy import DatasetWriter\n&gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000, \"test\": 1000},\n...     problem_generator=polynomial_generator,\n...     dataset_writer=writer,\n...     batch_size=100\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method 3: Generate datasets separately (if needed)\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"train\": 10000},\n...     problem_generator=polynomial_generator,\n...     save_dir=\"./datasets\",\n...     batch_size=100\n... )\n&gt;&gt;&gt; generator.run(\n...     dataset_sizes={\"test\": 1000, \"validation\": 500},\n...     problem_generator=polynomial_generator,\n...     save_dir=\"./datasets\",\n...     batch_size=100\n... )\n</code></pre> Source code in <code>src/calt/dataset_generator/sympy/dataset_generator.py</code> <pre><code>def run(\n    self,\n    dataset_sizes: dict[str, int],\n    problem_generator: Callable,\n    statistics_calculator: Callable | None = None,\n    dataset_writer: DatasetWriter | None = None,\n    batch_size: int = 100000,\n    save_dir: str | None = None,\n    save_text: bool = True,\n    save_json: bool = True,\n):\n    \"\"\"\n    Generate multiple datasets using parallel processing with batch writing.\n\n    This is the main entry point for dataset generation. It supports generating\n    multiple datasets (train/test) simultaneously or separately, with efficient\n    memory management through batch processing and parallel execution.\n\n    Key features:\n    - Parallel processing using joblib for high performance\n    - Batch-based memory management to handle large datasets\n    - Incremental statistics calculation to avoid memory issues\n    - Reproducible generation with unique seeds for each sample\n    - Support for nested data structures (up to 2 levels)\n    - Multiple output formats (pickle, text, JSON) via DatasetWriter\n\n    Args:\n        dataset_sizes: Dictionary mapping dataset names to number of samples.\n                      Any string can be used as dataset name (e.g., \"train\", \"test\", \"validation\").\n                      Duplicate names are not allowed.\n                      Example: {\"train\": 100000, \"test\": 1000} or {\"train\": 100000, \"validation\": 5000}\n        problem_generator: Function that generates (problem, solution) pair given a seed.\n                         Must accept a single integer seed parameter.\n        statistics_calculator: Optional function to calculate sample-specific statistics.\n                             Must accept (problem, solution) and return dict or None.\n        dataset_writer: DatasetWriter object for saving datasets to files.\n                      If None, a new DatasetWriter will be created using save_dir, save_text, and save_json parameters.\n        batch_size: Number of samples to process in each batch. Larger batches\n                   use more memory but may be more efficient for I/O operations.\n        save_dir: Base directory for saving datasets. Used only if dataset_writer is None.\n                 If None, uses current working directory.\n        save_text: Whether to save raw text files. Used only if dataset_writer is None.\n                  Text files use \"#\" as separator between problem and solution.\n        save_json: Whether to save JSON Lines files. Used only if dataset_writer is None.\n                  JSON Lines files preserve the original nested structure format.\n\n    Raises:\n        ValueError: If dataset_sizes is invalid or problem_generator is None\n        Exception: If parallel processing fails\n\n    Note:\n        - Each sample gets a unique seed for reproducibility\n        - Progress is logged if verbose=True (set in __init__)\n        - Memory usage scales with batch_size, not total dataset size\n        - Statistics are calculated incrementally to handle large datasets\n        - If dataset_writer is provided, save_dir, save_text, and save_json parameters are ignored\n\n    Examples:\n        &gt;&gt;&gt; # Define problem generator function\n        &gt;&gt;&gt; def polynomial_generator(seed):\n        ...     import random\n        ...     random.seed(seed)\n        ...     # Generate random polynomial problem\n        ...     problem = [random.randint(1, 1000) for _ in range(random.randint(1, 10))]\n        ...     solution = sum(problem)\n        ...     return problem, solution\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Initialize dataset generator\n        &gt;&gt;&gt; generator = DatasetGenerator(n_jobs=-1, verbose=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 1: Automatic DatasetWriter creation\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000, \"test\": 1000, \"validation\": 500},\n        ...     problem_generator=polynomial_generator,\n        ...     save_dir=\"./datasets\",\n        ...     save_text=True,\n        ...     save_json=True,\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 2: Manual DatasetWriter creation (for advanced use cases)\n        &gt;&gt;&gt; from calt.dataset_generator.sympy import DatasetWriter\n        &gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000, \"test\": 1000},\n        ...     problem_generator=polynomial_generator,\n        ...     dataset_writer=writer,\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Method 3: Generate datasets separately (if needed)\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"train\": 10000},\n        ...     problem_generator=polynomial_generator,\n        ...     save_dir=\"./datasets\",\n        ...     batch_size=100\n        ... )\n        &gt;&gt;&gt; generator.run(\n        ...     dataset_sizes={\"test\": 1000, \"validation\": 500},\n        ...     problem_generator=polynomial_generator,\n        ...     save_dir=\"./datasets\",\n        ...     batch_size=100\n        ... )\n    \"\"\"\n    # Create DatasetWriter if not provided\n    if dataset_writer is None:\n        dataset_writer = DatasetWriter(\n            save_dir=save_dir,\n            save_text=save_text,\n            save_json=save_json,\n        )\n        self.logger.info(f\"save_dir: {dataset_writer.save_dir}\")\n        self.logger.info(f\"Text output: {save_text}\")\n        self.logger.info(f\"JSON output: {save_json}\")\n\n    # Prepare common arguments\n    common_args = {\n        \"problem_generator\": problem_generator,\n        \"statistics_calculator\": statistics_calculator,\n        \"dataset_writer\": dataset_writer,\n        \"batch_size\": batch_size,\n    }\n\n    # Validate dataset_sizes\n    if not isinstance(dataset_sizes, dict):\n        raise ValueError(\"dataset_sizes must be a dictionary\")\n\n    if not dataset_sizes:\n        raise ValueError(\"dataset_sizes cannot be empty\")\n\n    if problem_generator is None:\n        raise ValueError(\"problem_generator must be provided\")\n\n    # Check for duplicate dataset names\n    if len(dataset_sizes) != len(set(dataset_sizes.keys())):\n        raise ValueError(\"Duplicate dataset names are not allowed\")\n\n    for dataset_name, num_samples in dataset_sizes.items():\n        if not isinstance(num_samples, int) or num_samples &lt;= 0:\n            raise ValueError(\n                f\"Number of samples must be a positive integer, got {num_samples} for {dataset_name}\"\n            )\n\n    # Log overall generation start\n    self.logger.info(\n        \"=========================== Dataset generation ===========================\\n\"\n    )\n    self.logger.info(\n        f\"Starting dataset generation for {len(dataset_sizes)} dataset(s)\"\n    )\n    self.logger.info(f\"Dataset sizes: {dataset_sizes}\\n\")\n\n    # Generate each dataset\n    for dataset_name, num_samples in dataset_sizes.items():\n        self._generate_dataset(\n            tag=dataset_name, num_samples=num_samples, **common_args\n        )\n\n    self.logger.info(\"All datasets generated successfully!\")\n    self.logger.info(\n        \"==========================================================================\\n\"\n    )\n</code></pre>"},{"location":"dataset_generator/#writing-and-statistics_1","title":"Writing and statistics","text":"<p>Dataset writer for saving problem-solution pairs in multiple formats.</p> <p>This class handles saving datasets with nested structure support up to 2 levels. It can save data in pickle (binary), raw text, and JSON Lines formats.</p> <p>Attributes:</p> Name Type Description <code>INNER_SEP</code> <code>str</code> <p>Separator for single-level lists (\" | \")</p> <code>OUTER_SEP</code> <code>str</code> <p>Separator for nested lists (\" || \")</p> <code>save_dir</code> <code>Path</code> <p>Base directory for saving datasets</p> <code>save_text</code> <code>bool</code> <p>Whether to save raw text files</p> <code>save_json</code> <code>bool</code> <p>Whether to save JSON Lines files</p> <code>_file_handles</code> <code>dict</code> <p>Dictionary to store open file handles</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str | None</code> <p>Base directory for saving datasets. If None, uses current working directory.</p> <code>None</code> <code>save_text</code> <code>bool</code> <p>Whether to save raw text files. Text files use \"#\" as separator       between problem and solution, with nested structures joined by separators.</p> <code>True</code> <code>save_json</code> <code>bool</code> <p>Whether to save JSON Lines files. JSON Lines files preserve the original       nested structure format, with one sample per line.</p> <code>True</code> Note <p>Pickle files are always saved as they are the primary format for data loading. Text and JSON Lines files are optional and controlled by save_text and save_json flags.</p> Usage Source code in <code>src/calt/dataset_generator/sympy/utils/dataset_writer.py</code> <pre><code>def __init__(\n    self,\n    save_dir: str | None = None,\n    save_text: bool = True,\n    save_json: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize dataset writer.\n\n    Args:\n        save_dir: Base directory for saving datasets. If None, uses current working directory.\n        save_text: Whether to save raw text files. Text files use \"#\" as separator\n                  between problem and solution, with nested structures joined by separators.\n        save_json: Whether to save JSON Lines files. JSON Lines files preserve the original\n                  nested structure format, with one sample per line.\n\n    Note:\n        Pickle files are always saved as they are the primary format for data loading.\n        Text and JSON Lines files are optional and controlled by save_text and save_json flags.\n\n    Usage:\n        # Efficient batch processing with file handle management\n        writer = DatasetWriter(save_dir=\"./datasets\")\n        writer.open(\"train\")  # Open file handles once\n        try:\n            for batch_idx, samples in enumerate(batches):\n                writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx)\n        finally:\n            writer.close(\"train\")  # Close file handles\n\n        # Or use context manager\n        with DatasetWriter(save_dir=\"./datasets\") as writer:\n            writer.open(\"train\")\n            for batch_idx, samples in enumerate(batches):\n                writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx)\n            writer.close(\"train\")\n\n        # Support for various dataset splits\n        writer.open(\"validation\")  # Validation set\n        writer.open(\"dev\")         # Development set\n        writer.open(\"eval\")        # Evaluation set\n    \"\"\"\n    self.save_dir = Path(save_dir) if save_dir else Path.cwd()\n    self.save_text = save_text\n    self.save_json = save_json\n    self.logger = logging.getLogger(__name__)\n    self._file_handles: dict[\n        str, dict[str, any]\n    ] = {}  # {tag: {file_type: file_handle}}\n    TimedeltaDumper.add_representer(timedelta, timedelta_representer)\n</code></pre> <p>Memory-efficient statistics calculator that uses incremental computation.</p> <p>This calculator avoids storing all data in memory by computing statistics incrementally as batches are processed using Welford's online algorithm for numerical stability and memory efficiency. All standard deviations are calculated as population standard deviations.</p> Source code in <code>src/calt/dataset_generator/sympy/utils/statistics_calculator.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize incremental sample statistics calculator.\"\"\"\n    self.runtime_stats = IncrementalStatistics()\n    self.sample_stats = {}  # Store aggregated sample statistics by category\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter--efficient-batch-processing-with-file-handle-management","title":"Efficient batch processing with file handle management","text":"<p>writer = DatasetWriter(save_dir=\"./datasets\") writer.open(\"train\")  # Open file handles once try:     for batch_idx, samples in enumerate(batches):         writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx) finally:     writer.close(\"train\")  # Close file handles</p>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter--or-use-context-manager","title":"Or use context manager","text":"<p>with DatasetWriter(save_dir=\"./datasets\") as writer:     writer.open(\"train\")     for batch_idx, samples in enumerate(batches):         writer.save_batch(samples, tag=\"train\", batch_idx=batch_idx)     writer.close(\"train\")</p>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter--support-for-various-dataset-splits","title":"Support for various dataset splits","text":"<p>writer.open(\"validation\")  # Validation set writer.open(\"dev\")         # Development set writer.open(\"eval\")        # Evaluation set</p>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter.open","title":"open","text":"<pre><code>open(tag: str) -&gt; None\n</code></pre> <p>Open file handles for the specified tag.</p> <p>This method should be called before starting batch processing to avoid repeated file open/close operations.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid</p> Source code in <code>src/calt/dataset_generator/sympy/utils/dataset_writer.py</code> <pre><code>def open(self, tag: str) -&gt; None:\n    \"\"\"\n    Open file handles for the specified tag.\n\n    This method should be called before starting batch processing to avoid\n    repeated file open/close operations.\n\n    Args:\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n\n    Raises:\n        ValueError: If tag is invalid\n    \"\"\"\n    self._validate_tag(tag)\n\n    if tag in self._file_handles:\n        self.logger.warning(f\"File handles for tag '{tag}' are already open\")\n        return\n\n    dataset_dir = self._create_dataset_dir()\n    self._file_handles[tag] = {}\n\n    # Create batch directory for pickle files\n    batch_dir = dataset_dir / f\"{tag}_batches\"\n    batch_dir.mkdir(exist_ok=True)\n    self._file_handles[tag][\"batch_dir\"] = batch_dir\n    self._file_handles[tag][\"batch_count\"] = 0\n\n    # Open text file if enabled\n    if self.save_text:\n        raw_path = dataset_dir / f\"{tag}_raw.txt\"\n        self._file_handles[tag][\"text\"] = open(raw_path, \"w\")\n\n    # Open JSON Lines file if enabled\n    if self.save_json:\n        json_path = dataset_dir / f\"{tag}_data.jsonl\"\n        self._file_handles[tag][\"json\"] = open(json_path, \"w\")\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter.close","title":"close","text":"<pre><code>close(tag: str) -&gt; None\n</code></pre> <p>Close file handles for the specified tag.</p> <p>This method should be called after finishing batch processing.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid</p> Source code in <code>src/calt/dataset_generator/sympy/utils/dataset_writer.py</code> <pre><code>def close(self, tag: str) -&gt; None:\n    \"\"\"\n    Close file handles for the specified tag.\n\n    This method should be called after finishing batch processing.\n\n    Args:\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n\n    Raises:\n        ValueError: If tag is invalid\n    \"\"\"\n    self._validate_tag(tag)\n\n    if tag not in self._file_handles:\n        self.logger.warning(f\"No open file handles found for tag '{tag}'\")\n        return\n\n    # Combine batch files into final pickle file\n    if \"batch_dir\" in self._file_handles[tag]:\n        self._combine_batch_files(tag)\n\n    # Close all open file handles\n    for file_type, file_handle in self._file_handles[tag].items():\n        if hasattr(file_handle, \"close\"):  # Only close actual file handles\n            try:\n                file_handle.close()\n            except Exception as e:\n                self.logger.error(\n                    f\"Error closing {file_type} file for tag '{tag}': {e}\"\n                )\n\n    del self._file_handles[tag]\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter.close_all","title":"close_all","text":"<pre><code>close_all() -&gt; None\n</code></pre> <p>Close all open file handles.</p> <p>This method should be called when the writer is no longer needed.</p> Source code in <code>src/calt/dataset_generator/sympy/utils/dataset_writer.py</code> <pre><code>def close_all(self) -&gt; None:\n    \"\"\"\n    Close all open file handles.\n\n    This method should be called when the writer is no longer needed.\n    \"\"\"\n    for tag in list(self._file_handles.keys()):\n        self.close(tag)\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Context manager entry.</p> Source code in <code>src/calt/dataset_generator/sympy/utils/dataset_writer.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Context manager exit - close all files.</p> Source code in <code>src/calt/dataset_generator/sympy/utils/dataset_writer.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit - close all files.\"\"\"\n    self.close_all()\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter.save_batch","title":"save_batch","text":"<pre><code>save_batch(\n    samples: StringSampleList, tag: str = \"train\", batch_idx: int = 0\n) -&gt; None\n</code></pre> <p>Save a batch of samples to files in multiple formats.</p> <p>This method saves samples in three formats: 1. Pickle (.pkl) - Binary format, always saved, used for loading 2. Raw text (.txt) - Human-readable format with separators (if save_text=True) 3. JSON Lines (.jsonl) - Structured format preserving nested structure (if save_json=True)</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>StringSampleList</code> <p>List of (problem, solution) pairs in string format</p> required <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> <code>'train'</code> <code>batch_idx</code> <code>int</code> <p>Batch index for incremental saving. Use 0 for first batch,       subsequent batches will append to existing files.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid or samples contain invalid nested structures</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Simple string samples (single problem-solution pairs)\n&gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n&gt;&gt;&gt; samples = [\n...     (\"x^2 + 2*x + 1\", \"(x + 1)^2\"),\n...     (\"2*x^3 - 3*x^2\", \"x^2*(2*x - 3)\"),\n... ]\n&gt;&gt;&gt; # Creates: train_data.pkl, train_raw.txt, train_data.jsonl\n&gt;&gt;&gt; writer.save_batch(samples, tag=\"train\", batch_idx=0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 1 level nested structure samples (multiple problems/solutions)\n&gt;&gt;&gt; samples = [\n...     ([\"x + y\", \"x - y\"], [\"2*x\", \"2*y\"]),\n...     ([\"x^2 + y^2\", \"x^2 - y^2\"], [\"2*x^2\", \"2*y^2\"]),\n... ]\n&gt;&gt;&gt; # Text output: \"x + y | x - y # 2*x | 2*y\"\n&gt;&gt;&gt; writer.save_batch(samples, tag=\"test\", batch_idx=0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 2 level nested structure samples (complex nested problems)\n&gt;&gt;&gt; samples = [\n...     ([[\"x\", \"y\"], [\"z\", \"w\"]], [[\"x\", \"z\"], [\"y\", \"w\"]]),\n...     ([[\"x + y\", \"x - y\"], [\"z + w\", \"z - w\"]], [[\"x + y\", \"z + w\"], [\"x - y\", \"z - w\"]]),\n... ]\n&gt;&gt;&gt; # Text output: \"x | y || z | w # x | z || y | w\"\n&gt;&gt;&gt; writer.save_batch(samples, tag=\"test\", batch_idx=0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Append more samples to existing dataset\n&gt;&gt;&gt; more_samples = [\n...     ([[\"a\", \"b\"], [\"c\", \"d\"]], [[\"a\", \"c\"], [\"b\", \"d\"]]),\n...     ([[\"e\", \"f\"], [\"g\", \"h\"]], [[\"e\", \"g\"], [\"f\", \"h\"]]),\n... ]\n&gt;&gt;&gt; # Appends to existing files instead of overwriting\n&gt;&gt;&gt; writer.save_batch(more_samples, tag=\"train\", batch_idx=1)\n</code></pre> Source code in <code>src/calt/dataset_generator/sympy/utils/dataset_writer.py</code> <pre><code>def save_batch(\n    self,\n    samples: StringSampleList,\n    tag: str = \"train\",\n    batch_idx: int = 0,\n) -&gt; None:\n    \"\"\"\n    Save a batch of samples to files in multiple formats.\n\n    This method saves samples in three formats:\n    1. Pickle (.pkl) - Binary format, always saved, used for loading\n    2. Raw text (.txt) - Human-readable format with separators (if save_text=True)\n    3. JSON Lines (.jsonl) - Structured format preserving nested structure (if save_json=True)\n\n    Args:\n        samples: List of (problem, solution) pairs in string format\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n        batch_idx: Batch index for incremental saving. Use 0 for first batch,\n                  subsequent batches will append to existing files.\n\n    Raises:\n        ValueError: If tag is invalid or samples contain invalid nested structures\n\n    Examples:\n        &gt;&gt;&gt; # Simple string samples (single problem-solution pairs)\n        &gt;&gt;&gt; writer = DatasetWriter(save_dir=\"./datasets\", save_text=True, save_json=True)\n        &gt;&gt;&gt; samples = [\n        ...     (\"x^2 + 2*x + 1\", \"(x + 1)^2\"),\n        ...     (\"2*x^3 - 3*x^2\", \"x^2*(2*x - 3)\"),\n        ... ]\n        &gt;&gt;&gt; # Creates: train_data.pkl, train_raw.txt, train_data.jsonl\n        &gt;&gt;&gt; writer.save_batch(samples, tag=\"train\", batch_idx=0)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 1 level nested structure samples (multiple problems/solutions)\n        &gt;&gt;&gt; samples = [\n        ...     ([\"x + y\", \"x - y\"], [\"2*x\", \"2*y\"]),\n        ...     ([\"x^2 + y^2\", \"x^2 - y^2\"], [\"2*x^2\", \"2*y^2\"]),\n        ... ]\n        &gt;&gt;&gt; # Text output: \"x + y | x - y # 2*x | 2*y\"\n        &gt;&gt;&gt; writer.save_batch(samples, tag=\"test\", batch_idx=0)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 2 level nested structure samples (complex nested problems)\n        &gt;&gt;&gt; samples = [\n        ...     ([[\"x\", \"y\"], [\"z\", \"w\"]], [[\"x\", \"z\"], [\"y\", \"w\"]]),\n        ...     ([[\"x + y\", \"x - y\"], [\"z + w\", \"z - w\"]], [[\"x + y\", \"z + w\"], [\"x - y\", \"z - w\"]]),\n        ... ]\n        &gt;&gt;&gt; # Text output: \"x | y || z | w # x | z || y | w\"\n        &gt;&gt;&gt; writer.save_batch(samples, tag=\"test\", batch_idx=0)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Append more samples to existing dataset\n        &gt;&gt;&gt; more_samples = [\n        ...     ([[\"a\", \"b\"], [\"c\", \"d\"]], [[\"a\", \"c\"], [\"b\", \"d\"]]),\n        ...     ([[\"e\", \"f\"], [\"g\", \"h\"]], [[\"e\", \"g\"], [\"f\", \"h\"]]),\n        ... ]\n        &gt;&gt;&gt; # Appends to existing files instead of overwriting\n        &gt;&gt;&gt; writer.save_batch(more_samples, tag=\"train\", batch_idx=1)\n    \"\"\"\n    self._validate_tag(tag)\n\n    # Validate samples\n    if not samples:\n        self.logger.warning(\n            \"Empty samples list provided - no files will be created\"\n        )\n        return\n\n    # Check if file handles are open for this tag\n    if tag not in self._file_handles:\n        # Fallback to old method if file handles are not open\n        self._save_batch_legacy(samples, tag, batch_idx)\n        return\n\n    # Save binary data (pickle format) - save batch individually\n    batch_file = (\n        self._file_handles[tag][\"batch_dir\"]\n        / f\"batch_{self._file_handles[tag]['batch_count']:06d}.pkl\"\n    )\n    with open(batch_file, \"wb\") as f:\n        pickle.dump(samples, f)\n    self._file_handles[tag][\"batch_count\"] += 1\n\n    # Save raw text data (optional)\n    if self.save_text:\n        for problem_str, solution_str in samples:\n            formatted_line = self._format_sample_strings(problem_str, solution_str)\n            self._file_handles[tag][\"text\"].write(f\"{formatted_line}\\n\")\n        self._file_handles[tag][\"text\"].flush()\n\n    # Save JSON Lines data (optional)\n    if self.save_json:\n        for problem_str, solution_str in samples:\n            json_data = self._get_json_data(problem_str, solution_str)\n            json_line = json.dumps(json_data, ensure_ascii=False)\n            self._file_handles[tag][\"json\"].write(f\"{json_line}\\n\")\n        self._file_handles[tag][\"json\"].flush()\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter.save_final_statistics","title":"save_final_statistics","text":"<pre><code>save_final_statistics(statistics: StatisticsDict, tag: str = 'train') -&gt; None\n</code></pre> <p>Save final overall statistics to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>statistics</code> <code>StatisticsDict</code> <p>Dictionary containing dataset statistics</p> required <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> <code>'train'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid</p> Note <p>Statistics are saved in YAML format for human readability. The file is named \"{tag}_stats.yaml\" in the dataset directory.</p> Source code in <code>src/calt/dataset_generator/sympy/utils/dataset_writer.py</code> <pre><code>def save_final_statistics(\n    self,\n    statistics: StatisticsDict,\n    tag: str = \"train\",\n) -&gt; None:\n    \"\"\"\n    Save final overall statistics to YAML file.\n\n    Args:\n        statistics: Dictionary containing dataset statistics\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n\n    Raises:\n        ValueError: If tag is invalid\n\n    Note:\n        Statistics are saved in YAML format for human readability.\n        The file is named \"{tag}_stats.yaml\" in the dataset directory.\n    \"\"\"\n    self._validate_tag(tag)\n    dataset_dir = self._create_dataset_dir()\n\n    stats_path = dataset_dir / f\"{tag}_stats.yaml\"\n    with open(stats_path, \"w\") as f:\n        yaml.dump(\n            statistics,\n            f,\n            Dumper=TimedeltaDumper,\n            default_flow_style=False,\n            sort_keys=False,\n            indent=4,\n        )\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter.load_dataset","title":"load_dataset","text":"<pre><code>load_dataset(tag: str) -&gt; StringSampleList\n</code></pre> <p>Load dataset from pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> required <p>Returns:</p> Type Description <code>StringSampleList</code> <p>List of (problem, solution) pairs in string format</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid</p> <code>FileNotFoundError</code> <p>If the pickle file doesn't exist</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; samples = writer.load_dataset(\"train\")\n&gt;&gt;&gt; print(f\"Loaded {len(samples)} samples\")\n&gt;&gt;&gt; for problem, solution in samples[:3]:\n...     print(f\"Problem: {problem}, Solution: {solution}\")\n</code></pre> Source code in <code>src/calt/dataset_generator/sympy/utils/dataset_writer.py</code> <pre><code>def load_dataset(self, tag: str) -&gt; StringSampleList:\n    \"\"\"\n    Load dataset from pickle file.\n\n    Args:\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n\n    Returns:\n        List of (problem, solution) pairs in string format\n\n    Raises:\n        ValueError: If tag is invalid\n        FileNotFoundError: If the pickle file doesn't exist\n\n    Examples:\n        &gt;&gt;&gt; samples = writer.load_dataset(\"train\")\n        &gt;&gt;&gt; print(f\"Loaded {len(samples)} samples\")\n        &gt;&gt;&gt; for problem, solution in samples[:3]:\n        ...     print(f\"Problem: {problem}, Solution: {solution}\")\n    \"\"\"\n    self._validate_tag(tag)\n    pickle_path = self.save_dir / f\"{tag}_data.pkl\"\n\n    if not pickle_path.exists():\n        raise FileNotFoundError(f\"Dataset file not found: {pickle_path}\")\n\n    with open(pickle_path, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.dataset_writer.DatasetWriter.load_dataset_jsonl","title":"load_dataset_jsonl","text":"<pre><code>load_dataset_jsonl(tag: str) -&gt; StringSampleList\n</code></pre> <p>Load dataset from JSON Lines file.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")</p> required <p>Returns:</p> Type Description <code>StringSampleList</code> <p>List of (problem, solution) pairs in string format</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tag is invalid</p> <code>FileNotFoundError</code> <p>If the JSON Lines file doesn't exist</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; samples = writer.load_dataset_jsonl(\"train\")\n&gt;&gt;&gt; print(f\"Loaded {len(samples)} samples\")\n&gt;&gt;&gt; for problem, solution in samples[:3]:\n...     print(f\"Problem: {problem}, Solution: {solution}\")\n</code></pre> Source code in <code>src/calt/dataset_generator/sympy/utils/dataset_writer.py</code> <pre><code>def load_dataset_jsonl(self, tag: str) -&gt; StringSampleList:\n    \"\"\"\n    Load dataset from JSON Lines file.\n\n    Args:\n        tag: Dataset tag (e.g., \"train\", \"test\", \"validation\", \"dev\", \"eval\")\n\n    Returns:\n        List of (problem, solution) pairs in string format\n\n    Raises:\n        ValueError: If tag is invalid\n        FileNotFoundError: If the JSON Lines file doesn't exist\n\n    Examples:\n        &gt;&gt;&gt; samples = writer.load_dataset_jsonl(\"train\")\n        &gt;&gt;&gt; print(f\"Loaded {len(samples)} samples\")\n        &gt;&gt;&gt; for problem, solution in samples[:3]:\n        ...     print(f\"Problem: {problem}, Solution: {solution}\")\n    \"\"\"\n    self._validate_tag(tag)\n    jsonl_path = self.save_dir / f\"{tag}_data.jsonl\"\n\n    if not jsonl_path.exists():\n        raise FileNotFoundError(f\"JSON Lines file not found: {jsonl_path}\")\n\n    samples = []\n    with open(jsonl_path, \"r\") as f:\n        for line_num, line in enumerate(f, 1):\n            line = line.strip()\n            if not line:  # Skip empty lines\n                continue\n            try:\n                data = json.loads(line)\n                problem = data[\"problem\"]\n                solution = data[\"solution\"]\n                samples.append((problem, solution))\n            except (json.JSONDecodeError, KeyError) as e:\n                self.logger.warning(f\"Error parsing line {line_num}: {e}\")\n                continue\n\n    return samples\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.statistics_calculator.MemoryEfficientStatisticsCalculator.update_batch","title":"update_batch","text":"<pre><code>update_batch(\n    runtimes: list[float],\n    batch_sample_stats: list[dict[str, dict[str, int | float]]],\n) -&gt; None\n</code></pre> <p>Update statistics with a batch of results using Welford's online algorithm.</p> <p>This method processes each sample individually, updating both runtime statistics and sample-specific statistics incrementally for better control and efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>runtimes</code> <code>list[float]</code> <p>List of runtime values for each sample in the batch</p> required <code>batch_sample_stats</code> <code>list[dict[str, dict[str, int | float]]]</code> <p>List of sample statistics dictionaries for the current batch.                Each dictionary has the structure:                {\"category1\": {\"metric1\": value1, ...},                 \"category2\": {\"metric1\": value1, ...}}                Example:                [{\"problem\": {\"total_degree\": 2, \"num_polynomials\": 3},                  \"solution\": {\"total_degree\": 3, \"num_polynomials\": 3}},                 {\"problem\": {\"total_degree\": 5, \"num_polynomials\": 4},                  \"solution\": {\"total_degree\": 8, \"num_polynomials\": 4}},                 ...]</p> required Source code in <code>src/calt/dataset_generator/sympy/utils/statistics_calculator.py</code> <pre><code>def update_batch(\n    self,\n    runtimes: list[float],\n    batch_sample_stats: list[dict[str, dict[str, int | float]]],\n) -&gt; None:\n    \"\"\"\n    Update statistics with a batch of results using Welford's online algorithm.\n\n    This method processes each sample individually, updating both runtime\n    statistics and sample-specific statistics incrementally for better\n    control and efficiency.\n\n    Args:\n        runtimes: List of runtime values for each sample in the batch\n        batch_sample_stats: List of sample statistics dictionaries for the current batch.\n                           Each dictionary has the structure:\n                           {\"category1\": {\"metric1\": value1, ...},\n                            \"category2\": {\"metric1\": value1, ...}}\n                           Example:\n                           [{\"problem\": {\"total_degree\": 2, \"num_polynomials\": 3},\n                             \"solution\": {\"total_degree\": 3, \"num_polynomials\": 3}},\n                            {\"problem\": {\"total_degree\": 5, \"num_polynomials\": 4},\n                             \"solution\": {\"total_degree\": 8, \"num_polynomials\": 4}},\n                            ...]\n    \"\"\"\n    # Update runtime statistics\n    for runtime in runtimes:\n        self.runtime_stats.update(runtime)\n\n    # Update sample statistics\n    for stats in batch_sample_stats:\n        # Update each numeric sample statistic incrementally\n        for category, category_stats in stats.items():\n            if isinstance(category_stats, dict):\n                # Handle nested structure like {\"problem\": {...}, \"solution\": {...}}\n                if category not in self.sample_stats:\n                    self.sample_stats[category] = {}\n\n                for stat_name, value in category_stats.items():\n                    if isinstance(value, (int, float)):\n                        if stat_name not in self.sample_stats[category]:\n                            self.sample_stats[category][stat_name] = (\n                                IncrementalStatistics()\n                            )\n                        self.sample_stats[category][stat_name].update(float(value))\n\n            elif isinstance(category_stats, (int, float)):\n                # Handle flat structure\n                if category not in self.sample_stats:\n                    self.sample_stats[category] = IncrementalStatistics()\n                self.sample_stats[category].update(float(category_stats))\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.statistics_calculator.MemoryEfficientStatisticsCalculator.get_overall_statistics","title":"get_overall_statistics","text":"<pre><code>get_overall_statistics(total_time: float, num_samples: int) -&gt; dict[str, Any]\n</code></pre> <p>Get overall statistics.</p> <p>Parameters:</p> Name Type Description Default <code>total_time</code> <code>float</code> <p>Total processing time</p> required <code>num_samples</code> <code>int</code> <p>Total number of samples</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing overall statistics with the structure:</p> <code>dict[str, Any]</code> <p>{ \"total_time\": float, \"num_samples\": int, \"samples_per_second\": float, \"generation_time\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float}, \"problem_stats\": {\"metric1\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float}, ...}, \"solution_stats\": {\"metric1\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float}, ...}</p> <code>dict[str, Any]</code> <p>}</p> Source code in <code>src/calt/dataset_generator/sympy/utils/statistics_calculator.py</code> <pre><code>def get_overall_statistics(\n    self, total_time: float, num_samples: int\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Get overall statistics.\n\n    Args:\n        total_time: Total processing time\n        num_samples: Total number of samples\n\n    Returns:\n        Dictionary containing overall statistics with the structure:\n        {\n            \"total_time\": float,\n            \"num_samples\": int,\n            \"samples_per_second\": float,\n            \"generation_time\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float},\n            \"problem_stats\": {\"metric1\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float}, ...},\n            \"solution_stats\": {\"metric1\": {\"mean\": float, \"std\": float, \"min\": float, \"max\": float}, ...}\n        }\n    \"\"\"\n    runtime_stats = self.runtime_stats.get_statistics()\n\n    overall_stats = {\n        \"total_time\": total_time,\n        \"num_samples\": num_samples,\n        \"samples_per_second\": num_samples / total_time if total_time &gt; 0 else 0.0,\n        \"generation_time\": {\n            \"mean\": runtime_stats[\"mean\"],\n            \"std\": runtime_stats[\"std\"],\n            \"min\": runtime_stats[\"min\"],\n            \"max\": runtime_stats[\"max\"],\n        },\n    }\n\n    # Add sample statistics by category\n    for category, category_stats in self.sample_stats.items():\n        if isinstance(category_stats, dict):\n            # Handle nested structure like {\"problem\": {...}, \"solution\": {...}}\n            overall_stats[f\"{category}_stats\"] = {\n                stat_name: stat_calc.get_statistics()\n                for stat_name, stat_calc in category_stats.items()\n            }\n        else:\n            # Handle flat structure\n            overall_stats[f\"{category}_stats\"] = category_stats.get_statistics()\n\n    return overall_stats\n</code></pre>"},{"location":"dataset_generator/#sampling_1","title":"Sampling","text":"<p>Generator for random polynomials with specific constraints</p> <p>Parameters:</p> Name Type Description Default <code>symbols</code> <code>str</code> <p>Symbols of polynomial ring</p> required <code>field_str</code> <code>str</code> <p>Field of polynomial ring</p> required <code>order</code> <code>str | MonomialOrder</code> <p>Order of polynomial ring</p> required <code>max_num_terms</code> <code>int | None</code> <p>Maximum number of terms in polynomial. If None, all possible terms are allowed.</p> <code>10</code> <code>max_degree</code> <code>int</code> <p>Maximum degree of polynomial</p> <code>5</code> <code>min_degree</code> <code>int</code> <p>Minimum degree of polynomial</p> <code>0</code> <code>max_coeff</code> <code>int | None</code> <p>Maximum coefficient value</p> <code>None</code> <code>num_bound</code> <code>int | None</code> <p>Maximum absolute value of coefficients</p> <code>None</code> <code>degree_sampling</code> <code>str</code> <p>How to sample degree ('uniform' or 'fixed')</p> <code>'uniform'</code> <code>term_sampling</code> <code>str</code> <p>How to sample number of terms ('uniform' or 'fixed')</p> <code>'uniform'</code> <code>strictly_conditioned</code> <code>bool</code> <p>Whether to strictly enforce conditions</p> <code>True</code> <code>nonzero_instance</code> <code>bool</code> <p>Whether to enforce non-zero instance</p> <code>True</code> <code>max_attempts</code> <code>int</code> <p>Maximum number of attempts to generate a polynomial satisfying conditions</p> <code>1000</code> Source code in <code>src/calt/dataset_generator/sympy/utils/polynomial_sampler.py</code> <pre><code>def __init__(\n    self,\n    symbols: str,\n    field_str: str,\n    order: str | MonomialOrder,\n    max_num_terms: int | None = 10,\n    max_degree: int = 5,\n    min_degree: int = 0,\n    degree_sampling: str = \"uniform\",  # 'uniform' or 'fixed'\n    term_sampling: str = \"uniform\",  # 'uniform' or 'fixed'\n    max_coeff: int | None = None,  # Used for RR and ZZ\n    num_bound: int | None = None,  # Used for QQ\n    strictly_conditioned: bool = True,\n    nonzero_instance: bool = True,\n    max_attempts: int = 1000,\n) -&gt; None:\n    \"\"\"\n    Initialize polynomial sampler\n\n    Args:\n        symbols: Symbols of polynomial ring\n        field_str: Field of polynomial ring\n        order: Order of polynomial ring\n        max_num_terms: Maximum number of terms in polynomial. If None, all possible terms are allowed.\n        max_degree: Maximum degree of polynomial\n        min_degree: Minimum degree of polynomial\n        max_coeff: Maximum coefficient value\n        num_bound: Maximum absolute value of coefficients\n        degree_sampling: How to sample degree ('uniform' or 'fixed')\n        term_sampling: How to sample number of terms ('uniform' or 'fixed')\n        strictly_conditioned: Whether to strictly enforce conditions\n        nonzero_instance: Whether to enforce non-zero instance\n        max_attempts: Maximum number of attempts to generate a polynomial satisfying conditions\n    \"\"\"\n\n    self.symbols = symbols\n    self.field_str = field_str\n    self.order = order\n    self.max_num_terms = max_num_terms\n    self.max_degree = max_degree\n    self.min_degree = min_degree\n    self.max_coeff = max_coeff\n    self.num_bound = num_bound\n    self.degree_sampling = degree_sampling\n    self.term_sampling = term_sampling\n    self.strictly_conditioned = strictly_conditioned\n    self.nonzero_instance = nonzero_instance\n    self.max_attempts = max_attempts\n    self.single_poly_sampler = SinglePolynomialSampler()\n</code></pre> <p>Sampler for single polynomial with specific constraints</p>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.polynomial_sampler.PolynomialSampler.get_field","title":"get_field","text":"<pre><code>get_field() -&gt; Domain\n</code></pre> <p>Convert field_str to actual sympy domain object</p> Source code in <code>src/calt/dataset_generator/sympy/utils/polynomial_sampler.py</code> <pre><code>def get_field(self) -&gt; Domain:\n    \"\"\"Convert field_str to actual sympy domain object\"\"\"\n    # Standard field mapping\n    standard_fields = {\"QQ\": QQ, \"RR\": RR, \"ZZ\": ZZ}\n    if self.field_str in standard_fields:\n        return standard_fields[self.field_str]\n\n    # Finite field handling\n    if not self.field_str.startswith(\"GF\"):\n        raise ValueError(f\"Unsupported field: {self.field_str}\")\n\n    try:\n        # Extract field size based on format\n        p = int(\n            self.field_str[3:-1]\n            if self.field_str.startswith(\"GF(\")\n            else self.field_str[2:]\n        )\n\n        if p &lt;= 1:\n            raise ValueError(f\"Field size must be greater than 1: {p}\")\n        return GF(p)\n    except ValueError as e:\n        raise ValueError(f\"Unsupported field: {self.field_str}\") from e\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.polynomial_sampler.PolynomialSampler.get_ring","title":"get_ring","text":"<pre><code>get_ring() -&gt; PolyRing\n</code></pre> <p>Generate polynomial ring</p> <p>Returns:</p> Name Type Description <code>PolyRing</code> <code>PolyRing</code> <p>Generated polynomial ring</p> Source code in <code>src/calt/dataset_generator/sympy/utils/polynomial_sampler.py</code> <pre><code>def get_ring(self) -&gt; PolyRing:\n    \"\"\"\n    Generate polynomial ring\n\n    Returns:\n        PolyRing: Generated polynomial ring\n    \"\"\"\n\n    R, *gens = ring(self.symbols, self.get_field(), self.order)\n    return R\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.polynomial_sampler.PolynomialSampler.sample","title":"sample","text":"<pre><code>sample(\n    num_samples: int = 1,\n    size: tuple[int, int] | None = None,\n    density: float = 1.0,\n    matrix_type: str | None = None,\n) -&gt; list[PolyElement] | list[np.ndarray]\n</code></pre> <p>Generate random polynomial samples</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>size</code> <code>tuple[int, int] | None</code> <p>If provided, generate matrix of polynomials with given size</p> <code>None</code> <code>density</code> <code>float</code> <p>Probability of non-zero entries in matrix</p> <code>1.0</code> <code>matrix_type</code> <code>str | None</code> <p>Special matrix type (e.g., 'unimodular_upper_triangular')</p> <code>None</code> <p>Returns:</p> Type Description <code>list[PolyElement] | list[ndarray]</code> <p>List of polynomials or polynomial matrices</p> Source code in <code>src/calt/dataset_generator/sympy/utils/polynomial_sampler.py</code> <pre><code>def sample(\n    self,\n    num_samples: int = 1,\n    size: tuple[int, int] | None = None,\n    density: float = 1.0,\n    matrix_type: str | None = None,\n) -&gt; list[PolyElement] | list[np.ndarray]:\n    \"\"\"\n    Generate random polynomial samples\n\n    Args:\n        num_samples: Number of samples to generate\n        size: If provided, generate matrix of polynomials with given size\n        density: Probability of non-zero entries in matrix\n        matrix_type: Special matrix type (e.g., 'unimodular_upper_triangular')\n\n    Returns:\n        List of polynomials or polynomial matrices\n    \"\"\"\n    if size is not None:\n        return [\n            self._sample_matrix(size, density, matrix_type)\n            for _ in range(num_samples)\n        ]\n    else:\n        return [self._sample_polynomial() for _ in range(num_samples)]\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.polynomial_sampler.PolynomialSampler.total_degree","title":"total_degree","text":"<pre><code>total_degree(poly: PolyElement) -&gt; int\n</code></pre> <p>Compute total degree of a polynomial</p> Source code in <code>src/calt/dataset_generator/sympy/utils/polynomial_sampler.py</code> <pre><code>def total_degree(self, poly: PolyElement) -&gt; int:\n    \"\"\"Compute total degree of a polynomial\"\"\"\n    if poly.is_zero:\n        return 0\n    else:\n        return max(sum(monom) for monom in poly.monoms())\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.single_polynomial_sampler.SinglePolynomialSampler.random_coeff","title":"random_coeff","text":"<pre><code>random_coeff(field: Domain, non_zero: bool = False, **kwargs) -&gt; Any\n</code></pre> <p>Generate a random coefficient in the given field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>Domain</code> <p>The coefficient field (e.g., ZZ, QQ, RR, GF)</p> required <code>non_zero</code> <code>bool</code> <p>If True, ensure the coefficient is non-zero</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters for coefficient generation - min: minimum value (default: -10) - max: maximum value (default: 10) - num_bound: bound for numerator and denominator in QQ (default: 10)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Random coefficient in the specified field</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameter ranges are invalid or non_zero cannot be satisfied</p> <code>NotImplementedError</code> <p>If the field is not supported</p> Source code in <code>src/calt/dataset_generator/sympy/utils/single_polynomial_sampler.py</code> <pre><code>def random_coeff(self, field: Domain, non_zero: bool = False, **kwargs) -&gt; Any:\n    \"\"\"\n    Generate a random coefficient in the given field.\n\n    Args:\n        field: The coefficient field (e.g., ZZ, QQ, RR, GF)\n        non_zero: If True, ensure the coefficient is non-zero\n        **kwargs: Additional parameters for coefficient generation\n            - min: minimum value (default: -10)\n            - max: maximum value (default: 10)\n            - num_bound: bound for numerator and denominator in QQ (default: 10)\n\n    Returns:\n        Random coefficient in the specified field\n\n    Raises:\n        ValueError: If parameter ranges are invalid or non_zero cannot be satisfied\n        NotImplementedError: If the field is not supported\n    \"\"\"\n\n    # Integer coefficient\n    if field == ZZ:\n        a = kwargs.get(\"min\", -10)\n        b = kwargs.get(\"max\", 10)\n\n        if a &gt; b:\n            raise ValueError(\"min must be &lt;= max\")\n\n        if non_zero and a == 0 and b == 0:\n            raise ValueError(\"Cannot generate non-zero ZZ with min=0 and max=0\")\n\n        # Define a generator function that returns a random ZZ in [a, b]\n        def gen_int():\n            return ZZ(random.randint(a, b))\n\n        return self._pick_random_until_nonzero(gen_int, non_zero)\n\n    # Real number coefficient\n    elif field == RR:\n        a = kwargs.get(\"min\", -10.0)\n        b = kwargs.get(\"max\", 10.0)\n\n        if a &gt; b:\n            raise ValueError(\"min must be &lt;= max\")\n\n        if non_zero and a == 0.0 and b == 0.0:\n            raise ValueError(\"Cannot generate non-zero RR with min=0.0 and max=0.0\")\n\n        # Define a generator function that returns a random RR in [a, b]\n        def gen_real():\n            return RR(random.uniform(a, b))\n\n        return self._pick_random_until_nonzero(gen_real, non_zero)\n\n    # Rational number coefficient\n    elif field == QQ:\n        num_bound = kwargs.get(\"num_bound\", 10)\n\n        if num_bound &lt;= 0:\n            raise ValueError(\"num_bound must be &gt; 0\")\n\n        # Define a generator function that returns a random QQ with numerator in [-num_bound, num_bound] and denominator in [1, num_bound]\n        def gen_rat():\n            numerator = random.randint(-num_bound, num_bound)\n            denominator = random.randint(1, num_bound)\n            return QQ(numerator, denominator)\n\n        return self._pick_random_until_nonzero(gen_rat, non_zero)\n\n    # Finite field\n    elif field.is_FiniteField:\n        p = field.characteristic()\n\n        if non_zero and p == 1:\n            raise ValueError(\n                \"Cannot generate non-zero finite field coefficient with characteristic 1\"\n            )\n\n        # Define a generator function that returns a random field element in GF(p)\n        def gen_gf():\n            return field(random.randint(0, p - 1))\n\n        return self._pick_random_until_nonzero(gen_gf, non_zero)\n\n    else:\n        raise NotImplementedError(\n            f\"Random coefficient generation not implemented for field {field}\"\n        )\n</code></pre>"},{"location":"dataset_generator/#calt.dataset_generator.sympy.utils.single_polynomial_sampler.SinglePolynomialSampler.random_element","title":"random_element","text":"<pre><code>random_element(\n    R: PolyRing,\n    degree: int = 2,\n    terms: int | None = None,\n    choose_degree: bool = False,\n    non_zero_coeff: bool = False,\n    **kwargs,\n) -&gt; PolyElement\n</code></pre> <p>Return a random polynomial of at most the specified degree and at most the specified number of terms.</p> <p>First monomials are chosen uniformly random from the set of all possible monomials of degree up to the specified degree (inclusive). This means that it is more likely that a monomial of the specified degree appears than a monomial of degree (specified degree - 1) because the former class is bigger.</p> <p>Exactly the specified number of distinct monomials are chosen this way and each one gets a random coefficient (possibly zero) from the base ring assigned.</p> <p>The returned polynomial is the sum of this list of terms.</p> <p>Parameters:</p> Name Type Description Default <code>R</code> <code>PolyRing</code> <p>Polynomial ring</p> required <code>degree</code> <code>int</code> <p>Maximum degree of the polynomial</p> <code>2</code> <code>terms</code> <code>int | None</code> <p>Number of terms in the polynomial</p> <code>None</code> <code>choose_degree</code> <code>bool</code> <p>Whether to choose degree randomly first</p> <code>False</code> <code>non_zero_coeff</code> <code>bool</code> <p>If True, ensure all coefficients are non-zero</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters for coefficient generation - min: minimum value (default: -10) - max: maximum value (default: 10) - num_bound: bound for numerator and denominator in QQ (default: 10)</p> <code>{}</code> <p>Returns:</p> Type Description <code>PolyElement</code> <p>Random polynomial in the given ring</p> Source code in <code>src/calt/dataset_generator/sympy/utils/single_polynomial_sampler.py</code> <pre><code>def random_element(\n    self,\n    R: PolyRing,\n    degree: int = 2,\n    terms: int | None = None,\n    choose_degree: bool = False,\n    non_zero_coeff: bool = False,\n    **kwargs,\n) -&gt; PolyElement:\n    \"\"\"\n    Return a random polynomial of at most the specified degree and at most the specified number of terms.\n\n    First monomials are chosen uniformly random from the set of all\n    possible monomials of degree up to the specified degree (inclusive). This means\n    that it is more likely that a monomial of the specified degree appears than\n    a monomial of degree (specified degree - 1) because the former class is bigger.\n\n    Exactly the specified number of distinct monomials are chosen this way and each one gets\n    a random coefficient (possibly zero) from the base ring assigned.\n\n    The returned polynomial is the sum of this list of terms.\n\n    Args:\n        R: Polynomial ring\n        degree: Maximum degree of the polynomial\n        terms: Number of terms in the polynomial\n        choose_degree: Whether to choose degree randomly first\n        non_zero_coeff: If True, ensure all coefficients are non-zero\n        **kwargs: Additional parameters for coefficient generation\n            - min: minimum value (default: -10)\n            - max: maximum value (default: 10)\n            - num_bound: bound for numerator and denominator in QQ (default: 10)\n\n\n    Returns:\n        Random polynomial in the given ring\n    \"\"\"\n    field = R.domain\n    n = R.ngens\n\n    counts, total = self._precomp_counts(n, degree)\n\n    if terms is not None and terms &lt; 0:\n        raise ValueError(\"terms must be &gt;= 0\")\n    if degree &lt; 0:\n        raise ValueError(\"degree must be &gt;= 0\")\n\n    # special cases\n    if terms == 0:\n        return R.zero\n    if degree == 0:\n        return R(self.random_coeff(field=field, non_zero=non_zero_coeff, **kwargs))\n\n    # adjust terms\n    if terms is None:\n        terms = min(5, total)\n    else:\n        terms = min(terms, total)\n\n    # total is 0. Just return\n    if total == 0:\n        return R.zero\n    elif terms &lt; total / 2:\n        # we choose random monomials if t &lt; total/2 because then we\n        # expect the algorithm to be faster than generating all\n        # monomials and picking a random index from the list. if t ==\n        # total/2 we expect every second random monomial to be a\n        # double such that our runtime is doubled in the worst case.\n        M: set[tuple[int, ...]] = set()\n        if not choose_degree:\n            while terms:\n                m = self._random_monomial_upto_degree_uniform(\n                    n, degree, counts, total\n                )\n                if m not in M:\n                    M.add(m)\n                    terms -= 1\n        else:\n            while terms:\n                m = self._random_monomial_upto_degree_class(n, degree)\n                if m not in M:\n                    M.add(m)\n                    terms -= 1\n    elif terms &lt;= total:\n        # generate a list of all monomials and choose among them\n        if not choose_degree:\n            M = sum(\n                [list(self._integer_vectors(_d, n)) for _d in range(degree + 1)], []\n            )\n            # we throw away those we don't need\n            for mi in range(total - terms):\n                M.pop(random.randint(0, len(M) - 1))\n            M = [tuple(m) for m in M]\n        else:\n            M = [list(self._integer_vectors(_d, n)) for _d in range(degree + 1)]\n            Mbar = []\n            for mi in range(terms):\n                # choose degree 'd' and monomial 'm' at random\n                d = random.randint(0, len(M) - 1)\n                m = random.randint(0, len(M[d]) - 1)\n                Mbar.append(M[d].pop(m))  # remove and insert\n                if len(M[d]) == 0:\n                    M.pop(d)  # bookkeeping\n            M = [tuple(m) for m in Mbar]\n\n    # Generate random coefficients\n    C = [\n        self.random_coeff(field=field, non_zero=non_zero_coeff, **kwargs)\n        for _ in range(len(M))\n    ]\n\n    # Create the polynomial using from_dict\n    return R.from_dict(dict(zip(M, C)))\n</code></pre>"},{"location":"trainer/","title":"Trainer","text":"<p>A convenient extension of the HuggingFace <code>Trainer</code> and utility helpers for training and evaluation. It streamlines device placement, metrics computation, and generation result saving.</p>"},{"location":"trainer/#class","title":"Class","text":"<p>               Bases: <code>Trainer</code></p> <p>Extension of HuggingFace :class:<code>~transformers.Trainer</code>.</p> <p>The trainer adds task-specific helpers that simplify training generative Transformer models. It accepts all the usual <code>HTrainer</code> keyword arguments and does not introduce new parameters - the default constructor is therefore forwarded verbatim.</p> Source code in <code>src/calt/trainer/trainer.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    # Keeps a chronological list of metric dictionaries that WandB has\n    # seen.  This enables the caller to inspect the *complete* training\n    # history after the run has finished without having to query WandB.\n    self.log_history = []\n\n    if self.compute_metrics is None:\n        self.compute_metrics = self._compute_metrics\n</code></pre>"},{"location":"trainer/#calt.trainer.trainer.Trainer.evaluate_and_save_generation","title":"evaluate_and_save_generation","text":"<pre><code>evaluate_and_save_generation(max_length: int = 512)\n</code></pre> <p>Run greedy/beam-search generation on the evaluation set.</p> <p>The helper decodes the model outputs into strings, stores the results in <code>eval_results.json</code> inside the trainer's output directory and finally computes exact-match accuracy between the generated and reference sequences.</p> <p>Parameters:</p> Name Type Description Default <code>max_length</code> <code>int</code> <p>Maximum generation length. Defaults to 512.</p> <code>512</code> <p>Returns:</p> Name Type Description <code>float</code> <p>Exact-match accuracy in the [0, 1] interval.</p> Source code in <code>src/calt/trainer/trainer.py</code> <pre><code>def evaluate_and_save_generation(self, max_length: int = 512):\n    \"\"\"Run greedy/beam-search generation on the evaluation set.\n\n    The helper decodes the model outputs into strings, stores the results in\n    ``eval_results.json`` inside the trainer's output directory and finally computes\n    exact-match accuracy between the generated and reference sequences.\n\n    Args:\n        max_length (int, optional): Maximum generation length. Defaults to 512.\n\n    Returns:\n        float: Exact-match accuracy in the [0, 1] interval.\n    \"\"\"\n    if self.eval_dataset is None:\n        raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n\n    all_generated_texts = []\n    all_reference_texts = []\n\n    eval_dataloader = self.get_eval_dataloader(self.eval_dataset)\n\n    self.model.eval()\n    tokenizer = self.processing_class\n\n    for batch in eval_dataloader:\n        inputs = self._prepare_inputs(batch)\n        input_ids = inputs.get(\"input_ids\")\n        attention_mask = inputs.get(\"attention_mask\")\n        labels = inputs.get(\"labels\")\n\n        if input_ids is None:\n            continue\n\n        with torch.no_grad():\n            generated_ids = self.model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,\n                # Optional: specify ``pad_token_id`` / ``eos_token_id`` as\n                # keyword arguments if the model configuration requires.\n            )\n\n        # generated_ids shape (batch_size, sequence_length)\n        current_generated_texts = tokenizer.batch_decode(\n            generated_ids,\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        )\n        all_generated_texts.extend(current_generated_texts)\n\n        if labels is not None:\n            labels[labels == -100] = tokenizer.pad_token_id\n            current_reference_texts = tokenizer.batch_decode(\n                labels,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=True,\n            )\n            all_reference_texts.extend(current_reference_texts)\n        else:\n            # Keep placeholder when reference labels are missing.\n            all_reference_texts.extend([\"\" for _ in current_generated_texts])\n\n    output_eval_file = os.path.join(\n        self.args.output_dir,\n        \"eval_results.json\",\n    )\n    results = []\n    for gen_text, ref_text in zip(all_generated_texts, all_reference_texts):\n        results.append(\n            {\n                \"generated\": gen_text,\n                \"reference\": ref_text,\n            }\n        )\n\n    with open(output_eval_file, \"w\") as writer:\n        json.dump(\n            results,\n            writer,\n            indent=4,\n            ensure_ascii=False,\n        )\n\n    correct_predictions = 0\n    total_predictions = len(all_generated_texts)\n\n    if total_predictions == 0:\n        return 0.0\n\n    for gen_text, ref_text in zip(all_generated_texts, all_reference_texts):\n        if gen_text.strip() == ref_text.strip():\n            correct_predictions += 1\n\n    success_rate = correct_predictions / total_predictions\n\n    return success_rate\n</code></pre>"},{"location":"trainer/#utilities","title":"Utilities","text":"<p>Count the number of CUDA devices visible to the current process.</p> <p>The function first inspects the environment variable <code>CUDA_VISIBLE_DEVICES</code>. When set, only the GPU indices listed there are considered visible and contribute to the count. When not set, the function falls back to <code>torch.cuda.device_count</code> and returns the total number of devices detected by the NVIDIA runtime.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of GPUs that the current process is allowed to use. <code>0</code> indicates no GPU</p> <code>int</code> <p>is available or that PyTorch was compiled without CUDA support.</p> Source code in <code>src/calt/trainer/utils.py</code> <pre><code>def count_cuda_devices() -&gt; int:\n    \"\"\"Count the number of CUDA devices visible to the current process.\n\n    The function first inspects the environment variable ``CUDA_VISIBLE_DEVICES``. When set,\n    only the GPU indices listed there are considered visible and contribute to the count.\n    When not set, the function falls back to ``torch.cuda.device_count`` and returns the\n    total number of devices detected by the NVIDIA runtime.\n\n    Returns:\n        int: Number of GPUs that the current process is allowed to use. ``0`` indicates no GPU\n        is available or that PyTorch was compiled without CUDA support.\n    \"\"\"\n\n    cuda_visible_devices = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n\n    if cuda_visible_devices is not None:\n        # ``CUDA_VISIBLE_DEVICES`` is set \u2013 split on commas to extract the\n        # list of allowed GPU indices (empty strings are filtered out).\n        visible_devices = [d for d in cuda_visible_devices.split(\",\") if d]\n        return len(visible_devices)\n\n    # Variable not set \u2013 fall back to the total number detected by PyTorch.\n    return torch.cuda.device_count()\n</code></pre> <p>Initialise a Weights &amp; Biases tracking session.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>Project name under which runs will appear in the WandB dashboard. Defaults to <code>\"transformer-algebra\"</code>.</p> <code>'transformer-algebra'</code> <code>entity</code> <code>str | None</code> <p>WandB entity (user or team) that owns the project. When <code>None</code>, the default entity configured in local WandB settings is used.</p> <code>None</code> <code>**extra_config</code> <p>Additional key-value pairs inserted into the run configuration. Useful for hyper-parameter sweeps or ad-hoc experiments.</p> <code>{}</code> Source code in <code>src/calt/trainer/utils.py</code> <pre><code>def setup_wandb(\n    project: str = \"transformer-algebra\",\n    entity: str | None = None,\n    **extra_config,\n) -&gt; None:\n    \"\"\"Initialise a Weights &amp; Biases tracking session.\n\n    Args:\n        project (str, optional): Project name under which runs will appear in the WandB dashboard.\n            Defaults to ``\"transformer-algebra\"``.\n        entity (str | None, optional): WandB entity (user or team) that owns the project.\n            When ``None``, the default entity configured in local WandB settings is used.\n        **extra_config: Additional key-value pairs inserted into the run configuration.\n            Useful for hyper-parameter sweeps or ad-hoc experiments.\n    \"\"\"\n    # Initialize wandb\n    import wandb\n\n    wandb.init(\n        project=project,\n        entity=entity,\n        config={\n            \"learning_rate\": 0.001,\n            \"batch_size\": 32,\n            \"epochs\": 10,\n        },\n    )\n</code></pre>"}]}