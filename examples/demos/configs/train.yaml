model:
  model_type: transformer
  num_encoder_layers: 2
  num_encoder_heads: 4
  num_decoder_layers: 2
  num_decoder_heads: 4
  d_model: 128
  encoder_ffn_dim: 512
  decoder_ffn_dim: 512
  max_sequence_length: 256

train:
  save_dir: ./results
  num_train_epochs: 5
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_ratio: 0.1
  batch_size: 32
  test_batch_size: 32
  lr_scheduler_type: linear
  max_grad_norm: 1.0
  optimizer: adamw_torch
  num_workers: 2
  seed: 42
  # Disable Weights & Biases logging (set no_wandb: false or remove to enable)
  wandb:
    no_wandb: true

data:
  train_dataset_path: ./data/train_raw.txt
  test_dataset_path: ./data/test_raw.txt
  # Use lexer config (includes vocab config)
  lexer_config: ./configs/lexer.yaml
