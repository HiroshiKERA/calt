{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **calt Minimal Demo**\n",
        "\n",
        "This notebook demonstrates the minimal code needed to:\n",
        "1. Generate a dataset\n",
        "2. Train a model\n",
        "3. Show evaluation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import sys\n",
        "# from pathlib import Path\n",
        "\n",
        "# # Add development calt to path (prioritize over pip-installed version)\n",
        "# # This notebook is in calt/examples/demos/, so we go up to calt/ and then to src/\n",
        "# # When running in Jupyter, Path.cwd() gives the notebook's directory\n",
        "# calt_dev_path = Path.cwd().parent.parent / \"src\"\n",
        "# sys.path.insert(0, str(calt_dev_path))\n",
        "\n",
        "# print(f\"Using development calt from: {calt_dev_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install calt-x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# On Colab: clone the repo so configs/ and data paths exist (skip if already present)\n",
        "import os\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB and not os.path.exists(\"configs/data.yaml\"):\n",
        "    !git clone --depth 1 https://github.com/HiroshiKERA/calt.git\n",
        "    os.chdir(\"calt/examples/demos\")\n",
        "\n",
        "print(\"Working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Generation\n",
        "\n",
        "<!-- Generate polynomial addition problems -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================== Dataset generation ===========================\n",
            "\n",
            "Starting dataset generation for 2 dataset(s)\n",
            "Dataset sizes: {'train': 10000, 'test': 1000}\n",
            "\n",
            "---------------------------------- train ----------------------------------\n",
            "Dataset size: 10000 samples  (Batch size: 10000)\n",
            "\n",
            "--- Batch 1/1 ---\n",
            "Processing samples 1-10000 (size: 10000)\n",
            "Starting parallel processing...\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=1)]: Done 799 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=1)]: Done 1249 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=1)]: Done 1799 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=1)]: Done 2449 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=1)]: Done 3199 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=1)]: Done 4049 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=1)]: Done 4999 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=1)]: Done 6049 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=1)]: Done 7199 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=1)]: Done 8449 tasks      | elapsed:    1.9s\n",
            "[Parallel(n_jobs=1)]: Done 9799 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    2.2s finished\n",
            "Parallel processing completed\n",
            "Batch 1 saved to file\n",
            "Batch 1/1 completed\n",
            "\n",
            "Overall statistics saved for train dataset\n",
            "Total time: 2.24 seconds\n",
            "\n",
            "\n",
            "---------------------------------- test ----------------------------------\n",
            "Dataset size: 1000 samples  (Batch size: 10000)\n",
            "\n",
            "--- Batch 1/1 ---\n",
            "Processing samples 1-1000 (size: 1000)\n",
            "Starting parallel processing...\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=1)]: Done 799 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
            "Parallel processing completed\n",
            "Batch 1 saved to file\n",
            "Batch 1/1 completed\n",
            "\n",
            "Overall statistics saved for test dataset\n",
            "Total time: 0.22 seconds\n",
            "\n",
            "\n",
            "All datasets generated successfully!\n",
            "==========================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from calt.dataset import DatasetPipeline\n",
        "from calt.dataset.sympy.utils.polynomial_sampler import PolynomialSampler\n",
        "\n",
        "\n",
        "# Define instance generator: polynomial addition\n",
        "def polynomial_addition_generator(seed):\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Initialize polynomial sampler\n",
        "    sampler = PolynomialSampler(\n",
        "        symbols=\"x0, x1\",\n",
        "        field_str=\"GF(7)\",\n",
        "        max_num_terms=2,\n",
        "        max_degree=2,\n",
        "        min_degree=1,\n",
        "    )\n",
        "\n",
        "    # Generate two polynomials\n",
        "    F = sampler.sample(num_samples=2)\n",
        "\n",
        "    # Solution is the sum\n",
        "    g = sum(F)\n",
        "\n",
        "    return F, g\n",
        "\n",
        "\n",
        "# Load config from YAML file\n",
        "cfg = OmegaConf.load(\"configs/data.yaml\")\n",
        "\n",
        "# Create dataset pipeline\n",
        "pipeline = DatasetPipeline.from_config(\n",
        "    cfg.dataset,\n",
        "    instance_generator=polynomial_addition_generator,\n",
        ")\n",
        "\n",
        "# Run dataset generation\n",
        "pipeline.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training\n",
        "\n",
        "<!-- Load data, create model, and train -->\n",
        "<!-- The entire training pipeline can be summarized in just a few lines: -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10000 samples from ./data/train_raw.txt\n",
            "Loaded 1000 samples from ./data/test_raw.txt\n",
            "/home/ara_shun/anaconda3/envs/calt-env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "/home/ara_shun/workspace/calt/src/calt/trainer/trainer.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating test dataset tokens... passed!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/ara_shun/.netrc.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshun-arkw\u001b[0m (\u001b[33mchiba-u\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "/home/ara_shun/anaconda3/envs/calt-env/lib/python3.12/site-packages/wandb/analytics/sentry.py:268: DeprecationWarning: Read the `app_url` setting from the appropriate Settings object.\n",
            "  app_url = wandb.util.app_url(tags[\"base_url\"])  # type: ignore[index]\n",
            "/home/ara_shun/anaconda3/envs/calt-env/lib/python3.12/site-packages/wandb/analytics/sentry.py:279: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
            "  self.scope.user = {\"email\": email}\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ara_shun/anaconda3/envs/calt-env/lib/python3.12/site-packages/wandb/analytics/sentry.py:268: DeprecationWarning: Read the `app_url` setting from the appropriate Settings object.\n",
            "  app_url = wandb.util.app_url(tags[\"base_url\"])  # type: ignore[index]\n",
            "/home/ara_shun/anaconda3/envs/calt-env/lib/python3.12/site-packages/wandb/analytics/sentry.py:279: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
            "  self.scope.user = {\"email\": email}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.24.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ara_shun/workspace/calt/examples/demos/wandb/run-20260220_031552-wpckj90o</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/chiba-u/huggingface/runs/wpckj90o' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/chiba-u/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/chiba-u/huggingface' target=\"_blank\">https://wandb.ai/chiba-u/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/chiba-u/huggingface/runs/wpckj90o' target=\"_blank\">https://wandb.ai/chiba-u/huggingface/runs/wpckj90o</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.4939, 'grad_norm': 2.7110846042633057, 'learning_rate': 3.121019108280255e-05, 'epoch': 0.1597444089456869}\n",
            "{'loss': 2.853, 'grad_norm': 2.885227680206299, 'learning_rate': 6.305732484076433e-05, 'epoch': 0.3194888178913738}\n",
            "{'loss': 2.0588, 'grad_norm': 3.3938708305358887, 'learning_rate': 9.490445859872612e-05, 'epoch': 0.4792332268370607}\n",
            "{'loss': 1.5457, 'grad_norm': 3.3687164783477783, 'learning_rate': 9.701704545454547e-05, 'epoch': 0.6389776357827476}\n",
            "{'loss': 1.2316, 'grad_norm': 3.6334102153778076, 'learning_rate': 9.346590909090909e-05, 'epoch': 0.7987220447284346}\n",
            "{'loss': 1.0385, 'grad_norm': 5.369253635406494, 'learning_rate': 8.991477272727273e-05, 'epoch': 0.9584664536741214}\n",
            "{'loss': 0.8943, 'grad_norm': 3.8548665046691895, 'learning_rate': 8.636363636363637e-05, 'epoch': 1.1182108626198084}\n",
            "{'loss': 0.8043, 'grad_norm': 3.8131232261657715, 'learning_rate': 8.28125e-05, 'epoch': 1.2779552715654952}\n",
            "{'loss': 0.7522, 'grad_norm': 3.678562879562378, 'learning_rate': 7.926136363636364e-05, 'epoch': 1.4376996805111821}\n",
            "{'loss': 0.6958, 'grad_norm': 5.4625043869018555, 'learning_rate': 7.571022727272727e-05, 'epoch': 1.5974440894568689}\n",
            "{'loss': 0.6535, 'grad_norm': 3.3823602199554443, 'learning_rate': 7.215909090909091e-05, 'epoch': 1.7571884984025559}\n",
            "{'loss': 0.6301, 'grad_norm': 4.124172687530518, 'learning_rate': 6.860795454545455e-05, 'epoch': 1.9169329073482428}\n",
            "{'loss': 0.599, 'grad_norm': 4.338685512542725, 'learning_rate': 6.505681818181818e-05, 'epoch': 2.07667731629393}\n",
            "{'loss': 0.5686, 'grad_norm': 5.789953708648682, 'learning_rate': 6.150568181818183e-05, 'epoch': 2.236421725239617}\n",
            "{'loss': 0.5647, 'grad_norm': 5.331116199493408, 'learning_rate': 5.7954545454545464e-05, 'epoch': 2.3961661341853033}\n",
            "{'loss': 0.541, 'grad_norm': 3.8600962162017822, 'learning_rate': 5.44034090909091e-05, 'epoch': 2.5559105431309903}\n",
            "{'loss': 0.5236, 'grad_norm': 5.343603610992432, 'learning_rate': 5.085227272727273e-05, 'epoch': 2.7156549520766773}\n",
            "{'loss': 0.5021, 'grad_norm': 3.7315242290496826, 'learning_rate': 4.7301136363636366e-05, 'epoch': 2.8753993610223643}\n",
            "{'loss': 0.5029, 'grad_norm': 5.734727382659912, 'learning_rate': 4.375e-05, 'epoch': 3.0351437699680512}\n",
            "{'loss': 0.4903, 'grad_norm': 6.545243263244629, 'learning_rate': 4.019886363636364e-05, 'epoch': 3.194888178913738}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluate_and_save_generation (step=1000, metric_key_prefix=eval)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.42273804545402527, 'eval_token_accuracy': 0.8702629543696829, 'eval_success_rate': 0.217, 'eval_runtime': 0.1394, 'eval_samples_per_second': 7176.068, 'eval_steps_per_second': 229.634, 'epoch': 3.194888178913738}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully saved generation results (step=1000, success_rate=0.2170)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_generation_success_rate': 0.217, 'eval_generation_step': 1000, 'epoch': 3.194888178913738}\n",
            "{'loss': 0.4813, 'grad_norm': 4.7628655433654785, 'learning_rate': 3.6647727272727274e-05, 'epoch': 3.3546325878594248}\n",
            "{'loss': 0.4641, 'grad_norm': 6.519922733306885, 'learning_rate': 3.3096590909090915e-05, 'epoch': 3.5143769968051117}\n",
            "{'loss': 0.4667, 'grad_norm': 4.255920886993408, 'learning_rate': 2.954545454545455e-05, 'epoch': 3.6741214057507987}\n",
            "{'loss': 0.464, 'grad_norm': 5.637125492095947, 'learning_rate': 2.5994318181818182e-05, 'epoch': 3.8338658146964857}\n",
            "{'loss': 0.4464, 'grad_norm': 4.342232704162598, 'learning_rate': 2.244318181818182e-05, 'epoch': 3.9936102236421727}\n",
            "{'loss': 0.448, 'grad_norm': 6.0849103927612305, 'learning_rate': 1.8892045454545457e-05, 'epoch': 4.15335463258786}\n",
            "{'loss': 0.4379, 'grad_norm': 5.596210956573486, 'learning_rate': 1.534090909090909e-05, 'epoch': 4.313099041533547}\n",
            "{'loss': 0.448, 'grad_norm': 6.68628454208374, 'learning_rate': 1.1789772727272728e-05, 'epoch': 4.472843450479234}\n",
            "{'loss': 0.4219, 'grad_norm': 3.6679375171661377, 'learning_rate': 8.238636363636363e-06, 'epoch': 4.63258785942492}\n",
            "{'loss': 0.4313, 'grad_norm': 3.55267596244812, 'learning_rate': 4.6875000000000004e-06, 'epoch': 4.792332268370607}\n",
            "{'loss': 0.4358, 'grad_norm': 3.977555513381958, 'learning_rate': 1.1363636363636364e-06, 'epoch': 4.952076677316294}\n",
            "{'train_runtime': 11.3454, 'train_samples_per_second': 4407.064, 'train_steps_per_second': 137.941, 'train_loss': 0.8313802487553118, 'epoch': 5.0}\n",
            "Success rate: 24.9%\n"
          ]
        }
      ],
      "source": [
        "# Complete minimal training code\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from calt.io import IOPipeline\n",
        "from calt.models import ModelPipeline\n",
        "from calt.trainer import TrainerPipeline\n",
        "\n",
        "# Load config from YAML file\n",
        "cfg = OmegaConf.load(\"./configs/train.yaml\")\n",
        "\n",
        "# Load data\n",
        "io_pipeline = IOPipeline.from_config(cfg.data)\n",
        "result = io_pipeline.build()\n",
        "\n",
        "# Create model\n",
        "model = ModelPipeline(cfg.model, result[\"tokenizer\"]).build()\n",
        "\n",
        "# Create trainer and train\n",
        "trainer = TrainerPipeline(\n",
        "    cfg.train,\n",
        "    model=model,\n",
        "    tokenizer=result[\"tokenizer\"],\n",
        "    train_dataset=result[\"train_dataset\"],\n",
        "    eval_dataset=result[\"test_dataset\"],\n",
        "    data_collator=result[\"data_collator\"],\n",
        ").build()\n",
        "\n",
        "trainer.train()\n",
        "success_rate = trainer.evaluate_and_save_generation()\n",
        "print(f\"Success rate: {100 * success_rate:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Showing Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------\n",
            " success cases \n",
            "-------------------------\n",
            "  [5] gen: 2*x0*x1+3*x0  |  ref: 2*x0*x1+3*x0\n",
            "  [12] gen: 2*x0+2*x1  |  ref: 2*x0+2*x1\n",
            "  [14] gen: 5*x0+x1  |  ref: 5*x0+x1\n",
            "  [16] gen: 3*x0^2+6*x1  |  ref: 3*x0^2+6*x1\n",
            "  [17] gen: 6*x0^2+3*x1+1  |  ref: 6*x0^2+3*x1+1\n",
            "  [18] gen: 6*x1^2+4*x0  |  ref: 6*x1^2+4*x0\n",
            "  [20] gen: 2*x0*x1+2*x1+5  |  ref: 2*x0*x1+2*x1+5\n",
            "  [25] gen: x0*x1+x1^2+4*x1  |  ref: x0*x1+x1^2+4*x1\n",
            "  [30] gen: 4*x0^2+x0*x1+2*x1  |  ref: 4*x0^2+x0*x1+2*x1\n",
            "  [35] gen: x0*x1+x0+3  |  ref: x0*x1+x0+3\n",
            "-------------------------\n",
            " failure cases \n",
            "-------------------------\n",
            "  [0] gen: 4*x1  |  ref: 2*x1\n",
            "  [1] gen: 4*x0^2+5*x1^2+4*x1+1  |  ref: 4*x0^2+4*x1^2+5*x1+1\n",
            "  [2] gen: 5*x0^2+4*x0*x1  |  ref: 4*x0^2+5*x0*x1\n",
            "  [3] gen: 4*x0*x1+3*x0  |  ref: 4*x0*x1+5*x0\n",
            "  [4] gen: x0^2+2*x0+6*x1+2  |  ref: x0^2+6*x0+2*x1+2\n",
            "  [6] gen: 6*x0+2*x1+4  |  ref: 2*x0+6*x1+4\n",
            "  [7] gen: 3*x0*x1+2*x0  |  ref: 2*x0*x1+4*x0\n",
            "  [8] gen: 4*x0*x1+2*x1^2+1  |  ref: 2*x0*x1+4*x1^2+1\n",
            "  [9] gen: 5*x0^2+3  |  ref: x0^2+4\n",
            "  [10] gen: 3*x0^2+5*x0+2*x1  |  ref: 5*x0^2+3*x0+2*x1\n"
          ]
        }
      ],
      "source": [
        "from utils import showcase\n",
        "\n",
        "# Show 10 success cases (requires running trainer.evaluate_and_save_generation())\n",
        "showcase(result[\"test_dataset\"], success_cases=True, num_show=10)\n",
        "\n",
        "# Show 10 failure cases\n",
        "showcase(result[\"test_dataset\"], success_cases=False, num_show=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "calt-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
