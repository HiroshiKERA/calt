{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **calt Minimal Demo**\n",
    "\n",
    "This notebook demonstrates the minimal code needed to:\n",
    "1. Generate a dataset\n",
    "2. Train a model\n",
    "3. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Add development calt to path (prioritize over pip-installed version)\n",
    "# # This notebook is in calt/examples/demos/, so we go up to calt/ and then to src/\n",
    "# # When running in Jupyter, Path.cwd() gives the notebook's directory\n",
    "# calt_dev_path = Path.cwd().parent.parent / \"src\"\n",
    "# sys.path.insert(0, str(calt_dev_path))\n",
    "\n",
    "# print(f\"Using development calt from: {calt_dev_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install calt-x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation\n",
    "\n",
    "<!-- Generate polynomial addition problems -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=========================== Dataset generation ===========================\n",
      "\n",
      "Starting dataset generation for 2 dataset(s)\n",
      "Dataset sizes: {'train': 10000, 'test': 1000}\n",
      "\n",
      "---------------------------------- train ----------------------------------\n",
      "Dataset size: 10000 samples  (Batch size: 10000)\n",
      "\n",
      "--- Batch 1/1 ---\n",
      "Processing samples 1-10000 (size: 10000)\n",
      "Starting parallel processing...\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=1)]: Done 4049 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=1)]: Done 4999 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=1)]: Done 6049 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=1)]: Done 7199 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=1)]: Done 8449 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=1)]: Done 9799 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    2.2s finished\n",
      "Parallel processing completed\n",
      "Batch 1 saved to file\n",
      "Batch 1/1 completed\n",
      "\n",
      "Overall statistics saved for train dataset\n",
      "Total time: 2.26 seconds\n",
      "\n",
      "\n",
      "---------------------------------- test ----------------------------------\n",
      "Dataset size: 1000 samples  (Batch size: 10000)\n",
      "\n",
      "--- Batch 1/1 ---\n",
      "Processing samples 1-1000 (size: 1000)\n",
      "Starting parallel processing...\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "Parallel processing completed\n",
      "Batch 1 saved to file\n",
      "Batch 1/1 completed\n",
      "\n",
      "Overall statistics saved for test dataset\n",
      "Total time: 0.23 seconds\n",
      "\n",
      "\n",
      "All datasets generated successfully!\n",
      "==========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from calt.dataset import DatasetPipeline\n",
    "from calt.dataset.sympy.utils.polynomial_sampler import PolynomialSampler\n",
    "\n",
    "\n",
    "# Define instance generator: polynomial addition\n",
    "def polynomial_addition_generator(seed):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Initialize polynomial sampler\n",
    "    sampler = PolynomialSampler(\n",
    "        symbols=\"x0, x1\",\n",
    "        field_str=\"GF(7)\",\n",
    "        max_num_terms=2,\n",
    "        max_degree=2,\n",
    "        min_degree=1,\n",
    "    )\n",
    "\n",
    "    # Generate two polynomials\n",
    "    F = sampler.sample(num_samples=2)\n",
    "\n",
    "    # Solution is the sum\n",
    "    g = sum(F)\n",
    "\n",
    "    return F, g\n",
    "\n",
    "\n",
    "# Load config from YAML file\n",
    "cfg = OmegaConf.load(\"configs/data.yaml\")\n",
    "\n",
    "# Create dataset pipeline\n",
    "pipeline = DatasetPipeline.from_config(\n",
    "    cfg.dataset,\n",
    "    instance_generator=polynomial_addition_generator,\n",
    ")\n",
    "\n",
    "# Run dataset generation\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training\n",
    "\n",
    "<!-- Load data, create model, and train -->\n",
    "<!-- The entire training pipeline can be summarized in just a few lines: -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded 10000 samples from ./data/train_raw.txt\n",
      "Loaded 1000 samples from ./data/test_raw.txt\n",
      "/home/ara_shun/anaconda3/envs/calt-env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "/home/ara_shun/workspace/calt/src/calt/trainer/trainer.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating test dataset tokens... passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/ara_shun/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshun-arkw\u001b[0m (\u001b[33mchiba-u\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/ara_shun/anaconda3/envs/calt-env/lib/python3.12/site-packages/wandb/analytics/sentry.py:268: DeprecationWarning: Read the `app_url` setting from the appropriate Settings object.\n",
      "  app_url = wandb.util.app_url(tags[\"base_url\"])  # type: ignore[index]\n",
      "/home/ara_shun/anaconda3/envs/calt-env/lib/python3.12/site-packages/wandb/analytics/sentry.py:279: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
      "  self.scope.user = {\"email\": email}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ara_shun/anaconda3/envs/calt-env/lib/python3.12/site-packages/wandb/analytics/sentry.py:268: DeprecationWarning: Read the `app_url` setting from the appropriate Settings object.\n",
      "  app_url = wandb.util.app_url(tags[\"base_url\"])  # type: ignore[index]\n",
      "/home/ara_shun/anaconda3/envs/calt-env/lib/python3.12/site-packages/wandb/analytics/sentry.py:279: DeprecationWarning: The `Scope.user` setter is deprecated in favor of `Scope.set_user()`.\n",
      "  self.scope.user = {\"email\": email}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ara_shun/workspace/calt/examples/demos/wandb/run-20260220_014718-qscvdzdr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chiba-u/huggingface/runs/qscvdzdr' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/chiba-u/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chiba-u/huggingface' target=\"_blank\">https://wandb.ai/chiba-u/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chiba-u/huggingface/runs/qscvdzdr' target=\"_blank\">https://wandb.ai/chiba-u/huggingface/runs/qscvdzdr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4124, 'grad_norm': 2.5769333839416504, 'learning_rate': 3.121019108280255e-05, 'epoch': 0.1597444089456869}\n",
      "{'loss': 2.7075, 'grad_norm': 2.8292036056518555, 'learning_rate': 6.305732484076433e-05, 'epoch': 0.3194888178913738}\n",
      "{'loss': 1.9319, 'grad_norm': 2.871173143386841, 'learning_rate': 9.490445859872612e-05, 'epoch': 0.4792332268370607}\n",
      "{'loss': 1.4419, 'grad_norm': 2.8783257007598877, 'learning_rate': 9.701704545454547e-05, 'epoch': 0.6389776357827476}\n",
      "{'loss': 1.1516, 'grad_norm': 5.196719169616699, 'learning_rate': 9.346590909090909e-05, 'epoch': 0.7987220447284346}\n",
      "{'loss': 0.9754, 'grad_norm': 4.357469081878662, 'learning_rate': 8.991477272727273e-05, 'epoch': 0.9584664536741214}\n",
      "{'loss': 0.8427, 'grad_norm': 2.8322551250457764, 'learning_rate': 8.636363636363637e-05, 'epoch': 1.1182108626198084}\n",
      "{'loss': 0.7684, 'grad_norm': 4.233269214630127, 'learning_rate': 8.28125e-05, 'epoch': 1.2779552715654952}\n",
      "{'loss': 0.7174, 'grad_norm': 4.59706449508667, 'learning_rate': 7.926136363636364e-05, 'epoch': 1.4376996805111821}\n",
      "{'loss': 0.6669, 'grad_norm': 3.8014562129974365, 'learning_rate': 7.571022727272727e-05, 'epoch': 1.5974440894568689}\n",
      "{'loss': 0.6344, 'grad_norm': 3.8304660320281982, 'learning_rate': 7.215909090909091e-05, 'epoch': 1.7571884984025559}\n",
      "{'loss': 0.6133, 'grad_norm': 5.397058010101318, 'learning_rate': 6.860795454545455e-05, 'epoch': 1.9169329073482428}\n",
      "{'loss': 0.5787, 'grad_norm': 5.181649684906006, 'learning_rate': 6.505681818181818e-05, 'epoch': 2.07667731629393}\n",
      "{'loss': 0.5501, 'grad_norm': 3.562634229660034, 'learning_rate': 6.150568181818183e-05, 'epoch': 2.236421725239617}\n",
      "{'loss': 0.5429, 'grad_norm': 4.718499183654785, 'learning_rate': 5.7954545454545464e-05, 'epoch': 2.3961661341853033}\n",
      "{'loss': 0.5247, 'grad_norm': 4.021854877471924, 'learning_rate': 5.44034090909091e-05, 'epoch': 2.5559105431309903}\n",
      "{'loss': 0.5118, 'grad_norm': 4.689764499664307, 'learning_rate': 5.085227272727273e-05, 'epoch': 2.7156549520766773}\n",
      "{'loss': 0.4936, 'grad_norm': 3.918870210647583, 'learning_rate': 4.7301136363636366e-05, 'epoch': 2.8753993610223643}\n",
      "{'loss': 0.4864, 'grad_norm': 6.420312404632568, 'learning_rate': 4.375e-05, 'epoch': 3.0351437699680512}\n",
      "{'loss': 0.479, 'grad_norm': 6.017299652099609, 'learning_rate': 4.019886363636364e-05, 'epoch': 3.194888178913738}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running evaluate_and_save_generation (step=1000, metric_key_prefix=eval)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39314210414886475, 'eval_token_accuracy': 0.8785769528228925, 'eval_success_rate': 0.216, 'eval_runtime': 0.1439, 'eval_samples_per_second': 6947.973, 'eval_steps_per_second': 222.335, 'epoch': 3.194888178913738}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully saved generation results (step=1000, success_rate=0.2160)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_generation_success_rate': 0.216, 'eval_generation_step': 1000, 'epoch': 3.194888178913738}\n",
      "{'loss': 0.4723, 'grad_norm': 5.4590678215026855, 'learning_rate': 3.6647727272727274e-05, 'epoch': 3.3546325878594248}\n",
      "{'loss': 0.4535, 'grad_norm': 9.179327964782715, 'learning_rate': 3.3096590909090915e-05, 'epoch': 3.5143769968051117}\n",
      "{'loss': 0.4619, 'grad_norm': 3.0299062728881836, 'learning_rate': 2.954545454545455e-05, 'epoch': 3.6741214057507987}\n",
      "{'loss': 0.4505, 'grad_norm': 5.546109199523926, 'learning_rate': 2.5994318181818182e-05, 'epoch': 3.8338658146964857}\n",
      "{'loss': 0.4416, 'grad_norm': 4.1923909187316895, 'learning_rate': 2.244318181818182e-05, 'epoch': 3.9936102236421727}\n",
      "{'loss': 0.4406, 'grad_norm': 6.049046039581299, 'learning_rate': 1.8892045454545457e-05, 'epoch': 4.15335463258786}\n",
      "{'loss': 0.4312, 'grad_norm': 3.7988717555999756, 'learning_rate': 1.534090909090909e-05, 'epoch': 4.313099041533547}\n",
      "{'loss': 0.4384, 'grad_norm': 4.3743815422058105, 'learning_rate': 1.1789772727272728e-05, 'epoch': 4.472843450479234}\n",
      "{'loss': 0.4136, 'grad_norm': 4.357372760772705, 'learning_rate': 8.238636363636363e-06, 'epoch': 4.63258785942492}\n",
      "{'loss': 0.4258, 'grad_norm': 4.382669448852539, 'learning_rate': 4.6875000000000004e-06, 'epoch': 4.792332268370607}\n",
      "{'loss': 0.4278, 'grad_norm': 5.024502277374268, 'learning_rate': 1.1363636363636364e-06, 'epoch': 4.952076677316294}\n",
      "{'train_runtime': 11.7029, 'train_samples_per_second': 4272.438, 'train_steps_per_second': 133.727, 'train_loss': 0.7992341605238259, 'epoch': 5.0}\n",
      "Success rate: 28.2%\n"
     ]
    }
   ],
   "source": [
    "# Complete minimal training code\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from calt.io import IOPipeline\n",
    "from calt.models import ModelPipeline\n",
    "from calt.trainer import TrainerPipeline\n",
    "\n",
    "# Load config from YAML file\n",
    "cfg = OmegaConf.load(\"./configs/train.yaml\")\n",
    "\n",
    "# Load data\n",
    "io_pipeline = IOPipeline.from_config(cfg.data)\n",
    "result = io_pipeline.build()\n",
    "\n",
    "# Create model\n",
    "model = ModelPipeline(cfg.model, result[\"tokenizer\"]).build()\n",
    "\n",
    "# Create trainer and train\n",
    "trainer = TrainerPipeline(\n",
    "    cfg.train,\n",
    "    model=model,\n",
    "    tokenizer=result[\"tokenizer\"],\n",
    "    train_dataset=result[\"train_dataset\"],\n",
    "    eval_dataset=result[\"test_dataset\"],\n",
    "    data_collator=result[\"data_collator\"],\n",
    ").build()\n",
    "\n",
    "trainer.train()\n",
    "success_rate = trainer.evaluate_and_save_generation()\n",
    "print(f\"Success rate: {100 * success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "calt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
